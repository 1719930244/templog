# AI 生成代码评测细粒度指标调研

> **版本**: v1.0 | **日期**: 2026-02-24 | **定位**: 面向 AI Coding 产品（AI IDE、代码助手、Coding Agent）的代码类评测指标体系梳理

---

## 一、功能正确性 (Functional Correctness)

> 核心问题：生成的代码能不能正确运行、实现预期功能？

### 1.1 pass@k 系列

pass@k 是当前 AI 代码生成评测中最广泛使用的指标，由 Chen et al. (2021) 在 Codex/HumanEval 中提出。

**定义**：从模型生成的 n 个候选中随机抽取 k 个，至少有一个通过所有测试用例的概率。

**无偏估计公式**：

```
pass@k = E[1 - C(n-c, k) / C(n, k)]
```

其中 n 为总生成样本数，c 为通过测试的样本数，C 为组合数。

**变体语义差异**：
- **pass@1**：最严格，衡量模型"一次做对"的能力，最接近真实使用场景
- **pass@5 / pass@10**：衡量模型在多次尝试下的潜力上限

**无偏估计器 vs 朴素估计**：直接用 c/n 近似 pass@1 会引入偏差，尤其在 n 较小时。无偏估计器通过组合数计算避免了这一问题（Chen et al., 2021）。

**局限性**：
- 二值判定：不区分"差一行就对了"和"完全跑不通"
- 测试依赖：测试用例不充分时会高估正确性（EvalPlus 揭示 HumanEval 的测试不足问题）
- 不衡量代码质量：通过测试的代码可能臃肿、不安全、不可维护

**参考文献**：
- Chen et al., 2021. "Evaluating Large Language Models Trained on Code" (Codex/HumanEval)
- Liu et al., 2024. "Is Your Code Generated by ChatGPT Really Correct?" (EvalPlus)

### 1.2 pass@k 增强变体

**Enhanced-test pass@k (EvalPlus)**：对原始 benchmark 的测试用例进行自动增强（增加边界条件、类型变异等），然后重新评估 pass@k。EvalPlus 发现 HumanEval 上许多模型的 pass@1 在增强测试后下降 10-20%。

**pass@t（多轮尝试）**：考虑模型在获得错误反馈后重试的能力，衡量"迭代修正"而非"一次生成"。

**自演进 pass@k**：EvoCodeBench (2602.10171, Feb 2026) 提出的动态评测框架，评测集随模型能力演进而自动升级难度，避免 benchmark 饱和。

### 1.3 任务通过率 (Task Pass Rate)

**定义**：端到端完成一个真实需求（而非单个测试用例）的比例。

**与 pass@k 的关键区别**：
- pass@k 评测的是"单函数能否通过单元测试"
- Task Pass Rate 评测的是"能否完成一个包含多文件修改、构建、部署的完整任务"

**工业实践**：头部 AI 编程产品的内部评测已从 pass@k 转向 Task Pass Rate。评测粒度从"函数"提升到"需求"，更贴近真实开发场景。评测集基于 ToB 数据脱敏后的真实任务失败案例动态生成。

### 1.4 测试通过率细分（修复场景）

针对代码修复/bug fix 场景，SWE-bench (Jimenez et al., 2024) 定义了更细粒度的测试通过率指标：

| 指标 | 定义 | 衡量 |
|------|------|------|
| **FAIL_TO_PASS (F2P)** | 修复后从失败变为通过的测试数 | 修复是否解决了目标问题 |
| **PASS_TO_PASS (P2P)** | 修复后仍然通过的测试数 | 修复是否引入了回归 |
| **Resolved Rate** | (F2P > 0) ∧ (P2P 全通过) | 综合判定：既修好了又没搞坏 |

**参考文献**：Jimenez et al., 2024. "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?"

### 1.5 执行正确性与部分正确性

**I/O 行为等价**：给定相同输入，生成代码的输出是否与参考实现一致。CRUXEval (Gu et al., 2024) 将此拆分为两个子任务：输入预测（给定代码和输出，预测输入）和输出预测（给定代码和输入，预测输出），评测模型对代码执行语义的理解。

**部分正确性**：通过的测试用例比例（而非全有全无的 pass/fail）。例如 10 个测试通过了 7 个，得分 0.7。比 pass@k 的二值判定更细粒度，但目前尚未被主流 benchmark 广泛采用。

---

## 二、工程质量 (Engineering Quality)

> 核心问题：代码不仅要"能用"，还要"好用"，符合工程规范。

### 2.1 Lint 规范性

**定义**：Linter 规则违反数 / KLOC（每千行代码）。

**计算方式**：对生成代码运行项目配置的 Linter（ESLint、Pylint、RuboCop 等），统计 warning + error 数量，除以代码行数（千行）。

**工业实践**：头部 AI 编程产品将 Lint 规范性作为核心工程质量指标，衡量生成代码是否符合项目编码规范。这是"能用"到"好用"的分水岭。

**适用场景**：所有代码生成任务，尤其是仓库级生成和代码补全。

### 2.2 复杂度指标

**圈复杂度 (Cyclomatic Complexity)**：
- 公式：V(G) = E - N + 2P（E: 边数, N: 节点数, P: 连通分量数）
- 阈值：1-10 简单，11-20 中等，21-50 复杂，>50 不可测试

**认知复杂度 (Cognitive Complexity)**：
- SonarSource 提出 (Campbell, 2018)，比圈复杂度更贴近人类理解难度
- 对嵌套结构施加递增惩罚，而非简单计数分支

**AI 生成代码的复杂度特征**：实证研究 (2508.21634, Aug 2025) 发现 AI 生成代码的圈复杂度分布与人写代码存在显著差异，AI 倾向于生成"中等复杂度"的代码，极简和极复杂的代码都较少。

### 2.3 可读性

**命名语义准确性**：变量名/函数名是否准确反映其语义。可通过 LLM-based 评估 (Siddiq et al., 2024) 或基于规则的检查（如 Python 的 PEP 8 命名规范）量化。

**嵌套深度**：建议 ≤ 4 层。AI 生成代码常出现过深嵌套，尤其在处理复杂条件逻辑时。

**LLM-as-Judge 可读性评估**：使用强模型（如 GPT-4、Claude）对生成代码的可读性进行 1-5 分评分。优势是能捕捉难以规则化的可读性因素，局限是评分一致性和可复现性。

### 2.4 代码异味密度

**定义**：AI 生成代码中的代码异味数 / KLOC。

**AI 特有异味模式**：
- 过度工程（Over-engineering）：为简单问题生成过于复杂的解决方案
- 冗余代码：不必要的变量声明、未使用的 import
- 不必要的抽象：为单次使用的逻辑创建类或函数

**实证研究**：
- "AI builds, We Analyze" (2601.16839, Jan 2026)：对 AI 生成的构建代码进行质量分析
- "Comparing Human and LLM Generated Code" (2501.16857, Jan 2025)：人写 vs AI 生成代码的异味对比

### 2.5 代码重复率

两个维度：
- **内部重复**：生成代码自身内部的重复片段比例
- **外部重复**：生成代码与已有代码库中代码的重复比例（可能暗示模型在"复制"而非"理解"）

### 2.6 风格一致性

**定义**：生成代码与项目现有代码风格的偏离度。

**量化方式**：
- Linter 规则违反数（项目自定义规则）
- 命名规范偏离率（如项目用 camelCase，生成代码用 snake_case）
- 惯用法程度（idiomatic code）：是否使用了语言的惯用写法（如 Python 的列表推导 vs 显式循环）

**架构一致性评分**：生成代码是否遵循项目的 Lint 规则和设计约定。这在 ToB 场景中尤为重要。

---

## 三、稳定性与鲁棒性 (Stability & Robustness)

> 核心问题：大规模使用时，生成代码的质量是否稳定可靠？

### 3.1 万行代码缺陷率 (Defects per 10K LoC)

**定义**：每万行生成代码中的缺陷数。

**计算**：(静态分析缺陷 + 动态测试缺陷) / (生成代码总行数 / 10000)

**工业意义**：这是衡量 AI 代码生成在大规模场景下可靠性的核心指标。单函数的 pass@k 可能很高，但当模型连续生成数千行代码时，累积缺陷率才是真正的风险度量。模拟了大规模代码库合并时的质量风险。

**参考基线**：传统软件工程中，成熟项目的缺陷密度约 0.1-1.0 缺陷/KLOC（生产环境）。AI 生成代码的这一指标目前缺乏系统性基准。

### 3.2 回归安全性 (P2P 保持率)

**定义**：修复/修改后，原本通过的测试仍然通过的比例。

**计算**：P2P_preserved / P2P_total × 100%

**衡量**：生成代码是否在解决一个问题的同时引入了新问题。在 SWE-bench 评测中，Resolved Rate 要求 P2P 全部保持，这是一个非常严格的标准。

### 3.3 边界条件处理

**量化方式**：
- 空值/null 处理覆盖率
- 整数溢出/下溢检测率
- 并发安全性（竞态条件检测）
- 异常处理完备性（try-catch 覆盖率）

目前缺乏标准化的 benchmark，但 EvalPlus 的测试增强方法论提供了一种自动化评估思路：通过变异测试自动生成边界条件测试用例。

### 3.4 输出一致性 (Consistency)

**定义**：相同输入（相同 prompt）下多次生成结果的一致性。

**计算方式**：
- 功能一致性：多次生成的代码是否都通过相同的测试用例
- 文本一致性：多次生成结果之间的编辑相似度均值
- 方差度量：pass@k 在多次独立实验中的标准差

**意义**：高一致性意味着模型输出可预测，低一致性意味着"运气成分"大。对于生产环境部署，一致性比峰值性能更重要。

---

## 四、安全性与合规 (Security & Compliance)

> 核心问题：生成的代码是否安全、合规、可信？

### 4.1 漏洞引入率

**定义**：生成代码中包含已知漏洞模式（CWE 分类）的样本比例。

**计算**：含漏洞的生成样本数 / 总生成样本数 × 100%

**评测方法**：使用 SAST 工具（Semgrep、Bandit、CodeQL）扫描生成代码，按 CWE 分类统计。

**参考基准**：
- CyberSecEval 2 (Bhatt et al., 2024, Meta)：多维度安全评测
- SecurityEval (Siddiq & Santos, 2022)：安全编码基准数据集

### 4.2 不安全代码建议率 (Insecure Code Suggestion Rate)

**定义**：AI 主动建议的代码片段中包含安全漏洞的比例。

CyberSecEval 2 的新维度。与 4.1 的区别在于：4.1 是"被动评测"（给定任务后检查输出），4.2 是"主动评测"（AI 在补全/建议场景中是否会主动引入不安全模式）。

### 4.3 Prompt Injection Resilience（抗提示词注入能力）

**定义**：在对抗性提示注入（如恶意代码注释、误导性 docstring）下，模型仍能生成安全代码的能力。

**最新研究**：
- "Can Adversarial Code Comments Fool AI Security Reviewers" (2602.16741, Feb 2026)：通过注释注入攻击 LLM 代码分析
- "Prompt Poisoning Code" (2510.22944, Oct 2025)：研究提示词投毒对代码缺陷引入率的影响

### 4.4 安全编码标准合规率

**定义**：生成代码符合特定安全编码标准的比例。

**标准覆盖**：MISRA（嵌入式）、OWASP Top 10（Web）、CWE Top 25（通用）。

**最新进展**：
- CVE-Factory (2602.03012, Feb 2026)：将安全漏洞发现做成 Agent 级任务
- "Security and Quality in LLM-Generated Code: Multi-Language Analysis" (2502.01853, Feb 2025)

### 4.5 宪法遵循 / 价值观对齐 (Constitutional Compliance)

**定义**：生成代码是否遵循安全策略，不生成恶意代码、不泄露敏感信息。

**评测维度**：
- 拒绝生成恶意代码（恶意软件、攻击脚本）的能力
- 敏感信息泄露检测（硬编码密钥、API token、文件路径）
- 许可证合规（生成代码是否包含受限许可证的代码片段）

**工业实践**：Anthropic 的 Constitutional AI 框架在代码生成中的应用，确保模型输出符合预定义的安全宪法。

---

## 五、运行时效率 (Runtime Efficiency)

> 核心问题：生成的代码跑得快不快、资源消耗合不合理？

### 5.1 时间效率

**定义**：生成代码的执行时间与参考实现（或最优解）的比值。

**评测方法**：在相同硬件环境下，对生成代码和参考实现运行相同测试输入，比较 wall-clock time。

**时间复杂度合理性**：生成代码是否选择了合理的算法复杂度。例如对排序任务生成了 O(n²) 而非 O(n log n) 的实现。

**参考基准**：EffiBench (Huang et al., 2024) 专门评测 LLM 生成代码的运行时效率，发现即使功能正确的代码，效率差异可达 10-100 倍。

### 5.2 空间效率

**定义**：生成代码的峰值内存使用量与参考实现的比值。

**参考基准**：PIE (Shypula et al., 2024) 评测模型生成性能优化代码的能力，包含内存效率维度。

### 5.3 计算效率 (FLOPs/BF)

**定义**：生成代码的浮点运算量 / 基准实现的浮点运算量。

**适用场景**：数值计算、ML pipeline、CUDA kernel、系统级代码等高性能计算场景。

**工业实践**：头部 AI 编程产品在评测中关注生成代码的计算开销，尤其是 C++/CUDA 场景下的运行时效率。这不仅关乎"代码对不对"，更关乎"代码能不能上生产"。

### 5.4 效率比 (Efficiency Ratio)

**定义**：生成代码运行时间 / 最优解运行时间。

**分级**：
- ER ≤ 1.5：优秀（接近最优）
- 1.5 < ER ≤ 5：可接受
- ER > 5：需要优化
- ER = ∞：超时/无法运行

---

## 六、上下文融合与环境适配 (Context Integration & Environment Adaptation)

> 核心问题：生成代码能否融入已有代码库和工具链？

### 6.1 跨文件引用正确性

**定义**：生成代码中跨文件引用（import 语句、函数调用、类继承）的正确比例。

**计算**：正确的跨文件引用数 / 总跨文件引用数 × 100%

**参考基准**：
- CrossCodeEval (Ding et al., 2024)：跨文件代码补全评测
- RepoEval (Zhang et al., 2023)：仓库级代码补全评测

### 6.2 上下文检索利用率

**定义**：AI 实际利用了多少提供的上下文信息来生成代码。

**评测思路**：给定不同量级的上下文（0 文件、1 文件、5 文件、全仓库），观察生成质量的变化曲线。利用率高意味着模型能有效消化上下文。

**最新基准**：
- ContextBench (2602.05892, Feb 2026)：专门评测 Coding Agent 的上下文检索能力
- SWE Context Bench (2602.08316, Feb 2026)：评测代码场景下的上下文学习能力

### 6.3 依赖兼容性

**评测维度**：
- 是否引入项目未声明的依赖
- 是否使用了不兼容版本的依赖
- API 版本正确性：是否使用了已废弃的 API（数据污染的常见表现）

### 6.4 架构契合度

**定义**：生成代码是否遵循项目的架构模式和设计约定。

**量化方式**：
- 设计模式一致性（如项目用 MVC，生成代码是否遵循）
- 模块边界尊重（是否跨越了不应跨越的模块边界）
- 仓库级上下文文件的利用 [Evaluating AGENTS.md, 2602.11988, Feb 2026]

### 6.5 工具链适配度 (Toolchain Adaptation)

**定义**：模型对特定 IDE/Agent 框架工具链调用的准确率。

**评测维度**：
- 终端命令正确率（生成的 shell 命令能否正确执行）
- 文件操作正确率（读写路径、权限等）
- 工具调用成功率（API 调用、数据库查询等）

**工业实践**：头部产品将代码生成置于具备终端操作、文件系统访问能力的 Agent 环境中，评测模型在真实工具链中的表现，而非孤立的代码片段生成。

### 6.6 私有 API 学习速率 (Private API Adaptation Rate)

**定义**：模型在接触非公开 SDK/API 文档后的快速上手能力。

**计算**：给定 N 个 API 使用示例后，模型正确调用该 API 的准确率曲线。

**工业意义**：ToB 场景中，企业往往有大量私有 SDK 和内部 API。模型能否在少量示例下快速适配，直接决定了产品在企业场景的可用性。

---

## 七、泛化性 (Generalization)

> 核心问题：模型在不同语言、不同领域的表现是否稳定？

### 7.1 跨语言泛化

**定义**：模型在主流语言（Python/JS/Java）与长尾语言（VB/COBOL/Fortran/Lua）之间的性能差距。

**计算**：长尾语言 pass@1 / 主流语言 pass@1

**工业实践**：头部产品特别评测 Visual Basic 等长尾语言的表现，验证模型在缺乏大规模高质量预训练语料下的推理迁移能力。

**参考基准**：
- MultiPL-E (Cassano et al., 2023)：HumanEval 翻译到 18+ 语言
- HumanEval-X (Zheng et al., 2023)：6 语言对齐评测

### 7.2 跨领域泛化

**评测维度**：通用代码 vs 领域特定代码的性能差距。

**新兴领域基准**：
- 移动端：SWE-bench Mobile (2602.09540, Feb 2026)
- 前端 UI：ComUIBench (2602.19276, Feb 2026)
- 智能合约：Agentic Pipeline for Smart Contract (2602.13808, Feb 2026)
- 硬件描述：SimulatorCoder (2602.17169, Feb 2026)

### 7.3 数据污染鲁棒性

**定义**：在排除训练数据泄漏后的真实性能。

**评测方法**：
- 时间戳过滤：只使用模型训练截止日期之后的数据 [LiveCodeBench, Jain et al., 2024]
- 数据去重：检测评测集与训练集的重叠
- 动态生成：评测集持续更新，避免被"刷榜"

**工业实践**：头部产品使用"黄金数据集"（Golden Dataset），排除所有可能的污染源，并以 15% 的周更新率动态刷新评测集。

---

## 八、文本与结构相似度指标（参考匹配类）

> 传统指标汇总。这些指标在代码补全、代码翻译、代码摘要等需要与参考答案比对的场景中仍有价值，但在功能正确性可通过执行验证的场景中，其重要性已下降。

### 8.1 精确匹配与编辑相似度

| 指标 | 定义 | 适用场景 | 局限 |
|------|------|---------|------|
| **Exact Match (EM)** | 生成代码与参考完全一致的比例 | 短片段补全 | 过于严格 |
| **Edit Similarity** | 1 - (编辑距离 / max(len_gen, len_ref)) | 补全、修复 | 不反映语义 |
| **Token 准确率** | 逐 token 匹配的比例 | 补全 | 不考虑结构 |

### 8.2 CodeBLEU 及变体

**CodeBLEU** (Ren et al., 2020)：专为代码设计的 BLEU 变体。

```
CodeBLEU = α·BLEU + β·BLEU_weight + γ·Match_ast + δ·Match_df
```

四组件：n-gram 匹配 + 关键词加权 n-gram + AST 子树匹配 + 数据流匹配。默认 α=β=γ=δ=0.25。

**CrystalBLEU** (Eghbali & Pradel, 2022)：去除高频 trivially shared n-grams（如 `return`、`if`、`{`），使得分数更能区分代码质量差异。

### 8.3 结构匹配

- **AST 匹配率**：生成代码与参考代码的 AST 子树匹配比例。对变量重命名、格式变化不敏感。
- **数据流匹配**：变量定义-使用链的匹配程度，衡量逻辑结构等价性。
- **标识符 F1**：变量名/函数名的 precision、recall、F1。

### 8.4 语义嵌入相似度

**BERTScore / CodeBERTScore**：基于预训练模型的向量余弦相似度。能捕捉语义等价但写法不同的代码。在代码摘要评测中常作为 BLEU/ROUGE 的补充。

---

## 九、补丁与编辑指标（修复/编辑场景）

### 9.1 补丁最小性

**定义**：补丁修改的行数 vs 最优修复所需的最小行数。

**Over-fixing 检测**：生成的补丁是否包含不必要的额外修改。实证研究 (2410.12468, Oct 2024) 发现 unresolved 的补丁平均 18.9 行，而 resolved 的补丁平均仅 2.7 行，说明"过度修改"是主要失败模式。

### 9.2 补丁应用成功率

**定义**：生成的 patch 能被 `git apply` 成功应用的比例。

**APPLY_PATCH_FAIL 率**：补丁格式错误、上下文不匹配等导致无法应用的比例。这是一个纯工程指标，但在实际评测中常被忽视。

### 9.3 State-Diff 评测（新范式）

**定义**：不依赖预定义测试用例，而是比较代码执行前后的系统状态差异来判断正确性。

**优势**：
- 不需要人工编写测试用例
- 能捕捉测试用例未覆盖的副作用
- 更适合 Agent 场景（Agent 的操作可能涉及文件系统、数据库等状态变更）

**参考**：Agent-Diff (2602.11224, Feb 2026) 提出基于状态差异的评测框架。

---

## 十、生成成本与交互效率 (Generation Cost & Interaction Efficiency)

### 10.1 生成侧成本

| 指标 | 定义 | 计算 |
|------|------|------|
| **Cost-per-Task** | 完成单个任务的总成本 | token 消耗 × 单价 |
| **Token-to-Solution Efficiency** | 解决同一问题所需的 token 数 | 对比不同模型/Agent |
| **步骤效率** | 完成任务的交互轮数 | Agent 场景 |
| **Wall-clock Time** | 从提交到完成的实际耗时 | 端到端 |

**成本-质量帕累托前沿**：在给定成本预算下能达到的最高质量。[Cost-Aware Model Selection, 2602.06370, Feb 2026] 提出了多目标优化框架。

**步骤效率最新研究**：
- EGSS (2602.05242, Feb 2026)：基于熵引导的逐步扩展策略
- SWE-Replay (2601.22129, Jan 2026)：高效的 test-time scaling

### 10.2 产品侧体验

**接受率与持久率**：
- 接受率 (Acceptance Rate)：用户接受 AI 建议的比例 [Ziegler et al., 2022]
- 持久率 (Persistence Rate)：接受后 30s/5min/1day 内未被删除的比例
- 持久率比接受率更能反映建议的真实有用性

**生产力提升**：
- 任务完成时间缩短比例 [Peng et al., 2023]
- "Intuition to Evidence: Measuring AI's True Impact on Developer Productivity" (2408.03934, Sep 2025)

**首次建议延迟 (TTFT)**：P50/P95/P99 延迟，直接影响用户体验。

### 10.3 评测集动态更新机制

**工业实践**：头部产品以约 15% 的周更新率动态刷新评测集，基于 ToB 数据脱敏后的真实任务失败案例生成新评测项。

**解决的问题**：
- 数据污染：评测集不断更新，模型无法"记住"答案
- 时效性：评测集反映最新的开发实践和 API 变化
- 真实性：评测项来自真实用户的痛点，而非人工构造

---

## 十一、意图对齐指标（需求→代码）

### 11.1 指令遵循率 (Instruction Following Rate)

**定义**：生成代码满足显式约束的比例。

**约束类型**：语言指定、复杂度限制、禁用 API、输出格式要求等。

**计算**：约束检查通过数 / 总约束数

**参考基准**：BigCodeBench (Zhuo et al., 2024) 评测复杂指令下的约束满足能力。

### 11.2 需求-代码对齐度 (SBC)

**定义**：生成代码与需求规约的语义对齐程度。

**评测方法**：逆向生成需求再比对。从生成的代码反向推导出需求描述，与原始需求比较语义相似度。[SBC Metric, 2502.07835, Feb 2025]

### 11.3 功能点覆盖率

**定义**：需求中的功能点被生成代码实现的比例。

**与 pass@k 的区别**：pass@k 只测"能不能跑通测试"，功能点覆盖测"需求中的功能做没做全"。一个通过所有测试但只实现了 60% 功能点的代码，pass@1=1 但功能点覆盖率=0.6。

---

## 十二、指标对比与选择矩阵

### 12.1 维度×任务适用性矩阵

| 评测维度 | 代码生成 | 代码补全 | 代码修复 | Agent 任务 | ToB 场景 |
|---------|:-------:|:-------:|:-------:|:---------:|:-------:|
| **功能正确性** (pass@k) | ★★★ | ★★ | ★★★ | ★★★ | ★★★ |
| **任务通过率** | ★★ | ★ | ★★★ | ★★★ | ★★★ |
| **工程质量** (Lint/复杂度) | ★★ | ★★★ | ★★ | ★★ | ★★★ |
| **稳定性** (万行缺陷率) | ★★ | ★★ | ★ | ★★★ | ★★★ |
| **安全性** | ★★ | ★★ | ★★ | ★★★ | ★★★ |
| **运行时效率** | ★★★ | ★ | ★ | ★★ | ★★★ |
| **上下文融合** | ★ | ★★★ | ★★★ | ★★★ | ★★★ |
| **泛化性** | ★★★ | ★★ | ★★ | ★★ | ★★★ |
| **文本相似度** | ★★ | ★★★ | ★ | ★ | ★ |
| **补丁指标** | ★ | ★ | ★★★ | ★★★ | ★★ |
| **成本效率** | ★ | ★ | ★★ | ★★★ | ★★★ |
| **意图对齐** | ★★★ | ★ | ★★ | ★★★ | ★★★ |

★★★ = 核心指标 | ★★ = 重要参考 | ★ = 可选

### 12.2 指标间相关性

- **pass@k ↔ 工程质量**：弱相关。通过测试的代码不一定质量好。
- **Lint 规范性 ↔ 风格一致性**：强相关。Lint 违反通常意味着风格偏离。
- **万行缺陷率 ↔ pass@k**：中等相关。pass@k 高不保证大规模缺陷率低。
- **安全性 ↔ 功能正确性**：弱相关。功能正确的代码可能包含安全漏洞。
- **成本效率 ↔ 功能正确性**：负相关趋势。更多 token/轮次通常带来更高正确性，但边际收益递减。

### 12.3 各指标计算成本对比

| 指标类别 | 计算成本 | 是否需要执行 | 是否需要参考答案 |
|---------|---------|:----------:|:-----------:|
| pass@k | 高（需沙箱执行） | ✅ | ✅（测试用例） |
| EM / Edit Similarity | 低 | ❌ | ✅ |
| CodeBLEU | 中（需 AST 解析） | ❌ | ✅ |
| Lint 规范性 | 低 | ❌ | ❌ |
| 复杂度 | 低 | ❌ | ❌ |
| 安全扫描 | 中 | ❌ | ❌ |
| 运行时效率 | 高（需执行+计时） | ✅ | ✅（基准实现） |
| 上下文融合 | 中 | 部分 | ❌ |
| LLM-as-Judge | 高（需 API 调用） | ❌ | 可选 |
| State-Diff | 高（需执行+状态比较） | ✅ | ❌ |

---

## 参考文献

### 功能正确性与 Benchmark
1. Chen et al., 2021. "Evaluating Large Language Models Trained on Code" (Codex/HumanEval). arXiv:2107.03374
2. Liu et al., 2024. "Is Your Code Generated by ChatGPT Really Correct?" (EvalPlus). arXiv:2305.01210
3. Jimenez et al., 2024. "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?" arXiv:2310.06770
4. Gu et al., 2024. "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution". arXiv:2401.03065
5. Jain et al., 2024. "LiveCodeBench: Holistic and Contamination Free Evaluation". arXiv:2403.07974
6. Zhuo et al., 2024. "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls". arXiv:2406.15877
7. EvoCodeBench, Feb 2026. arXiv:2602.10171

### 文本与结构相似度
8. Ren et al., 2020. "CodeBLEU: a Method for Automatic Evaluation of Code Synthesis". arXiv:2009.10297
9. Eghbali & Pradel, 2022. "CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code". ASE 2022

### 工程质量与可读性
10. Campbell, 2018. "Cognitive Complexity: An Overview and Evaluation". SonarSource
11. Siddiq et al., 2024. "Using LLMs to Evaluate Code Readability"
12. "AI builds, We Analyze: Empirical Study of AI-Generated Build Code Quality", Jan 2026. arXiv:2601.16839
13. "Comparing Human and LLM Generated Code: The Jury is Still Out!", Jan 2025. arXiv:2501.16857
14. "Human-Written vs. AI-Generated Code: Defects, Vulnerabilities, Complexity", Aug 2025. arXiv:2508.21634

### 安全性
15. Bhatt et al., 2024. "CyberSecEval 2" (Meta). arXiv:2404.13161
16. Siddiq & Santos, 2022. "SecurityEval Dataset". arXiv:2212.09520
17. "CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security", Feb 2026. arXiv:2602.03012
18. "Can Adversarial Code Comments Fool AI Security Reviewers", Feb 2026. arXiv:2602.16741
19. "Prompt Poisoning Code: Defect Induction Rates and Security Mitigation", Oct 2025. arXiv:2510.22944
20. "Security and Quality in LLM-Generated Code: Multi-Language Analysis", Feb 2025. arXiv:2502.01853
21. "SecureCode: Production-Grade Multi-Turn Dataset for Security-Aware Code Generation", Dec 2025. arXiv:2512.18542

### 运行时效率
22. Huang et al., 2024. "EffiBench: Benchmarking the Efficiency of Automatically Generated Code". arXiv:2402.02037
23. Shypula et al., 2024. "Learning Performance-Improving Code Edits" (PIE). arXiv:2302.07867

### 上下文融合与仓库级
24. Ding et al., 2024. "CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion". arXiv:2310.11248
25. Zhang et al., 2023. "RepoEval: Evaluating Code Completion Models with Repository-Level Code"
26. "ContextBench: Benchmark for Context Retrieval in Coding Agents", Feb 2026. arXiv:2602.05892
27. "SWE Context Bench: Benchmark for Context Learning in Coding", Feb 2026. arXiv:2602.08316
28. "Evaluating AGENTS.md: Are Repository-Level Context Files Helpful?", Feb 2026. arXiv:2602.11988

### 泛化性与多语言
29. Cassano et al., 2023. "MultiPL-E: A Scalable and Polyglot Approach to Benchmarks". arXiv:2208.08227
30. Zheng et al., 2023. "CodeGeeX: A Pre-Trained Model with Multilingual Benchmarking on HumanEval-X"

### 补丁与编辑
31. "Evaluating Software Development Agents: Patch Patterns, Code Quality", Oct 2024. arXiv:2410.12468
32. "Agent-Diff: State-Diff-Based Evaluation", Feb 2026. arXiv:2602.11224

### 成本效率与产品体验
33. Ziegler et al., 2022. "Productivity Assessment of Neural Code Completion"
34. Peng et al., 2023. "The Impact of AI on Developer Productivity: Evidence from GitHub Copilot"
35. "Intuition to Evidence: Measuring AI's True Impact on Developer Productivity", Sep 2025. arXiv:2408.03934
36. "Cost-Aware Model Selection for Text Classification", Feb 2026. arXiv:2602.06370
37. "EGSS: Entropy-guided Stepwise Scaling for Reliable SE", Feb 2026. arXiv:2602.05242
38. "SWE-Replay: Efficient Test-Time Scaling for SE Agents", Jan 2026. arXiv:2601.22129

### Agentic Coding 与前沿
39. "FeatureBench: Benchmarking Agentic Coding for Complex Feature Development", Feb 2026. arXiv:2602.10975
40. "OmniCode: A Benchmark for Evaluating Software Engineering Agents", Feb 2026. arXiv:2602.02262
41. "SWE-bench Mobile: Can LLMs Develop Industry-Level Mobile Applications?", Feb 2026. arXiv:2602.09540
42. "SWE-AGI: Benchmarking Specification-Driven Software Construction", Feb 2026. arXiv:2602.09447
43. "The Limits of Long-Context Reasoning in Automated Bug Fixing", Feb 2026. arXiv:2602.16069
44. "Debug2Fix: Supercharging Coding Agents with Interactive Debugging", Feb 2026. arXiv:2602.18571
45. "Advances and Frontiers of LLM-based Issue Resolution in SE: A Survey", Jan 2026. arXiv:2601.11655

### 意图对齐
46. "Bridging LLM-Generated Code and Requirements: SBC Metric", Feb 2025. arXiv:2502.07835
47. "From What to How: Bridging User Requirements with Software Development", Feb 2026. arXiv:2602.13611

---

**报告生成日期**: 2026 年 2 月 24 日
**版本**: v1.0
**适用范围**: AI Coding 产品评测、学术研究评测体系设计、企业内部 AI 编程工具选型
