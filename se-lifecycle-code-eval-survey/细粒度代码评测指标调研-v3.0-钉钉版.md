# 细粒度代码评测指标调研

:::
虽然面向的是代码 但评估角度有一些还是和模型有关的
:::

## 一、功能正确性 (Functional Correctness)

### 1.1 pass@k 系列

pass@k 由 Chen et al. (2021) 在 HumanEval 中提出，无偏估计公式为 `pass@k = E[1 - C(n-c, k) / C(n, k)]`，其中 n 为总生成样本数，c 为通过测试的样本数。pass@1 衡量"一次做对"的能力，pass@5/10 衡量多次尝试下的潜力上限。主要局限在于二值判定（不区分部分正确）、测试依赖（EvalPlus 揭示 HumanEval 测试不足导致高估）以及不衡量代码质量。

### 1.2 pass@k 增强变体

**Enhanced-test pass@k (EvalPlus)**：对原始 benchmark 的测试用例进行自动增强（增加边界条件、类型变异等），然后重新评估 pass@k。EvalPlus 发现 HumanEval 上许多模型的 pass@1 在增强测试后下降 10-20%。

**pass@t（多轮尝试）**：考虑模型在获得错误反馈后重试的能力，衡量"迭代修正"而非"一次生成"。

**自演进 pass@k (EvoCodeBench)**：EvoCodeBench (Zhang et al., 2602.10171, KDD 2026) 评测 LLM 驱动的自演进编码系统。评测基于 LeetCode 的 3,822 道题目（评测子集 100 题），覆盖 Python、C++、Java、Go、Kotlin 五种语言。除 pass rate 外，EvoCodeBench 引入了人类参照指标：Average Runtime Beats (%) 和 Average Memory Beats (%)，即生成代码在运行时间和内存使用上超过多少比例的人类提交（由 LeetCode 平台直接报告每个 AC 提交在同语言同题目人类提交中的百分位，取所有 AC 题目的均值）。这使得即使 pass rate 饱和，效率维度仍能区分模型能力。

> **[v3.0 新增]** **3 轮迭代预算的具体机制**：
>
> **初始提交（与 vanilla agent 相同）**：(1) 构造 prompt：问题描述 + 语言特定的 starter template；(2) 模型推理：单次前向传播，启用推理模式，最大 65,536 output tokens；(3) 确定性代码提取：从模型结构化响应的 `reasoning` + `code` 字段中提取代码；(4) 提交至 LeetCode 在线评测系统，获取判定结果和诊断信息。
>
> **自演进循环（最多 3 轮）**：
>
> *Step 1 — 执行反馈分析*：检查 OJ 判定结果。若 Accepted 且 runtime beats 和 memory beats 均超过 65%，则提前终止。否则进入反思。
>
> *Step 2 — 反思与修订*：根据判定结果分为两条路径：
> - **Bugfix Reflection**（WA/CE/RE/TLE/MLE 时）：反思 prompt 包含 (a) 原始问题描述，(b) 上一轮提交的代码，(c) OJ 判定结果及错误详情（WA 时含失败测试用例的输入/期望输出，CE 时含编译错误信息），(d) 分析失败原因并提出修复的指令。模型输出结构化 JSON：`{analysis, improved_reasoning, improved_code}`
> - **Optimization Reflection**（AC 但 beats < 65% 时）：提供当前运行时/内存统计和 beats 百分位，要求模型在保持正确性的前提下优化性能。输出格式相同
>
> *Step 3 — 迭代与终止*：重复 Step 1-2，终止条件为：(1) AC 且双 beats 均 > 65%；(2) 达到 3 轮上限；(3) 模型无法生成有效反思响应。Agent 始终保留跨所有轮次的最优提交（优先 AC，其次最高 runtime beats）。
>
> **关键设计约束**：仅解决方案代码变化，问题规格、prompt 模式、评测接口和模型参数在整个过程中固定不变，从而隔离推理时自演进的效果。反馈来自真实代码执行（LeetCode OJ），而非模拟，Agent 接收实际判定（AC/WA/TLE/MLE/CE/RE）、通过的测试用例数（如 "passed 85/87"）以及 AC 时的运行时间（ms）、内存（MB）和百分位 beats。

实验结果显示 GPT-5.2 在多数语言上 pass rate 达 84-91%，但 Claude-4.5-sonnet 的 TLE 率高达 36-42%，表明功能正确但效率不足；DeepSeek-v3.2 在长尾语言 Kotlin 上 pass rate 骤降至 7%，揭示了跨语言泛化的脆弱性。

> **[v3.0 新增]** 自演进 agent（以 gemini-3-flash-preview 为例）相比 vanilla 单次生成，pass rate 提升 10.1%-26.7%，编译型语言收益最大（C++ 达 99/100，Java 98/100），运行时间下降 7.8%-46.4%。

### 1.3 执行正确性与部分正确性

**I/O 行为等价**：CRUXEval (Gu et al., 2024) 将代码执行语义理解拆分为输入预测和输出预测两个子任务，评测模型对代码运行行为的推理能力。**部分正确性**：以通过的测试用例比例作为连续分数（如 10 个测试通过 7 个得 0.7），比 pass@k 的二值判定更细粒度，但尚未被主流 benchmark 广泛采用。

---

## 二、工程质量 (Engineering Quality)

> 核心问题：代码不仅要"能用"，还要"好用"，符合工程规范。

### 2.1 Lint 规范性

**定义**：Linter 规则违反数 / KLOC（每千行代码）。

**计算方式**：对生成代码运行项目配置的 Linter（ESLint、Pylint、RuboCop 等），统计 warning + error 数量，除以代码行数（千行）。

**适用场景**：所有代码生成任务，尤其是仓库级生成和代码补全。

> **[v3.0 新增]** **主流 Linter/静态分析工具一览**：
>
> | 工具 | 语言 | 规则类别 | 规则数 | 开源 | AI 评测常用 |
> |------|------|---------|--------|------|------------|
> | Pylint | Python | 风格/缺陷/重构 | 400+ | ✓ | ✓ |
> | Ruff | Python | 风格/缺陷/安全（集成 Flake8+Bandit 等） | 800+ | ✓ | 新兴 |
> | Flake8 | Python | 风格/复杂度 | 100+ | ✓ | ✓ |
> | Bandit | Python | 安全 | 40+ | ✓ | ✓ |
> | ESLint | JS/TS | 风格/缺陷/最佳实践 | 300+ | ✓ | ✓ |
> | Biome | JS/TS/JSON | 风格/缺陷 | 200+ | ✓ | 新兴 |
> | Checkstyle | Java | 风格/命名/Javadoc | 200+ | ✓ | ✓ |
> | PMD | Java/多语言 | 缺陷/设计/性能 | 400+ | ✓ | ✓ |
> | SpotBugs | Java | 缺陷/安全/性能 | 400+ | ✓ | |
> | Error Prone | Java | 编译期缺陷检测 | 500+ | ✓ | |
> | cppcheck | C/C++ | 缺陷/未定义行为 | 300+ | ✓ | |
> | clang-tidy | C/C++ | 风格/现代化/缺陷 | 300+ | ✓ | |
> | golangci-lint | Go | 元 Linter（聚合 50+ linter） | 1000+ | ✓ | |
> | clippy | Rust | 风格/正确性/性能 | 700+ | ✓ | |
> | RuboCop | Ruby | 风格/安全/性能 | 400+ | ✓ | |
> | SonarQube | 30+ 语言 | 缺陷/漏洞/异味/复杂度/重复 | 5000+ | 社区版 | ✓ |
> | Semgrep | 30+ 语言 | 安全/缺陷（模式匹配，自定义规则） | 自定义 | OSS | ✓ |
> | CodeQL | 8 语言 | 语义安全分析（查询式） | 2000+ | 查询开源 | ✓ |
>
> 实证研究中最常用的组合：Pylint + SonarQube（代码质量评估）和 Bandit + Semgrep + CodeQL（安全漏洞检测）。

### 2.2 复杂度指标

**圈复杂度 (Cyclomatic Complexity)**：

*   公式：V(G) = E - N + 2P（E: 边数, N: 节点数, P: 连通分量数）
    
*   阈值：1-10 简单，11-20 中等，21-50 复杂，>50 不可测试
    

**认知复杂度 (Cognitive Complexity)**：SonarSource 提出 (Campbell, 2018)，基于三条核心规则计算：

1.  **结构性递增 (+1)**：对打断线性流程的控制结构（`if`、`for`、`while`、`catch`、`switch`、三元运算符）各计 +1，且这些结构会增加嵌套层级
    
2.  **嵌套惩罚 (+nesting level)**：当上述结构嵌套在其他控制结构内部时，每层嵌套额外 +1。例如 `if` 内嵌 `for` 再嵌 `while`，最内层的 `while` 贡献 1(结构) + 2(嵌套) = 3
    
3.  **基础递增 (+1，无嵌套惩罚)**：`break LABEL`、`continue LABEL`、`goto` 等跳转语句，以及逻辑运算符序列中每次运算符类型切换（如 `a && b || c` 计 +2，而 `a && b && c` 仅计 +1）
    

与圈复杂度的关键差异：`switch` 整体只计 +1 而非按 case 累加；`else if` 计 +1 但不受嵌套惩罚；递归调用额外 +1；方法入口不计数。

**AI 生成代码的复杂度实证研究**："Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity" (Cotroneo, Improta & Liguori, 2508.21634, ISSRE 2025) 对超过 50 万个 Python 和 Java 代码样本进行分析，对比 ChatGPT、DeepSeek-Coder、Qwen-Coder 与人写代码。主要发现：AI 生成代码整体更简单、更具重复性，但更容易出现未使用的构造和硬编码调试残留；人写代码结构复杂度更高，可维护性问题更集中；AI 生成代码包含更多高风险安全漏洞。两类代码呈现出截然不同的缺陷画像。

### 2.3 可读性

**命名语义准确性**：变量名/函数名是否准确反映其语义。可通过 LLM-based 评估 (Siddiq et al., 2024) 或基于规则的检查（如 Python 的 PEP 8 命名规范）量化。

**嵌套深度**：建议 ≤ 4 层。AI 生成代码常出现过深嵌套，尤其在处理复杂条件逻辑时。

**LLM-as-Judge 可读性评估**：使用强模型（如 GPT-4、Claude）对生成代码的可读性进行 1-5 分评分。优势是能捕捉难以规则化的可读性因素，局限是评分一致性和可复现性。

> **[v3.0 新增]** **可读性量化方法体系**：
>
> **Buse & Weimer (2010, IEEE TSE)**：基于 25 个结构特征训练逻辑回归模型，120 名标注者对 100 个 Java 片段评分。核心特征包括：行长度、标识符平均长度、最大缩进深度、注释密度（注释行/总行数）、空行比例、字符熵、关键字密度、运算符密度等。输出为 [0,1] 概率分数，准确率约 80%，优于单个人类标注者的平均水平。
>
> **Scalabrino et al. (2017-2018)**：扩展 Buse-Weimer 模型，加入文本/语言学特征：标识符语义性（WordNet 覆盖率）、注释 Flesch-Kincaid 可读性指数、命名规范一致性。在 600 个代码片段、5,000+ 标注者上验证，组合模型优于纯结构特征模型，但没有单一指标能独立捕捉代码可理解性。
>
> **工程实践阈值**（综合 PEP 8、Google Style Guide、SonarQube 默认规则）：
>
> | 可读性维度 | 推荐阈值 | 说明 |
> |-----------|---------|------|
> | 行长度 | ≤ 80-120 字符 | PEP 8 推荐 79，Google Java 100 |
> | 函数体长度 | ≤ 30-50 行 | SonarQube 默认阈值 |
> | 标识符长度 | 3-25 字符 | 过短无语义，过长降低扫描效率 |
> | 注释密度 | 15%-30% | 过低缺乏解释，过高干扰阅读 |
> | 嵌套深度 | ≤ 4 层 | AI 生成代码常超标 |
> | 函数参数数 | ≤ 5 个 | 超过建议封装为对象 |

### 2.4 代码异味密度

**定义**：AI 生成代码中的代码异味数 / KLOC。

**AI 特有异味模式**：

*   过度工程（Over-engineering）：为简单问题生成过于复杂的解决方案
    
*   冗余代码：不必要的变量声明、未使用的 import
    
*   不必要的抽象：为单次使用的逻辑创建类或函数
    

**实证研究**：

*   "AI builds, We Analyze" (2601.16839, Jan 2026)：对 AI 生成的构建代码进行质量分析
    
*   "Comparing Human and LLM Generated Code" (2501.16857, Jan 2025)：人写 vs AI 生成代码的异味对比

> **[v3.0 新增]** **实证研究量化结论**：
>
> - **(2510.03029, Oct 2025)**：对比 Gemini Pro、ChatGPT、Codex、Falcon 生成的 Java 代码与专业参考实现。LLM 代码异味平均增加 63.34%，其中实现异味（含重复代码、长方法等）增加 73.35%，设计异味增加 21.42%。按模型分：Falcon 表现最好（增加 42.28%），Gemini Pro 62.07%，ChatGPT 和 Codex 更高。任务复杂度越高，异味增幅越大
> - **(2601.16839, Jan 2026)**：AI 生成的构建代码中发现 364 个可维护性/安全相关构建异味。AI Agent 在重构时难以消除预存异味，输入代码质量与 AI 能力同等重要
> - **(2511.15817, Nov 2025)**：提出 PSC（Propensity Smelly Score），利用下一 token 概率估计模型生成特定异味的倾向。发现 prompt 设计和模型架构对异味倾向起决定性作用，通过 prompt 工程可降低异味发生率
> - **(2508.21634, ISSRE 2025)**：50 万+ Python/Java 样本，AI 代码整体更简单但更具重复性，更易出现未使用构造和硬编码调试残留
> - **综合结论**：约 60.9% 的 AI 生成代码单元包含至少一个异味（ENIAC 2024）；LLM 特有代码异味影响 60.50% 的被分析系统（2512.18020, Dec 2025）

![image.png](https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/vBPlN5jjAd0GdOdG/img/34d17416-2202-40ed-be0c-c4e84ab30f8f.png)

[https://zh.wikipedia.org/zh-hans/%E4%BB%A3%E7%A0%81%E5%BC%82%E5%91%B3: https://zh.wikipedia.org/zh-hans/%E4%BB%A3%E7%A0%81%E5%BC%82%E5%91%B3](https://zh.wikipedia.org/zh-hans/%E4%BB%A3%E7%A0%81%E5%BC%82%E5%91%B3)

:::
代码异味是一个软工喜欢探究的评估维度 会综合第二章的风格、规划、质量、设计模式等角度

《代码整洁之道》

#### 耦合与内聚指标 \*\*高内聚 低耦合 代码模块化是软件工程中软件设计部分的重点\*\*

**耦合度 (Coupling)**

**传入耦合 (Ca)**: 依赖该模块的其他模块数

**传出耦合 (Ce)**: 该模块依赖的其他模块数

**CBO (Coupling Between Objects)**: 类之间的耦合关系数

**测量工具**: JDepend, NDepend, Understand

**内聚度 (Cohesion)**

**LCOM (Lack of Cohesion of Methods)**

优秀: < 0.3

良好: 0.3-0.5

需改进: > 0.5

**测量工具**: Metrics, JDepend
:::

### 2.5 代码重复率

两个维度：

*   **内部重复**：生成代码自身内部的重复片段比例
    
*   **外部重复**：生成代码与已有代码库中代码的重复比例（可能暗示模型在"复制"而非"理解"）
    

**实证研究**：Cotroneo et al. (2508.21634, ISSRE 2025) 在 50 万+ 样本上确认 AI 生成代码"更具重复性"。"Investigating The Smells of LLM Generated Code" (2510.03029, Oct 2025) 对比 Gemini Pro、ChatGPT、Codex、Falcon 生成的 Java 代码与专业参考实现，发现 LLM 生成代码的代码异味平均增加 63.34%，其中 73.35% 为实现异味（含重复代码），且任务复杂度越高异味增幅越大。"On Inter-dataset Code Duplication and Data Leakage in Large Language Models" (2401.07930, Jan 2024) 从数据集层面研究了 LLM 训练/评测数据间的代码重复与数据泄漏问题。

### 2.6 风格一致性

**定义**：生成代码与项目现有代码风格的偏离度。

**量化方式与研究支撑**：

*   **编码标准符合度**：Licorish et al. (2501.16857, Jan 2025) 使用 Pylint 评分衡量 Python 代码对 PEP 8 标准的遵循程度，发现人写代码的编码标准评分高于 GPT-4 生成代码
    
*   **Halstead 指标一致性**：Clark et al. (2024, IEEE ICoSSE) 使用 7 项 Halstead 复杂度指标评估 625 个 ChatGPT 生成的 Python 代码样本的质量一致性，发现 ChatGPT 在迭代提示和不同版本间保持了较稳定的质量水平

> **[v3.0 新增]** **Halstead 复杂度指标体系** (Halstead, 1977)：基于四个基础度量：n1（不同运算符数）、n2（不同操作数数）、N1（运算符总出现次数）、N2（操作数总出现次数）。运算符包括关键字（`if`、`while`、`return`）、算术/逻辑符号（`+`、`-`、`==`）、分隔符和函数调用；操作数包括变量、常量和字面量。
>
> | 指标 | 符号 | 公式 | 含义 |
> |------|------|------|------|
> | 程序词汇量 | n | n = n1 + n2 | 不同符号总数 |
> | 程序长度 | N | N = N1 + N2 | 符号总出现次数 |
> | 估算程序长度 | N̂ | N̂ = n1·log₂(n1) + n2·log₂(n2) | 仅从词汇量估算的长度 |
> | 程序体积 | V | V = N·log₂(n) | 信息量（bit），表示程序的信息内容 |
> | 难度 | D | D = (n1/2)·(N2/n2) | 理解/编写难度，值越高越易出错 |
> | 工作量 | E | E = D·V | 开发或理解程序所需的心智努力 |
> | 实现时间 | T | T = E/18（秒） | 基于 Stroud 数（18 次心智辨别/秒）的时间估算 |
>
> 附加派生指标：预期缺陷数 B = V/3000（或 B = E^(2/3)/3000）。Halstead 指标纯结构化（无需执行），适合自动化评测流水线。Difficulty 和 Effort 指标对比较 AI 与人写代码的可维护性尤为有用。
*   **异味倾向评分 (Propensity Smelly Score, PSC)**：(2511.15817, Nov 2025) 提出的概率性指标，估计模型生成特定类型代码异味的倾向。研究发现 prompt 设计和模型架构选择对风格质量起决定性作用
    
*   **多维工程质量评估**：RTLBench (Fang et al., 2025, ICCD) 提出覆盖语法正确性、功能性、Lint 合规、可读性和风格一致性的多维评测框架，采用 LLM-as-Judge 机制评估主观质量维度

> **[v3.0 新增]** **RTLBench 五维评测设计**：RTLBench 针对 RTL（寄存器传输级）代码提出的评测框架具体包括：(1) 语法正确性：编译工具（Icarus Verilog/Verilator）自动检查，二值判定；(2) 功能正确性：仿真测试台验证，pass@k 度量；(3) Lint 合规：静态分析工具（Verilator --lint-only, SpyGlass）检查编码标准、竞争条件、隐含锁存器等；(4) 可读性：LLM-as-Judge，按 1-5 分评分量表评估命名规范、注释质量、模块化程度；(5) 风格一致性：LLM-as-Judge，对比生成代码与项目参考代码的风格偏离度。LLM-as-Judge 机制的具体实现：强模型（如 GPT-4）接收评分量表（rubric）和待评代码，输出数值分数和文本理由。部分框架采用成对比较（pairwise comparison）替代绝对评分以减少偏差。

> **[v3.0 新增]**
>
> ### 2.7 代码自然性 (Code Naturalness)
>
> **定义**：代码符合统计语言模型预期的程度，即代码的"可预测性"。自然性越高，代码越符合开发者社区的惯用模式。
>
> **理论基础**：Hindle et al. (2012, ICSE) 的开创性研究发现，源代码比自然语言更具重复性和可预测性。Java 代码的交叉熵约 2.5-3.5 bits/token，而英语文本约 7-8 bits/token。
>
> **量化方式**：
>
> - **交叉熵**：H(c) = -(1/N) · ∑ᵢ log₂ P(tᵢ | t₁...tᵢ₋₁)，其中 tᵢ 为第 i 个 token，P 为语言模型给出的条件概率。值越低，代码越"自然"
> - **困惑度**：PP = 2^{H(c)}，值越低代码越符合统计模式。可使用传统 n-gram 模型或现代 Transformer 模型计算
> - **局部自然性**：对代码的每一行或每个语句计算局部交叉熵，识别"不自然"的代码片段（可能是 bug 或非惯用写法）
>
> **AI 代码的双刃剑效应**：LLM 生成代码天然具有高自然性（训练于大规模代码语料），但统计上"自然"的代码可能语义不正确或过于泛化。近期研究观察到 AI 反复优化代码时出现"熵衰减"（entropy decay）现象，即代码趋向于统计上最常见的模式而非最优解。自然性与可读性相关但不等价：自然性衡量模式符合度，可读性还依赖命名、注释、结构等因素。
>
> **评测意义**：自然性可作为代码质量的辅助信号。异常低的自然性可能暗示非惯用写法或潜在缺陷；异常高的自然性可能暗示过度泛化或"模板化"生成。

---

## 三、稳定性与鲁棒性 (Stability & Robustness)

> 核心问题：大规模使用时，生成代码的质量是否稳定可靠？

### 3.1 万行代码缺陷率 (Defects per 10K LoC)

**定义**：每万行生成代码中的缺陷数。

**计算**：(静态分析缺陷 + 动态测试缺陷) / (生成代码总行数 / 10000)

**工业意义**：这是衡量 AI 代码生成在大规模场景下可靠性的核心指标。单函数的 pass@k 可能很高，但当模型连续生成数千行代码时，累积缺陷率才是真正的风险度量。模拟了大规模代码库合并时的质量风险。

**参考基线**：

> **[v3.0 新增]** 传统软件工程中缺陷密度的经典数据来源：
>
> | 来源 | 场景 | 缺陷/KLOC |
> |------|------|-----------|
> | 行业平均（开发阶段） | 测试前 | 15-50 |
> | 行业平均（发布后） | 商业软件 | 1-25 |
> | McConnell: Microsoft Apps | 发布后 | ~0.5 |
> | Coverity Scan | 开源高质量项目 | ~0.68-1.0 |
> | Capers Jones: CMMI Level 5 | 发布后 | 0.5-1.0 |
> | McConnell: Cleanroom | 发布后 | ~0.1 |
> | CMU SEI: TSP | 发布后 | 0.06-0.4 |
>
> "0.1-1.0 缺陷/KLOC"范围的下界来自 Cleanroom/TSP 最佳实践（McConnell, "Code Complete", 2004, Chapter 20），上界来自 Coverity Scan 开源项目基准和 Capers Jones CMMI Level 5 数据（Jones, "Applied Software Measurement", 2008）。AI 生成代码目前缺乏系统性基准，但初步研究表明 AI 代码逻辑错误率约高出人写代码 30%，且 AI 代码的缺陷画像与人写代码截然不同（更多未使用构造和硬编码残留，更少结构复杂度问题）。

### 3.2 回归安全性 (P2P 保持率)

**定义**：修复/修改后，原本通过的测试仍然通过的比例。

**计算**：P2P\_preserved / P2P\_total × 100%

**衡量**：生成代码是否在解决一个问题的同时引入了新问题。在 SWE-bench 评测中，Resolved Rate 要求 P2P 全部保持，这是一个非常严格的标准。

### 3.3 输出一致性 (Consistency)

**定义**：相同 prompt 下多次生成结果在功能和文本层面的稳定程度。

**计算方式**：

*   功能一致性：多次生成的代码是否都通过相同的测试用例集合

> **[v3.0 新增]** **功能一致性形式化定义**：设对同一 prompt 生成 m 次代码 {c₁, c₂, ..., cₘ}，测试用例集 T = {t₁, t₂, ..., tₙ}，定义通过矩阵 R ∈ {0,1}^{m×n}，其中 R_{ij} = 1 当且仅当 cᵢ 通过 tⱼ。
>
> - 单测试一致性：Agreement(tⱼ) = max(∑ᵢ R_{ij}, m - ∑ᵢ R_{ij}) / m
> - 功能一致性分数：FC = (1/n) · ∑ⱼ Agreement(tⱼ)
> - FC = 1 表示所有生成结果在每个测试上行为完全一致（全通过或全失败）；FC 接近 0.5 表示行为高度随机
> - 等价度量：Fleiss' κ 系数，将 m 次生成视为 m 个评分者对 n 个测试的二值评分，κ > 0.8 表示高一致性，κ < 0.4 表示低一致性

*   文本一致性：多次生成结果之间的编辑相似度均值
    
*   方差度量：pass@k 在多次独立实验中的标准差
    

**解码参数的影响**：输出一致性与采样参数（temperature、top-p）密切相关。Ouyang et al. (2308.02828) 对 829 个代码生成问题的实证研究发现，即使在 temperature=0 的确定性设置下，ChatGPT 仍表现出显著的非确定性行为，这源于 GPU 浮点运算的非确定性和 API 实现细节。Donato et al. (2502.17450, Feb 2025) 进一步发现 top-p 对输出变异性的影响大于 temperature，并量化了为控制非确定性所需的最小重复实验次数。这意味着评测 pass@k 时，若不控制采样参数并进行多次重复，结论的可复现性存疑。

**意义**：对于生产环境部署，一致性比峰值性能更重要。高一致性意味着模型输出可预测，低一致性意味着"运气成分"大。

---

## 四、安全性与合规 (Security & Compliance)

> 核心问题：生成的代码是否安全、合规、可信？

### 4.1 漏洞引入率

**定义**：生成代码中包含已知漏洞模式（CWE 分类）的样本比例。

**计算**：含漏洞的生成样本数 / 总生成样本数 × 100%

**评测方法**：使用 SAST 工具（Semgrep、Bandit、CodeQL）扫描生成代码，按 CWE 分类统计。区分被动评测（给定任务后检查输出）和主动评测（AI 在补全/建议场景中是否主动引入不安全模式，CyberSecEval 2 的新维度）。

**参考基准**：

*   CyberSecEval 2 (Bhatt et al., 2024, Meta)：多维度安全评测
    
*   SecurityEval (Siddiq & Santos, 2022)：安全编码基准数据集
    

### 4.2 Prompt Injection Resilience（抗提示词注入能力）

**定义**：在对抗性提示注入（如恶意代码注释、误导性 docstring）下，模型仍能生成安全代码的能力。

**最新研究**：

*   "Can Adversarial Code Comments Fool AI Security Reviewers" (2602.16741, Feb 2026)：通过注释注入攻击 LLM 代码分析
    
*   "Prompt Poisoning Code" (2510.22944, Oct 2025)：研究提示词投毒对代码缺陷引入率的影响
    

### 4.3 安全编码标准合规率

**定义**：生成代码符合特定安全编码标准的比例。

**标准覆盖**：MISRA（嵌入式）、OWASP Top 10（Web）、CWE Top 25（通用）。

> **[v3.0 新增]** **各标准详述**：
>
> **MISRA C:2023**：175 条准则（指令 + 规则），分为 Mandatory（必须遵守）、Required（应当遵守，偏差需正式文档）、Advisory（建议遵守）三级，以及 Decidable（工具可自动判定）和 Undecidable（需人工审查）两类。覆盖类型安全、指针使用、内存管理、并发等 15+ 主题。合规报告形式：违规数/KLOC + 正式偏差文档（Deviation Record）。
>
> **OWASP Top 10 (2021 版)**：
>
> | 编号 | 类别 | 说明 |
> |------|------|------|
> | A01 | 访问控制失效 | 权限绕过、越权访问 |
> | A02 | 加密失败 | 敏感数据明文传输/存储 |
> | A03 | 注入 | SQL/NoSQL/OS/LDAP 注入 |
> | A04 | 不安全设计（新增） | 架构层面的安全缺陷 |
> | A05 | 安全配置错误 | 默认配置、不必要的功能启用 |
> | A06 | 易受攻击和过时组件 | 已知漏洞的依赖 |
> | A07 | 认证失败 | 弱密码、会话管理缺陷 |
> | A08 | 软件与数据完整性失败（新增） | 不安全的反序列化、CI/CD 完整性 |
> | A09 | 安全日志与监控失败 | 缺乏审计追踪 |
> | A10 | 服务端请求伪造 SSRF（新增） | 未验证的服务端 URL 请求 |
>
> **CWE Top 25 (2024 版)**：前 5 为 CWE-79（XSS）、CWE-787（越界写）、CWE-89（SQL 注入）、CWE-352（CSRF，上升 5 位）、CWE-22（路径遍历）。最大变动：CWE-94（代码注入）上升 12 位。
>
> **CERT 安全编码标准**：SEI CERT C 包含 116 条规则，Java 包含 168 条规则，按主题分类（IDS 输入验证/DCL 声明/EXP 表达式/INT 整数/STR 字符串/MEM 内存/FIO 文件 I/O 等），设 L1/L2/L3 三级优先级。其中 46 条 C 规则已标准化为 ISO/IEC TS 17961。
>
> **合规率计算**：(通过的规则检查数 / 适用规则总数) × 100%。可按严重级别加权：Critical 权重 3、Major 权重 2、Minor 权重 1。呈现形式通常为分类合规率雷达图或违规热力图（按 CWE 类别聚合）。

**最新进展**：

*   CVE-Factory (2602.03012, Feb 2026)：将安全漏洞发现做成 Agent 级任务
    
*   "Security and Quality in LLM-Generated Code: Multi-Language Analysis" (2502.01853, Feb 2025)
    

### 4.4 许可证合规与价值观对齐 (License Compliance & Constitutional Compliance)

**许可证合规**：AI 生成代码可能包含与受限许可证（如 GPL、AGPL）关联的代码片段，引发法律风险。

**评测基准与工具**：

*   **LiCoEval** (Xu et al., 2408.02487, ICSE 2025)：首个系统评测 LLM 许可证合规能力的基准。评测 14 个主流 LLM，发现 0.88%-2.01% 的生成代码与已有开源代码存在"显著相似性"（striking similarity），且多数模型无法准确提供许可证信息，尤其对 copyleft 许可证的识别能力薄弱
    
*   **CodeGenLink** (Bifolco et al., 2510.01077, ASE 2025)：VS Code 插件，结合 LLM 与 Web 搜索追溯生成代码的可能来源及其许可证信息
    
*   **RepoMark** (Qu et al., 2508.21432, Aug 2025)：数据标记框架，通过语义等价代码变体作为"水印"审计代码 LLM 是否未经授权使用特定仓库进行训练，在 5% 误检率下检测成功率超 90%
    

**训练数据许可证风险**："Cracks in The Stack" (Jahanshahi & Mockus, 2501.02628, LLM4Code 2025) 分析 The Stack v2 数据集，发现因来源识别错误导致非宽松许可证代码被纳入训练集。Salerno et al. (2501.17501, MSR 2025) 发现 StarCoder2-15B 中 54.9% 的可提取预训练数据可被还原，许可证信息是最容易被记忆但也最容易在微调后遗忘的数据类别。

**版权保护机制**：Anchored Decoding (He et al., 2602.07120, Feb 2026) 提出推理时方法，通过将生成约束在宽松许可证训练的"安全模型"附近，消除最多 75% 的可测量复制行为。PROD (Jiang et al., 2506.17125, AAAI 2026) 提出代码遗忘基准，定义了 Pareto Dominance Ratio (PDR) 指标联合评估遗忘质量与模型效用。

**价值观对齐**：生成代码是否遵循安全策略，包括拒绝生成恶意代码、检测硬编码密钥和敏感信息泄露等。

---
---

## 五、上下文适配与泛化能力 (Context Adaptation & Generalization)

> 核心问题：生成代码能否融入已有代码库，并在不同语言、领域间保持稳定表现？

### 5.1 架构契合度

**定义**：生成代码是否遵循项目的架构模式和设计约定。

**量化方式**：

*   设计模式一致性（如项目用 MVC，生成代码是否遵循）
    
*   模块边界尊重（是否跨越了不应跨越的模块边界）
    
*   仓库级上下文文件的利用 \[Evaluating AGENTS.md, 2602.11988, Feb 2026\]
    

### 5.2 工具链适配度 (Toolchain Adaptation)

**定义**：模型对特定 IDE/Agent 框架工具链调用的准确率。

**评测维度**：

*   终端命令正确率（生成的 shell 命令能否正确执行）
    
*   文件操作正确率（读写路径、权限等）
    
*   工具调用成功率（API 调用、数据库查询等）
    

**工业实践**：头部产品将代码生成置于具备终端操作、文件系统访问能力的 Agent 环境中，评测模型在真实工具链中的表现，而非孤立的代码片段生成。

### 5.3 私有 API 学习速率 (Private API Adaptation Rate)

**定义**：模型在接触非公开 SDK/API 文档后的快速上手能力。

**计算**：给定 N 个 API 使用示例后，模型正确调用该 API 的准确率曲线。

**工业意义**：ToB 场景中，企业往往有大量私有 SDK 和内部 API。模型能否在少量示例下快速适配，直接决定了产品在企业场景的可用性。

:::
感觉很有意义
:::

### 5.4 跨语言泛化

**定义**：模型在主流语言（Python/JS/Java）与长尾语言（VB/COBOL/Fortran/Lua）之间的性能差距。

**计算**：长尾语言 pass@1 / 主流语言 pass@1

**工业实践**：头部产品特别评测 Visual Basic 等长尾语言的表现，验证模型在缺乏大规模高质量预训练语料下的推理迁移能力。

**参考基准**：

*   MultiPL-E (Cassano et al., 2023)：HumanEval 翻译到 18+ 语言
    
*   HumanEval-X (Zheng et al., 2023)：6 语言对齐评测
    

### 5.5 跨领域泛化

**评测维度**：通用代码 vs 领域特定代码的性能差距。

**新兴领域基准**：

*   移动端：SWE-bench Mobile (2602.09540, Feb 2026)
    
*   前端 UI：ComUIBench (2602.19276, Feb 2026)
    
*   智能合约：Agentic Pipeline for Smart Contract (2602.13808, Feb 2026)
    
*   硬件描述：SimulatorCoder (2602.17169, Feb 2026)
    

---

## 六、补丁与编辑指标（修复/编辑场景）

### 6.1 补丁最小性与过度修复

**核心论文**："Evaluating Software Development Agents: Patch Patterns, Code Quality, and Issue Complexity in Real-World GitHub Scenarios" (Chen & Jiang, 2410.12468, SANER 2025)。该研究分析了 10 个顶级 Agent 在 SWE-Bench Verified 的 500 个真实 GitHub issue 上生成的 4,892 个补丁。

**关键发现**：

*   330/500 个 issue 被解决，仅 5.8% 的 issue 被所有 10 个 Agent 同时解决，10.2% 仅被单个 Agent 解决
    
*   文件级对齐度高（FactoryCodeDroid 达 94.59% F1=1），但函数级对齐度骤降至 24.32%，说明 Agent 能定位正确文件但常修改错误函数
    
*   过度修改是主要失败模式：HoneyComb 平均 47.54 行代码变更 vs 金标准补丁 7.71 行；SWE-Agent (Claude 3.5) 平均 43.35 行 vs 7.85 行
    
*   相反，Gru (5.67 行) 和 Amazon-Q-Dev (6.08 行) 比金标准补丁更精简
    
*   SWE-bench 的单元测试基于金标准补丁编写，当 Agent 修改了不同文件/函数但仍通过测试时，测试可能未覆盖 Agent 的实际改动，存在评测盲区
    

### 6.2 State-Diff 评测

**核心论文**："Agent-Diff: Benchmarking LLM Agents on Enterprise API Tasks via Code Execution with State-Diff-Based Evaluation" (Pysklo, Zhuravel & Watson, 2602.11224, Feb 2026)。

**方法论**：将每个服务建模为状态机，状态 S 为类型化关系数据库。在 Agent 执行前后分别快照所有数据库表，计算状态差异：

*   Delta\_add = T\_after \ T\_before（新增实体）
    
*   Delta\_del = T\_before \ T\_after（删除实体）
    
*   Delta\_mod = {(e, e') : e.pk = e'.pk ∧ e ≠ e'}（修改实体）
    

任务正确性通过声明式断言验证：每条断言指定 diff 类型、目标实体、字段约束和期望数量。关键设计是闭世界假设：状态差异中任何未被断言覆盖的变更（意外插入、删除或字段修改）视为副作用，任务得分归零。

**评测规模**：224 个任务，覆盖 Slack、Box、Linear、Google Calendar 四个企业服务，108 个 API 端点，平均任务跨度 5.3 次 API 调用。

---

## 七、意图对齐指标（需求→代码）

### 7.1 指令遵循率 (Instruction Following Rate)

**定义**：生成代码满足显式约束的比例。

**约束类型**：语言指定、复杂度限制、禁用 API、输出格式要求等。

**计算**：约束检查通过数 / 总约束数

**参考基准**：BigCodeBench (Zhuo et al., 2024) 评测复杂指令下的约束满足能力。

### 7.2 需求-代码对齐度 (SBC)

**核心论文**："Bridging LLM-Generated Code and Requirements: Reverse Generation Technique and SBC Metric for Developer Insights" (Ponnusamy, 2502.07835, Feb 2025)。

**方法论（逆向生成技术）**：

1.  给定需求描述，LLM 生成代码
    
2.  将生成的代码回传给 LLM，要求其从代码反向重建原始需求（逆向生成）
    
3.  将逆向生成的需求与原始需求进行对比，计算 SBC 分数
    

**SBC 公式**：

```plaintext
SBC = 0.7 × semantic_score + 0.1 × BLEU + 0.2 × completeness

```

三个组件：

*   **语义相似度 (权重 0.7)**：使用 `all-MiniLM-L6-v2` 句向量模型计算原始需求与逆向生成需求的余弦相似度，衡量意图层面的对齐
    
*   **BLEU (权重 0.1)**：标准 n-gram 匹配，作为语义相似度的补充
    
*   **完整性 (权重 0.2)**：提取两份需求中的关键词（名词、动词、专有名词），计算 `completeness = max(0, 1 - (|missing| + |extra|) / max(|keywords_union|, 1))`，其中 missing 为原始需求有但逆向生成缺失的关键词，extra 为逆向生成多出的关键词（可能暗示幻觉）
    

**权重设计理由**：语义相似度权重最高（0.7）因为它直接衡量意图对齐；完整性（0.2）惩罚功能遗漏和幻觉但不主导评分；BLEU（0.1）可靠性较低但提供互补信号。

**实验结果**：在 90 个需求（覆盖 UI/React/Angular、数据层/SQL、业务逻辑/.NET/Node.js/Spring Boot）上评测 Codellama 13B、Qwen2.5-Coder 14B、Deepseek Coder 6.7B、Codestral 22B 四个模型。SBC > 0.55 表示逆向生成需求质量较高，SBC > 0.65 表示语义高度接近。`missing_elements` 字段揭示被遗漏的需求组件，`extra_elements` 揭示幻觉内容。

---

## 参考文献

### 功能正确性与 Benchmark

1.  Chen et al., 2021. "Evaluating Large Language Models Trained on Code" (Codex/HumanEval). arXiv:2107.03374
    
2.  Liu et al., 2024. "Is Your Code Generated by ChatGPT Really Correct?" (EvalPlus). arXiv:2305.01210
    
3.  Gu et al., 2024. "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution". arXiv:2401.03065
    
4.  Jain et al., 2024. "LiveCodeBench: Holistic and Contamination Free Evaluation". arXiv:2403.07974
    
5.  Zhuo et al., 2024. "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls". arXiv:2406.15877
    
6.  Zhang et al., 2026. "EvoCodeBench: A Human-Performance Benchmark for Self-Evolving LLM-Driven Coding Systems". arXiv:2602.10171
    
    ### 工程质量与可读性
    
7.  Campbell, 2018. "Cognitive Complexity: An Overview and Evaluation". SonarSource
    
8.  Siddiq et al., 2024. "Using LLMs to Evaluate Code Readability"
    
9.  "AI builds, We Analyze: Empirical Study of AI-Generated Build Code Quality", Jan 2026. arXiv:2601.16839
    
10.  Licorish et al., 2025. "Comparing Human and LLM Generated Code: The Jury is Still Out!". arXiv:2501.16857
    
11.  Cotroneo, Improta & Liguori, 2025. "Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity" (ISSRE 2025). arXiv:2508.21634
    
12.  "Investigating The Smells of LLM Generated Code", Oct 2025. arXiv:2510.03029
    
13.  "A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code", Nov 2025. arXiv:2511.15817
    
14.  "On Inter-dataset Code Duplication and Data Leakage in Large Language Models", Jan 2024. arXiv:2401.07930
    
15.  Clark et al., 2024. "A Quantitative Analysis of Quality and Consistency in AI-generated Code". IEEE ICoSSE
    
16.  Fang et al., 2025. "RTLBench: Multi-dimensional Evaluation Framework for RTL Code". ICCD 2025
    
    ### 稳定性与一致性
    
17.  Ouyang et al., 2023. "An Empirical Study of the Non-determinism of ChatGPT in Code Generation". arXiv:2308.02828
    
18.  Donato et al., 2025. "Studying How Configurations Impact Code Generation in LLMs". arXiv:2502.17450
    
    ### 安全性与许可证合规
    
19.  Bhatt et al., 2024. "CyberSecEval 2" (Meta). arXiv:2404.13161
    
20.  Siddiq & Santos, 2022. "SecurityEval Dataset". arXiv:2212.09520
    
21.  "CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security", Feb 2026. arXiv:2602.03012
    
22.  "Can Adversarial Code Comments Fool AI Security Reviewers", Feb 2026. arXiv:2602.16741
    
23.  "Prompt Poisoning Code: Defect Induction Rates and Security Mitigation", Oct 2025. arXiv:2510.22944
    
24.  "Security and Quality in LLM-Generated Code: Multi-Language Analysis", Feb 2025. arXiv:2502.01853
    
25.  Xu et al., 2024. "LiCoEval: Evaluating LLMs on License Compliance in Code Generation" (ICSE 2025). arXiv:2408.02487
    
26.  Bifolco et al., 2025. "CodeGenLink: A Tool to Find the Likely Origin and License of Automatically Generated Code" (ASE 2025). arXiv:2510.01077
    
27.  Qu et al., 2025. "RepoMark: A Data-Usage Auditing Framework for Code Large Language Models". arXiv:2508.21432
    
28.  Jahanshahi & Mockus, 2025. "Cracks in The Stack: Hidden Vulnerabilities and Licensing Risks in LLM Pre-Training Datasets". arXiv:2501.02628
    
29.  Salerno et al., 2025. "How Much Do Code Language Models Remember?" (MSR 2025). arXiv:2501.17501
    
30.  He et al., 2026. "Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model". arXiv:2602.07120
    
31.  Jiang et al., 2025. "Large Language Model Unlearning for Source Code (PROD)" (AAAI 2026). arXiv:2506.17125
    
    ### 效率与成本
    
32.  Huang et al., 2024. "EffiBench: Benchmarking the Efficiency of Automatically Generated Code" (NeurIPS 2024 D&B). arXiv:2402.02037
    
33.  Qing et al., 2025. "EffiBench-X: Multi-Language Efficiency Benchmark". arXiv:2505.13004
    
34.  Ou et al., 2026. "MaxCode: Max-Reward RL for Code Optimization". arXiv:2601.05475
    
35.  Mikasa et al., 2026. "Improving HPC Code Generation Capability of LLMs via Online RL with Real-Machine Benchmark Rewards". arXiv:2602.12049
    
36.  Rahman et al., 2025. "MARCO: Multi-Agent Framework for HPC Code Optimization". arXiv:2505.03906
    
37.  "Cost-Aware Model Selection for Text Classification", Feb 2026. arXiv:2602.06370
    
38.  "EGSS: Entropy-guided Stepwise Scaling for Reliable SE", Feb 2026. arXiv:2602.05242
    
39.  "SWE-Replay: Efficient Test-Time Scaling for SE Agents", Jan 2026. arXiv:2601.22129
    
    ### 上下文适配与泛化
    
40.  "Evaluating AGENTS.md: Are Repository-Level Context Files Helpful?", Feb 2026. arXiv:2602.11988
    
41.  Cassano et al., 2023. "MultiPL-E: A Scalable and Polyglot Approach to Benchmarks". arXiv:2208.08227
    
42.  Zheng et al., 2023. "CodeGeeX: A Pre-Trained Model with Multilingual Benchmarking on HumanEval-X"
    
    ### 补丁与编辑
    
43.  Chen & Jiang, 2024. "Evaluating Software Development Agents: Patch Patterns, Code Quality, and Issue Complexity" (SANER 2025). arXiv:2410.12468
    
44.  Pysklo, Zhuravel & Watson, 2026. "Agent-Diff: Benchmarking LLM Agents on Enterprise API Tasks via State-Diff-Based Evaluation". arXiv:2602.11224
    
    ### Agentic Coding 与前沿
    
45.  "SWE-bench Mobile: Can LLMs Develop Industry-Level Mobile Applications?", Feb 2026. arXiv:2602.09540
    
46.  "ComUIBench", Feb 2026. arXiv:2602.19276
    
47.  "SimulatorCoder", Feb 2026. arXiv:2602.17169
    
    ### 意图对齐
    
48.  Ponnusamy, 2025. "Bridging LLM-Generated Code and Requirements: Reverse Generation Technique and SBC Metric". arXiv:2502.07835
    
49.  "From What to How: Bridging User Requirements with Software Development", Feb 2026. arXiv:2602.13611

> **[v3.0 新增]**
>
> ### v3.0 新增参考文献
>
> 50. Halstead, 1977. "Elements of Software Science". Elsevier
> 51. McConnell, 2004. "Code Complete" (2nd ed.), Chapter 20: The Software-Quality Landscape. Microsoft Press
> 52. Jones, 2008. "Applied Software Measurement" (3rd ed.). McGraw-Hill
> 53. Buse & Weimer, 2010. "Learning a Metric for Code Readability". IEEE TSE, 36(4):546-558
> 54. Hindle et al., 2012. "On the Naturalness of Software". ICSE 2012. DOI:10.1109/ICSE.2012.6227135
> 55. Scalabrino et al., 2018. "A Comprehensive Model for Code Readability". JSS, 145:240-259
> 56. "LLM-Specific Code Smells: A Taxonomy and Empirical Study", Dec 2025. arXiv:2512.18020