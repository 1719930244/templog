# AI 代码生成细粒度评测指标体系构建提案

> **版本**: v0.2 | **日期**: 2026-02-28 | **定位**: 面向 AI Coding 产品的细粒度代码评测指标体系构建方案，覆盖 SE 全生命周期与 Agent 交互场景，包含指标定义、适用粒度、实现路径、资源需求与可行性评估

---

## 指标卡片格式说明

本文档中每个指标采用统一的卡片格式：

- **定义 & 公式**：指标的精确定义和计算方式
- **适用粒度**：snippet（函数/片段级）/ file（文件级）/ repo（仓库级）
- **实现路径**：全自动 / 半自动（LLM-as-Judge）/ 人工标注驱动
- **工具链**：所需的开源工具、商业工具或 LLM API
- **数据需求**：评测数据集来源（开源复用 / 自建）
- **人力需求**：开发、标注、校准所需人力
- **落地难度**：★☆☆☆☆（极易）到 ★★★★★（极难）
- **优先级**：P0（必建）/ P1（重要）/ P2（可选）
- **参考文献**：核心论文与工具

---

## 〇、背景与目标

### 现状与局限

当前 AI Coding 评测以 pass@k 为核心，存在三个结构性问题：

1. **粒度过粗**：pass@k 是二值判定（通过/不通过），无法区分"差一点通过"和"完全错误"，也不衡量代码的工程质量、可维护性、安全性等非功能属性
2. **阶段单一**：主流 benchmark（HumanEval、MBPP、LiveCodeBench）集中在"代码生成"环节，忽略了需求理解、测试生成、Bug 修复、代码审查等 SE 生命周期的其他阶段
3. **脱离工程实践**：算法题场景与真实软件开发差距大，仓库级评测（SWE-bench）虽然更贴近实际，但评测维度仍以"是否解决 issue"为主

### 目标

构建一套覆盖 SE 全生命周期的细粒度代码评测指标体系，满足：

- **细粒度**：每个阶段有多个可量化的子指标，而非单一分数
- **可落地**：每个指标有明确的实现路径、工具链和资源预估
- **可复现**：优先采用自动化指标，人工评估部分有标准化流程
- **可扩展**：框架支持新指标的接入

### 生命周期框架总览

```
需求 ──→ 设计与实现 ──→ 测试 ──→ 维护
 │          │           │        │
 │    功能正确性      测试生成   Bug定位
 │    工程质量        覆盖率    补丁质量
 │    代码效率        变异测试   代码审查
 │    安全性          预言质量   重构质量
 │    许可证合规
 │
 需求对齐 / 指令遵循 / 跨模态理解

    ┌──── Agent / 交互式开发 ────┐
    │ 多轮交互 / 工具调用正确性  │
    │ 跨文件编辑 / 环境搭建能力  │
    │ 计划-执行-反思 / 长程依赖  │
    └───────────────────────────┘

    ┌────── 跨阶段通用指标 ──────┐
    │ 输出一致性 / 鲁棒性        │
    │ 数据污染 / 生成成本        │
    │ 价值观对齐 / 开发者体验    │
    └───────────────────────────┘
```

---

## 一、需求阶段指标

> 评测 AI 对自然语言需求的理解与转化能力。这一阶段的指标回答：生成的代码是否忠实反映了用户意图？

### 1.1 需求-代码语义对齐度 (SBC)

**定义 & 公式**：通过"逆向生成"技术评估代码对需求的忠实度。给定需求 R，模型生成代码 C，再由 LLM 从 C 反向重建需求 R'，从三个子维度分别评估 R 与 R' 的一致性：

```
SBC_semantic  = cosine_sim(embed(R), embed(R'))          # 语义相似度
SBC_coverage  = |matched_constraints| / |total_constraints(R)|  # 约束覆盖率
SBC_halluc    = |extra_constraints(R')| / max(|all_constraints(R')|, 1)  # 幻觉率（越低越好）

SBC = w₁·SBC_semantic + w₂·SBC_coverage - w₃·SBC_halluc
```

权重 w₁/w₂/w₃ 应通过标注小样本（≥50 对）做回归校准确定，而非预设固定值。约束提取采用结构化槽位（功能点、输入输出、异常处理、性能要求）而非简单关键词匹配。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 半自动：LLM 逆向生成 + embedding 计算 + 约束槽位提取 |
| 工具链 | sentence-transformers (all-MiniLM-L6-v2)、LLM API（逆向生成 + 约束提取）、spaCy |
| 数据需求 | 需求-代码对。可从 BigCodeBench（复杂指令）、SWE-bench（issue 描述）复用；需 ≥50 对标注样本做权重校准 |
| 人力需求 | 1 人开发 pipeline（约 1 周）+ 权重校准标注（约 2 人天） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P1 |
| 参考 | Ponnusamy, 2502.07835 |

### 1.2 指令遵循率 (Instruction Following Rate, IFR)

**定义 & 公式**：生成代码满足显式约束的比例。约束按可判定性分为三层：

```
IFR_hard    = 通过的硬约束数 / 硬约束总数      # AST/编译/导入/禁用 API 等可自动验证
IFR_runtime = 通过的运行时约束数 / 运行时约束总数  # 行为/性能/输出格式等需执行验证
IFR_soft    = 通过的软约束数 / 软约束总数        # 风格/偏好等需 LLM 判定

IFR = (IFR_hard × n_hard + IFR_runtime × n_runtime + IFR_soft × n_soft) / (n_hard + n_runtime + n_soft)
```

建议分别报告三类 IFR，并输出失败归因（具体哪条约束未通过）。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 全自动：规则引擎检查（正则匹配、AST 分析、import 检查） |
| 工具链 | 自研规则引擎 + tree-sitter（AST 解析），部分约束可用 LLM 判定 |
| 数据需求 | 带约束标注的 prompt 集。BigCodeBench 已有此类数据，可直接复用 |
| 人力需求 | 1 人开发规则引擎（约 2 周），约束类型可渐进扩展 |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P0 |
| 参考 | Zhuo et al., 2024 (BigCodeBench) |

### 1.3 需求分解与设计对齐

**定义**：评测模型将高层需求分解为设计方案（模块划分、接口定义、数据结构选择）的能力，以及设计方案与最终代码的一致性。

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 半自动：LLM-as-Judge 评估设计方案质量 + 代码-设计一致性检查 |
| 工具链 | LLM API（GPT-4 / Claude 评分）、DesBench 评测框架 |
| 数据需求 | 需求-设计-代码三元组。DesBench (2602.13611) 提供了初步数据集 |
| 人力需求 | 需专家定义评分 rubric（约 1 人周），LLM 评分后需 10-20% 抽检校准 |
| 落地难度 | ★★★☆☆ |
| 优先级 | P2 |
| 参考 | "From What to How", 2602.13611 |

### 1.4 需求歧义处理能力

**定义**：当需求描述存在歧义、不完整或矛盾时，模型是否能识别并主动澄清，而非盲目生成。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 人工标注驱动：需构建含歧义需求的测试集，人工判定模型响应是否合理 |
| 工具链 | 自建歧义需求数据集 + LLM-as-Judge 初筛 + 人工终审 |
| 数据需求 | 需自建。可从 Stack Overflow 的"需要更多信息"类问题中采集种子数据 |
| 人力需求 | 2-3 人标注团队（约 2-4 周），需定义歧义分类体系 |
| 落地难度 | ★★★★☆ |
| 优先级 | P2 |
| 参考 | 尚无成熟基准，属前沿方向 |

### 1.5 跨模态需求理解

**定义**：模型从非文本输入（UI 设计稿、流程图、数据库 ER 图）生成代码的能力。

| 要素 | 说明 |
|------|------|
| 适用粒度 | file |
| 实现路径 | 半自动：视觉相似度（截图对比）+ 功能测试 + LLM-as-Judge |
| 工具链 | Design2Code pipeline、Playwright/Puppeteer（截图）、SSIM/LPIPS（视觉相似度） |
| 数据需求 | 设计稿-代码对。ComUICoder (2602.19276) 提供前端 UI 数据集 |
| 人力需求 | 1-2 人开发 pipeline，视觉评估部分可自动化 |
| 落地难度 | ★★★☆☆ |
| 优先级 | P2 |
| 参考 | ComUICoder, 2602.19276; Design2Code, Si et al. |

---

## 二、设计与实现阶段指标

> 指标最密集的阶段。评测生成代码的功能正确性、工程质量、运行效率、安全性与合规性。

### 2.1 功能正确性

#### 2.1.1 pass@k 及变体

**定义 & 公式**：生成 n 个样本，c 个通过全部测试，从中取 k 个至少有一个正确的概率。

```
pass@k = E[1 - C(n-c, k) / C(n, k)]
```

变体包括：Enhanced-test pass@k（EvalPlus 增强测试用例后重新评估）、pass@t（多轮迭代修正）、自演进 pass@k（EvoCodeBench，含 Runtime Beats 和 Memory Beats 效率维度）。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 全自动：代码执行 + 测试用例判定 |
| 工具链 | 沙箱执行环境（Docker/Firejail）、EvalPlus 框架、LeetCode OJ API |
| 数据需求 | HumanEval/MBPP/LiveCodeBench/BigCodeBench/EvoCodeBench，均已开源 |
| 人力需求 | 1 人搭建执行环境（约 1 周），后续全自动 |
| 落地难度 | ★☆☆☆☆ |
| 优先级 | P0 |
| 参考 | Chen et al., 2107.03374; Liu et al., 2305.01210 (EvalPlus); Zhang et al., 2602.10171 (EvoCodeBench) |

#### 2.1.2 部分正确性

**定义 & 公式**：以通过的测试用例比例作为连续分数，替代 pass@k 的二值判定。

```
partial_score = 通过的测试用例数 / 总测试用例数
```

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 全自动：与 pass@k 共用执行环境，改为逐用例统计 |
| 工具链 | 同 pass@k，改造评分脚本即可 |
| 数据需求 | 同 pass@k，需确保每题测试用例数量充足（≥10） |
| 人力需求 | 0.5 人天（脚本改造） |
| 落地难度 | ★☆☆☆☆ |
| 优先级 | P0 |
| 参考 | 尚未被主流 benchmark 广泛采用，但实现成本极低 |

### 2.2 工程质量

#### 2.2.1 Lint 规范性

**定义 & 公式**：Linter 规则违反数归一化到代码规模。

```
lint_density = (warning_count + error_count) / (lines_of_code / 1000)
```

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file / repo |
| 实现路径 | 全自动：对生成代码运行项目配置的 Linter |
| 工具链 | Python: Pylint/Ruff; JS/TS: ESLint/Biome; Java: Checkstyle/PMD; 通用: SonarQube |
| 数据需求 | 无额外数据需求，直接对生成代码运行 |
| 人力需求 | 1 人配置 Linter 规则集（约 2-3 天），可复用社区推荐配置 |
| 落地难度 | ★☆☆☆☆ |
| 优先级 | P0 |
| 参考 | Licorish et al., 2501.16857; Cotroneo et al., 2508.21634 |

#### 2.2.2 复杂度指标

**圈复杂度 (Cyclomatic Complexity)**：

```
V(G) = E - N + 2P    （E: 边数, N: 节点数, P: 连通分量数）
阈值：1-10 简单, 11-20 中等, 21-50 复杂, >50 不可测试
```

**认知复杂度 (Cognitive Complexity)**：SonarSource 提出，三条规则：(1) 控制结构 +1；(2) 嵌套层级额外 +nesting level；(3) 跳转语句和逻辑运算符切换 +1。与圈复杂度的关键差异：switch 整体 +1 而非按 case 累加，else if 不受嵌套惩罚，递归 +1。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 全自动：静态分析工具直接计算 |
| 工具链 | radon (Python)、complexity-report (JS)、SonarQube（全语言认知复杂度）、lizard（多语言圈复杂度） |
| 数据需求 | 无额外数据需求 |
| 人力需求 | 0.5 人天（工具集成） |
| 落地难度 | ★☆☆☆☆ |
| 优先级 | P0 |
| 参考 | Campbell, 2018 (SonarSource) |

#### 2.2.3 可读性

**量化方法体系**：

- **Buse-Weimer 模型**：25 个结构特征（行长度、标识符平均长度、缩进深度、注释密度、字符熵等）训练逻辑回归，输出 [0,1] 概率分数，准确率约 80%
- **Scalabrino 扩展**：加入文本/语言学特征（WordNet 覆盖率、Flesch-Kincaid 指数、命名一致性），组合模型优于纯结构特征
- **LLM-as-Judge**：强模型按 1-5 分评分量表评估命名规范、注释质量、模块化程度

**工程阈值参考**：行长度 ≤ 80-120 字符，函数体 ≤ 30-50 行，标识符 3-25 字符，注释密度 15%-30%，嵌套深度 ≤ 4 层，函数参数 ≤ 5 个。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 混合：结构特征全自动提取 + LLM-as-Judge 评估主观维度 |
| 工具链 | 自研特征提取脚本（AST + 正则）、LLM API（用于 Judge）|
| 数据需求 | Buse-Weimer 原始数据集（100 个 Java 片段）可复用；LLM-as-Judge 需定义 rubric |
| 人力需求 | 1 人开发特征提取（1 周）+ 1 人设计 rubric 并校准（1 周） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P1 |
| 参考 | Buse & Weimer, 2010 (IEEE TSE); Scalabrino et al., 2018 (JSS) |

#### 2.2.4 代码异味密度

**定义 & 公式**：

```
smell_density = 检出的代码异味数 / (lines_of_code / 1000)
```

AI 特有异味模式：过度工程、冗余代码（未使用 import/变量）、不必要的抽象、硬编码调试残留。实证数据：约 60.9% 的 AI 生成代码单元包含至少一个异味（ENIAC 2024）；LLM 代码异味平均比人写代码增加 63.34%（2510.03029）。

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 全自动：静态分析工具检测 |
| 工具链 | SonarQube（异味检测）、Pylint（部分异味规则）、PMD（Java）、自定义 Semgrep 规则（AI 特有异味） |
| 数据需求 | 无额外数据需求；AI 特有异味规则需参考文献定义 |
| 人力需求 | 1 人配置规则 + 编写 AI 特有异味检测规则（约 1-2 周） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P1 |
| 参考 | 2510.03029; 2511.15817 (PSC); 2508.21634 (ISSRE 2025) |

#### 2.2.5 代码重复率

**两个维度**：

- **内部重复**：生成代码自身内部的重复片段比例（暗示模板化生成）
- **外部重复**：与已有代码库/训练数据的重复比例（暗示记忆而非理解）

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 全自动：克隆检测工具 |
| 工具链 | SonarQube（重复检测）、PMD-CPD（跨文件克隆检测）、jplag/moss（外部重复检测） |
| 数据需求 | 内部重复无额外需求；外部重复需参考代码库（如 The Stack v2 子集） |
| 人力需求 | 0.5 人天（工具配置） |
| 落地难度 | ★☆☆☆☆ |
| 优先级 | P1 |
| 参考 | 2401.07930; Cotroneo et al., 2508.21634 |

#### 2.2.6 风格一致性

**定义**：生成代码与项目现有代码风格的偏离度。

**量化方式**：编码标准符合度（Pylint 评分）、Halstead 指标一致性（跨生成结果的 Difficulty/Effort 方差）、PSC 异味倾向评分。

**Halstead 指标体系**（基于 n1 不同运算符数、n2 不同操作数数、N1 运算符总次数、N2 操作数总次数）：程序体积 V = N·log₂(n)，难度 D = (n1/2)·(N2/n2)，工作量 E = D·V。

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 全自动：静态分析 + 统计比较 |
| 工具链 | radon (Python Halstead)、multimetric（多语言）、项目 Linter 配置 |
| 数据需求 | 需项目参考代码作为风格基线 |
| 人力需求 | 1 人开发风格偏离度计算脚本（约 3-5 天） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P1 |
| 参考 | Clark et al., 2024 (IEEE ICoSSE); Halstead, 1977 |

#### 2.2.7 代码自然性

**定义 & 公式**：代码符合统计语言模型预期的程度。

```
交叉熵: H(c) = -(1/N) · Σᵢ log₂ P(tᵢ | t₁...tᵢ₋₁)
困惑度: PP = 2^{H(c)}
```

Java 代码交叉熵约 2.5-3.5 bits/token，英语文本约 7-8 bits/token。LLM 生成代码天然高自然性，但可能出现"熵衰减"（趋向统计最常见模式而非最优解）。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 全自动：语言模型计算困惑度 |
| 工具链 | 小型代码语言模型（如 CodeBERT、UniXcoder）计算 token 级概率 |
| 数据需求 | 无额外数据需求，模型自带概率输出 |
| 人力需求 | 1 人开发计算 pipeline（约 3-5 天） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P2 |
| 参考 | Hindle et al., 2012 (ICSE) |

#### 2.2.8 耦合与内聚

**定义**：衡量代码模块化程度，遵循"高内聚低耦合"原则。

- **传入耦合 (Ca)**：依赖该模块的其他模块数
- **传出耦合 (Ce)**：该模块依赖的其他模块数
- **CBO (Coupling Between Objects)**：类之间的耦合关系数
- **LCOM (Lack of Cohesion of Methods)**：方法间共享实例变量的程度。优秀 < 0.3，良好 0.3-0.5，需改进 > 0.5

| 要素 | 说明 |
|------|------|
| 适用粒度 | repo |
| 实现路径 | 全自动：OO 度量工具 |
| 工具链 | JDepend/NDepend (Java/.NET)、Understand (多语言)、radon (Python 部分指标) |
| 数据需求 | 需仓库级代码（单函数场景不适用） |
| 人力需求 | 0.5 人天（工具配置） |
| 落地难度 | ★★☆☆☆（仅适用于仓库级评测场景） |
| 优先级 | P2 |
| 参考 | Chidamber & Kemerer, 1994 (CK metrics) |

### 2.3 代码效率

#### 2.3.1 执行效率指标

**NET (Normalized Execution Time)**：

```
NET = T_generated / T_baseline
```

其中 T_baseline 应明确定义为以下之一（需在评测报告中声明）：
- 题库官方最优解（如 LeetCode 最优提交）
- 人类提交的 P50 或 P90 分位数
- 同题已知最优公开实现

NET < 1 表示生成代码比基线更快，NET > 1 表示更慢。测量时需控制方差：排除 JIT 预热（前 3 次运行丢弃）、固定随机种子、禁用缓存、取 5 次中位数。

**NTMU (Normalized Total Memory Usage)**：

```
NTMU = M_generated / M_baseline
```

基线定义同 NET。

**Runtime Beats / Memory Beats**：生成代码在运行时间/内存上超过人类提交的百分位（EvoCodeBench 引入，由 LeetCode OJ 直接报告）。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 全自动：沙箱执行 + 性能采集 |
| 工具链 | 沙箱环境（Docker + cgroups 资源限制）、time/perf（时间）、valgrind/tracemalloc（内存） |
| 数据需求 | 需参考实现作为基线。EffiBench（1,000 题）、EffiBench-X（多语言）均已开源 |
| 人力需求 | 1 人搭建性能采集 pipeline（约 1 周） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P0 |
| 参考 | Huang et al., 2402.02037 (EffiBench); Qing et al., 2505.13004 (EffiBench-X) |

### 2.4 安全性

#### 2.4.1 漏洞引入率

**定义 & 公式**：

```
vuln_rate = 含漏洞的生成样本数 / 总生成样本数 × 100%
```

按 CWE 分类统计（CWE-79 XSS、CWE-89 SQL 注入、CWE-787 越界写等），区分被动评测（检查输出）和主动评测（模型是否主动引入不安全模式）。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file / repo |
| 实现路径 | 全自动：SAST 工具扫描 |
| 工具链 | Bandit (Python 安全)、Semgrep (多语言模式匹配)、CodeQL (语义分析) |
| 数据需求 | SecurityEval 数据集可复用；CyberSecEval 2 评测框架可直接使用 |
| 人力需求 | 1 人配置扫描规则（约 3-5 天）；CodeQL 需要学习成本（约 1-2 周） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P0 |
| 参考 | Bhatt et al., 2024 (CyberSecEval 2); Siddiq & Santos, 2022 (SecurityEval) |

#### 2.4.2 安全编码标准合规率

**定义 & 公式**：

```
compliance_rate = 通过的规则检查数 / 适用规则总数 × 100%
可按严重级别加权：Critical ×3, Major ×2, Minor ×1
```

覆盖标准：OWASP Top 10（Web）、CWE Top 25（通用）、MISRA（嵌入式）、CERT（C/Java）。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file / repo |
| 实现路径 | 全自动：基于 SAST 工具的规则映射 |
| 工具链 | 同 2.4.1 + 标准规则映射表（CWE → Semgrep/CodeQL 规则） |
| 数据需求 | 无额外数据，规则集内置于工具 |
| 人力需求 | 1 人建立规则-标准映射（约 1 周） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P1 |
| 参考 | OWASP Top 10 (2021); CWE Top 25 (2024) |

#### 2.4.3 抗提示词注入能力

**定义**：在对抗性输入（恶意代码注释、误导性 docstring）下，模型仍能生成安全代码的能力。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 半自动：对抗样本生成 + SAST 扫描 + 人工审查 |
| 工具链 | 自研对抗 prompt 生成器 + SAST 工具链 |
| 数据需求 | 需自建对抗样本集。可参考 CyberSecEval 2 的 prompt injection 测试集 |
| 人力需求 | 1-2 人设计对抗样本（约 2 周），后续可自动化 |
| 落地难度 | ★★★☆☆ |
| 优先级 | P2 |
| 参考 | 2602.16741; 2510.22944 |

#### 2.4.4 供应链与依赖安全 [v0.2 新增]

**定义 & 公式**：AI 生成代码引入的依赖项中存在已知漏洞、恶意包或密钥泄露的风险。

```
dependency_vuln_rate = 含已知漏洞的依赖数 / 总依赖数 × 100%
secret_leak_rate = 含硬编码密钥/凭证的文件数 / 总生成文件数 × 100%
malicious_pkg_rate = 引入可疑/恶意包的样本数 / 总样本数 × 100%
```

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 全自动：SCA 工具扫描依赖 + 密钥检测 |
| 工具链 | Trivy/Grype/OSV-Scanner（依赖漏洞）、detect-secrets/gitleaks（密钥泄露）、Socket.dev（恶意包检测） |
| 数据需求 | 无额外数据需求，工具内置漏洞数据库（NVD/OSV） |
| 人力需求 | 1 人配置扫描工具（约 3-5 天） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P0 |
| 参考 | OSV (Google); Socket.dev; OWASP Dependency-Check |

### 2.5 许可证合规

**定义**：生成代码与受限许可证（GPL/AGPL）关联代码的相似度风险。

**量化方式**：代码相似度扫描（与 The Stack v2 等训练数据集比对），0.88%-2.01% 的 LLM 生成代码与已有开源代码存在"显著相似性"（LiCoEval, ICSE 2025）。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 半自动：代码相似度检测 + 许可证数据库查询 |
| 工具链 | MOSS/JPlag（相似度检测）、ScanCode-toolkit（许可证识别）、CodeGenLink（VS Code 插件） |
| 数据需求 | 需开源代码许可证数据库（ScanCode 内置 SPDX 数据库） |
| 人力需求 | 1 人搭建扫描 pipeline（约 1 周） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P1 |
| 参考 | Xu et al., 2408.02487 (LiCoEval); Bifolco et al., 2510.01077 (CodeGenLink) |

### 2.6 跨语言与跨领域泛化

**跨语言泛化**：主流语言（Python/JS/Java）与长尾语言（VB/COBOL/Kotlin）的性能差距。

```
cross_lang_ratio = pass@1_tail_language / pass@1_mainstream_language
```

**跨领域泛化**：通用代码 vs 领域特定代码（移动端、前端 UI、智能合约、硬件描述）的性能差距。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 全自动：在多语言/多领域 benchmark 上运行 pass@k |
| 工具链 | MultiPL-E（18+ 语言翻译）、HumanEval-X（6 语言对齐）、领域特定基准 |
| 数据需求 | MultiPL-E、SWE-bench Mobile、ComUIBench 等均已开源 |
| 人力需求 | 1 人配置多语言执行环境（约 1 周） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P1 |
| 参考 | Cassano et al., 2023 (MultiPL-E); 2602.09540 (SWE-bench Mobile) |

---

## 三、测试阶段指标

> 评测 AI 生成测试代码的质量。这一阶段的产物不是"被测代码"而是"测试代码"本身，评测逻辑与实现阶段不同。

### 3.1 测试生成基础质量

**Build Rate**：生成的测试代码能否编译/解析通过。

```
build_rate = 编译通过的测试数 / 总生成测试数
```

**Pass Rate**：生成的测试在正确实现上能否通过（不含误报）。

```
pass_rate = 在正确实现上通过的测试数 / 编译通过的测试数
```

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file / repo |
| 实现路径 | 全自动：编译 + 执行 |
| 工具链 | pytest/unittest (Python)、Jest (JS)、JUnit (Java)、沙箱执行环境 |
| 数据需求 | TestGenEval (Meta, 2410.00752) 提供 68,647 个函数级测试生成任务；也可基于现有代码库自建 |
| 人力需求 | 1 人搭建测试执行 pipeline（约 1 周） |
| 落地难度 | ★☆☆☆☆ |
| 优先级 | P0 |
| 参考 | Alagarsamy et al., 2023 (TestGen-LLM, Meta); Varshney et al., 2410.00752 (TestGenEval) |

### 3.2 代码覆盖率增益

**定义 & 公式**：AI 生成的测试在已有测试基础上带来的增量覆盖率提升。

```
Δ_coverage = coverage(existing_tests ∪ generated_tests) - coverage(existing_tests)
```

覆盖率类型：行覆盖率、分支覆盖率、MC/DC 覆盖率（安全关键领域）。

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 全自动：覆盖率工具采集 |
| 工具链 | coverage.py / pytest-cov (Python)、istanbul/c8 (JS)、JaCoCo (Java)、gcov (C/C++) |
| 数据需求 | 需被测项目代码 + 已有测试套件作为基线 |
| 人力需求 | 0.5 人天（工具集成） |
| 落地难度 | ★☆☆☆☆ |
| 优先级 | P0 |
| 参考 | Alagarsamy et al., 2023 (TestGen-LLM) |

### 3.3 变异测试分数 (Mutation Score)

**定义 & 公式**：通过向源代码注入人工缺陷（变异体），检验测试套件的缺陷检测能力。

```
mutation_score = 被杀死的变异体数 / 非等价变异体总数
```

变异算子包括：算术运算符替换（AOR）、关系运算符替换（ROR）、条件运算符替换（COR）、语句删除（SDL）等。

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 全自动：变异测试框架 |
| 工具链 | mutmut/cosmic-ray (Python)、Stryker (JS/TS)、PIT (Java)、mull (C/C++) |
| 数据需求 | 无额外数据需求，在被测代码上直接运行 |
| 人力需求 | 1 人配置变异测试框架（约 2-3 天），主要成本在计算资源（变异体数量级大） |
| 落地难度 | ★★☆☆☆（技术简单，但计算成本高） |
| 优先级 | P1 |
| 参考 | MuTAP, 2024 (LLM 辅助变异测试); Papadakis et al., 2019 (变异测试综述) |

### 3.4 测试预言质量

**定义**：生成的断言（assert）是否准确反映预期行为，而非仅检查"不抛异常"。

**量化方式**：

- **断言密度**：每个测试方法中的 assert 语句数
- **断言类型分布**：assertEqual/assertTrue/assertRaises 等的比例，过度依赖 assertTrue 通常意味着预言不精确
- **预言准确率**：在正确实现上断言通过、在缺陷实现上断言失败的比例

| 要素 | 说明 |
|------|------|
| 适用粒度 | file |
| 实现路径 | 半自动：AST 分析提取断言 + 变异测试验证预言有效性 |
| 工具链 | tree-sitter（断言提取）、变异测试框架（验证） |
| 数据需求 | 需正确实现 + 缺陷实现对（可从 Defects4J、BugsInPy 获取） |
| 人力需求 | 1 人开发断言分析脚本（约 3-5 天） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P1 |
| 参考 | Watson et al., 2020; Dinella et al., 2022 (TOGA) |

### 3.5 回归测试安全性

**定义 & 公式**：修改代码后，原本通过的测试是否仍然通过。

```
P2P_preserved_rate = 修改后仍通过的原有测试数 / 修改前通过的测试总数 × 100%
```

SWE-bench 要求 P2P 全部保持，这是极严格的标准。

| 要素 | 说明 |
|------|------|
| 适用粒度 | repo |
| 实现路径 | 全自动：修改前后分别运行测试套件 |
| 工具链 | 标准测试框架 + diff 比较脚本 |
| 数据需求 | SWE-bench / SWE-bench Verified 已内置 P2P 测试 |
| 人力需求 | 0.5 人天（脚本编写） |
| 落地难度 | ★☆☆☆☆ |
| 优先级 | P0 |
| 参考 | Jimenez et al., 2024 (SWE-bench) |

### 3.6 测试可维护性

**定义**：生成的测试代码是否易于理解和维护，而非"一次性"测试。

**量化维度**：测试命名规范性（是否描述测试意图）、测试结构（Arrange-Act-Assert 模式遵循度）、测试独立性（是否存在测试间依赖）、测试异味密度（魔法数字、硬编码路径等）。

| 要素 | 说明 |
|------|------|
| 适用粒度 | file |
| 实现路径 | 半自动：规则检查 + LLM-as-Judge |
| 工具链 | 自定义 Lint 规则（测试异味检测）、LLM API |
| 数据需求 | 需定义测试质量 rubric |
| 人力需求 | 1 人定义规则 + rubric（约 1 周） |
| 落地难度 | ★★★☆☆ |
| 优先级 | P2 |
| 参考 | Deursen et al., 2001 (测试异味); Varshney et al., 2410.00752 |

### 3.7 测试稳定性与可靠性 [v0.2 新增]

**定义 & 公式**：AI 生成测试的执行稳定性，排除不稳定测试（flaky test）对评测结果的干扰。

```
flaky_test_rate = 多次运行中结果不一致的测试数 / 总测试数 × 100%
false_positive_rate = 在正确实现上失败的测试数 / 总测试数 × 100%
determinism_score = 连续 N 次运行结果完全一致的测试比例（建议 N ≥ 5）
```

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 全自动：多次执行 + 结果比对 |
| 工具链 | pytest-repeat / JUnit RerunFailingTests + 自研比对脚本 |
| 数据需求 | 无额外数据需求，在被测代码上重复运行 |
| 人力需求 | 0.5 人天（脚本开发），主要成本在计算资源（多次执行） |
| 落地难度 | ★☆☆☆☆ |
| 优先级 | P0 |
| 参考 | TestGen-LLM (Meta, FSE 2024); Luo et al., 2014 (flaky test 研究) |

---

## 四、维护阶段指标

> 评测 AI 在代码维护场景中的能力：定位 Bug、生成补丁、辅助审查、执行重构。

### 4.1 Bug 定位准确率

**定义**：模型在给定 Bug 报告/失败测试后，定位到正确故障位置的能力。

**多粒度评估**：

```
文件级准确率 = 正确定位到目标文件的比例
函数级准确率 = 正确定位到目标函数的比例
行级准确率   = 正确定位到目标行的比例
```

实证数据（SANER 2025）：Agent 文件级对齐度高达 94.59% F1=1，但函数级骤降至 24.32%，说明能定位文件但常修改错误函数。

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 全自动：与金标准补丁位置对比 |
| 工具链 | git diff 解析 + 位置匹配脚本 |
| 数据需求 | SWE-bench Verified（500 个真实 GitHub issue + 金标准补丁） |
| 人力需求 | 1 人开发位置匹配脚本（约 2-3 天） |
| 落地难度 | ★☆☆☆☆ |
| 优先级 | P0 |
| 参考 | Chen & Jiang, 2410.12468 (SANER 2025) |

### 4.2 补丁质量

#### 4.2.1 补丁最小性

**定义 & 公式**：生成补丁的代码变更规模与金标准补丁的比值。

```
patch_bloat_ratio = agent_patch_lines / gold_patch_lines
```

实证数据：HoneyComb 平均 47.54 行 vs 金标准 7.71 行（bloat 6.2x）；Gru 仅 5.67 行，比金标准更精简。

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 全自动：diff 行数统计 |
| 工具链 | git diff --stat + 自定义统计脚本 |
| 数据需求 | SWE-bench 金标准补丁 |
| 人力需求 | 0.5 人天 |
| 落地难度 | ★☆☆☆☆ |
| 优先级 | P0 |
| 参考 | Chen & Jiang, 2410.12468 |

#### 4.2.2 State-Diff 评测

**定义**：将服务建模为状态机，在 Agent 执行前后快照数据库状态，通过声明式断言验证正确性。关键设计：闭世界假设，任何未被断言覆盖的状态变更视为副作用，得分归零。

```
Delta_add = T_after \ T_before     （新增实体）
Delta_del = T_before \ T_after     （删除实体）
Delta_mod = {(e,e') : e.pk=e'.pk ∧ e≠e'}  （修改实体）
```

| 要素 | 说明 |
|------|------|
| 适用粒度 | repo |
| 实现路径 | 全自动：数据库快照 + 断言验证 |
| 工具链 | Agent-Diff 框架（需搭建服务沙箱 + 数据库快照机制） |
| 数据需求 | Agent-Diff 提供 224 个任务（Slack/Box/Linear/Google Calendar） |
| 人力需求 | 2 人搭建服务沙箱环境（约 2-3 周），需要 API key 采买 |
| 落地难度 | ★★★★☆ |
| 优先级 | P2 |
| 参考 | Pysklo et al., 2602.11224 |

### 4.3 代码可维护性指标

#### 4.3.1 万行代码缺陷率

**定义 & 公式**：分别统计静态分析和动态测试发现的缺陷，避免混合计数导致语义不一致。

```
static_findings_density = 去重后的 SAST 告警数 / (代码行数 / 1000)   # 按 CWE/规则 ID 去重
test_failure_rate = 失败的测试用例数 / 总测试用例数
unique_bug_estimate = 去重后的缺陷实例数 / (代码行数 / 1000)        # 合并 SAST + 测试，按根因去重
```

注意：同一缺陷可能触发多条 SAST 告警或多个测试失败，需按根因（缺陷位置 + 类型）去重后再计算密度。

参考基线：行业平均发布后 1-25 缺陷/KLOC，高质量项目（CMMI Level 5）0.5-1.0 缺陷/KLOC，Cleanroom 最佳实践约 0.1 缺陷/KLOC。

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 全自动：SAST + 测试结果汇总 |
| 工具链 | SonarQube（缺陷检测）+ 测试框架（动态缺陷） |
| 数据需求 | 需大规模生成代码样本（≥10K LoC），模拟真实场景 |
| 人力需求 | 1 人搭建统计 pipeline（约 3-5 天） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P0 |
| 参考 | McConnell, 2004 (Code Complete); Cotroneo et al., 2508.21634 |

#### 4.3.2 可维护性指数 (MI)

```
MI = 171 - 5.2·ln(V) - 0.23·V(G) - 16.2·ln(LoC) + 50·sin(√(2.4·CM))
```

其中 V 为 Halstead 体积，V(G) 为圈复杂度，LoC 为代码行数，CM 为注释占比。MI 0-100 区间，> 85 良好，65-85 中等，< 65 难以维护。

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 全自动：组合已有指标计算 |
| 工具链 | radon (Python MI)、Visual Studio（内置 MI）、multimetric |
| 数据需求 | 无额外需求，组合 2.2.2 和 2.2.6 的输出 |
| 人力需求 | 0.5 人天（脚本编写） |
| 落地难度 | ★☆☆☆☆ |
| 优先级 | P1 |
| 参考 | Oman & Hagemeister, 1992; SEI, 2002 |

### 4.4 代码审查辅助质量

**定义**：AI 生成的代码审查意见的质量和有用性。

**量化维度**：

- **意见分类准确率**：Bug/Style/Design/Performance 分类是否正确
- **可操作性**：审查意见是否包含具体修改建议（而非泛泛而谈）
- **误报率**：指出的"问题"实际上是正确代码的比例

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 人工标注驱动：需专家评判审查意见质量 |
| 工具链 | AACR-Bench、SWR-Bench 框架 |
| 数据需求 | AACR-Bench (2601.19494) 提供来自 GitHub PR 的审查数据 |
| 人力需求 | 3-5 名开发者标注审查意见质量（约 2-4 周），需定义标注规范 |
| 落地难度 | ★★★★☆ |
| 优先级 | P2 |
| 参考 | AACR-Bench, 2601.19494; SWR-Bench, 2509.01494 |

### 4.5 重构质量

**定义**：AI 执行代码重构后，是否保持功能等价且改善了代码质量。

**量化维度**：

- **功能保持率**：重构前后所有测试仍通过的比例
- **质量改善度**：重构后的复杂度/异味数/可读性分数相对于重构前的变化
- **重构准确性**：是否正确应用了预期的重构模式（如 Extract Method、Rename）

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 半自动：测试回归验证 + 质量指标前后对比 + LLM-as-Judge |
| 工具链 | 测试框架 + SonarQube 前后对比 + RefactoringMiner（重构检测） |
| 数据需求 | TestRefactoring (FSE 2025) 提供初步数据集 |
| 人力需求 | 1 人开发前后对比 pipeline（约 1 周） |
| 落地难度 | ★★★☆☆ |
| 优先级 | P2 |
| 参考 | TestRefactoring (FSE 2025); Tsantalis et al. (RefactoringMiner) |

---

## 五、Agent 与交互式开发指标 [v0.2 新增]

> 评测 AI 在多轮交互、工具调用、仓库级变更等 Agent 场景下的综合能力。2025-2026 年前沿已从"单轮生成"转向"多轮交互 + 工具调用 + 代码库级变更"的综合评测。

### 5.1 多轮交互效率

**定义 & 公式**：Agent 在多轮对话中完成任务的效率与质量。

```
turn_efficiency = 成功完成的任务数 / 总交互轮数
task_completion_rate = 在 N 轮内完成的任务数 / 总任务数
```

| 要素 | 说明 |
|------|------|
| 适用粒度 | file / repo |
| 实现路径 | 全自动：Agent 执行日志分析 |
| 工具链 | Agent 框架日志 + 自研统计脚本 |
| 数据需求 | SWE-bench、CodeFlowBench 等多轮任务基准 |
| 人力需求 | 1 人开发日志分析 pipeline（约 1 周） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P0 |
| 参考 | CodeFlowBench; DevBench |

### 5.2 工具调用正确性

**定义 & 公式**：Agent 调用外部工具（文件读写、终端命令、搜索、API）的准确率。

```
tool_call_accuracy = 正确工具调用数 / 总工具调用数
tool_selection_precision = 选择了正确工具的次数 / 总工具选择次数
unnecessary_call_rate = 冗余/无效工具调用数 / 总工具调用数
```

| 要素 | 说明 |
|------|------|
| 适用粒度 | repo |
| 实现路径 | 半自动：日志解析 + 金标准对比 + LLM-as-Judge |
| 工具链 | Agent 执行日志 + 工具调用 schema 验证 |
| 数据需求 | 需构建带金标准工具调用序列的任务集 |
| 人力需求 | 1-2 人开发 + 标注（约 2 周） |
| 落地难度 | ★★★☆☆ |
| 优先级 | P1 |
| 参考 | CodeFlowBench; CODE2BENCH |

### 5.3 跨文件编辑一致性

**定义 & 公式**：Agent 在仓库级任务中跨多个文件进行编辑时，变更的内部一致性。

```
cross_file_consistency = 跨文件变更中无冲突的编辑数 / 总跨文件编辑数
import_consistency = 新增引用中可正确解析的比例
interface_consistency = 跨文件接口调用中签名匹配的比例
```

| 要素 | 说明 |
|------|------|
| 适用粒度 | repo |
| 实现路径 | 全自动：AST 解析 + 类型检查 + 编译验证 |
| 工具链 | tree-sitter（AST）、pyright/tsc（类型检查）、编译器 |
| 数据需求 | SWE-bench Verified、仓库级代码生成基准 |
| 人力需求 | 1 人开发一致性检查脚本（约 1 周） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P0 |
| 参考 | SWE-bench; DevBench |

### 5.4 环境搭建与依赖管理能力

**定义 & 公式**：Agent 正确配置项目环境、解决依赖冲突的能力。

```
build_success_rate = 成功构建的项目数 / 总项目数
dependency_resolution_rate = 正确解决依赖冲突的次数 / 总依赖冲突数
reproducible_build_rate = 重复构建结果一致的比例
```

| 要素 | 说明 |
|------|------|
| 适用粒度 | repo |
| 实现路径 | 全自动：CI/CD pipeline 执行 + 构建日志分析 |
| 工具链 | Docker（隔离环境）、pip/npm/gradle（依赖管理）、构建日志解析 |
| 数据需求 | 真实开源项目集（含 CI 配置），可从 SWE-bench 项目池采集 |
| 人力需求 | 2 人搭建多语言构建环境（约 2-3 周） |
| 落地难度 | ★★★☆☆ |
| 优先级 | P1 |
| 参考 | SWE-bench; SWE-bench Mobile (2602.09540) |

### 5.5 计划-执行-反思能力

**定义**：Agent 在复杂任务中制定计划、执行步骤、根据反馈调整策略的能力。

**量化维度**：
- **计划合理性**：LLM-as-Judge 评估计划的完整性和可行性（1-5 分）
- **执行偏离度**：实际执行步骤与计划的偏离程度
- **反思有效性**：遇到错误后修正策略的成功率

| 要素 | 说明 |
|------|------|
| 适用粒度 | repo |
| 实现路径 | 半自动：日志分析 + LLM-as-Judge |
| 工具链 | Agent 执行日志 + LLM API（评估计划质量） |
| 数据需求 | 需构建含多步骤的复杂任务集 |
| 人力需求 | 1-2 人设计评估 rubric + 开发分析脚本（约 2 周） |
| 落地难度 | ★★★☆☆ |
| 优先级 | P2 |
| 参考 | CodeFlowBench; Agent-Diff (2602.11224) |

---

## 六、跨阶段通用指标

> 不依赖特定 SE 阶段，衡量 AI 代码生成系统的基础特性。

### 6.1 输出一致性

**定义 & 公式**：相同 prompt 多次生成结果在功能和文本层面的稳定程度。

对同一 prompt 生成 m 次代码 {c₁, ..., cₘ}，在测试用例集 T = {t₁, ..., tₙ} 上运行，Rᵢⱼ ∈ {0,1} 表示第 i 次生成在第 j 个测试上是否通过：

```
功能一致性: FC = (1/n) · Σⱼ max(Σᵢ Rᵢⱼ, m - Σᵢ Rᵢⱼ) / m
  其中: m = 生成次数, n = 测试用例数, Rᵢⱼ = 第 i 次生成在第 j 个测试上的通过结果 (0/1)
  FC = 1 表示完全一致（所有生成在每个测试上结果相同），FC ≈ 0.5 表示高度随机
等价度量: Fleiss' κ，κ > 0.8 高一致性，κ < 0.4 低一致性
文本一致性: 多次生成结果间的编辑距离均值
```

关键发现：即使 temperature=0，ChatGPT 仍表现出显著非确定性（GPU 浮点运算导致）；top-p 对变异性的影响大于 temperature。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 全自动：多次采样 + 统计分析 |
| 工具链 | 自研采样脚本 + scipy（统计计算）|
| 数据需求 | 任意 benchmark，需多次独立运行（≥10 次） |
| 人力需求 | 0.5 人天（脚本开发），主要成本在 LLM API 调用费用 |
| 落地难度 | ★☆☆☆☆（技术简单，但 API 成本倍增） |
| 优先级 | P1 |
| 参考 | Ouyang et al., 2308.02828; Donato et al., 2502.17450 |

### 6.2 数据污染鲁棒性

**定义**：评测集是否被模型训练数据覆盖，导致评估结果虚高。

**检测方法**：

- **时间截止**：使用模型训练截止日期之后的数据（LiveCodeBench 策略）
- **n-gram 重叠检测**：检查评测 prompt 与已知训练集的文本重叠度
- **成员推断**：通过模型困惑度判断特定数据是否在训练集中

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 半自动：时间截止全自动，n-gram/成员推断需实验 |
| 工具链 | LiveCodeBench（持续更新）、自研 n-gram 检测脚本 |
| 数据需求 | LiveCodeBench 持续采集 LeetCode/Codeforces/AtCoder 新题 |
| 人力需求 | 1 人维护数据采集 pipeline（持续性工作） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P1 |
| 参考 | Jain et al., 2024 (LiveCodeBench); 2401.07930 |

### 6.3 生成成本

**定义**：完成任务所需的计算资源和经济成本。

**量化维度**：

- **Token 消耗**：input tokens + output tokens
- **API 费用**：按模型定价计算单任务成本
- **延迟**：首 token 延迟 (TTFT) + 总生成时间
- **成本效益比**：pass@1 / 单任务平均成本

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file / repo |
| 实现路径 | 全自动：API 调用日志统计 |
| 工具链 | API 调用封装层（记录 token 数和延迟）|
| 数据需求 | 无额外需求，从 API 响应中采集 |
| 人力需求 | 0.5 人天 |
| 落地难度 | ★☆☆☆☆ |
| 优先级 | P0 |
| 参考 | 2602.06370 (Cost-Aware Model Selection); 2602.05242 (EGSS) |

### 6.4 价值观对齐

**定义**：模型是否遵循安全策略，拒绝生成恶意代码、检测硬编码密钥和敏感信息泄露。

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 半自动：对抗 prompt 测试 + SAST 扫描敏感信息 |
| 工具链 | CyberSecEval 2（含 prompt injection 测试）、detect-secrets（硬编码密钥检测） |
| 数据需求 | CyberSecEval 2 测试集可复用 |
| 人力需求 | 1 人配置测试套件（约 3-5 天） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P1 |
| 参考 | Bhatt et al., 2024 (CyberSecEval 2) |

### 6.5 鲁棒性 [v0.2 新增]

**定义 & 公式**：模型对输入扰动（同义改写、无关上下文噪声、接口轻微变更）的功能保持能力。

```
paraphrase_robustness = 同义改写后仍通过测试的比例
noise_robustness = 注入无关上下文后仍通过测试的比例
api_change_robustness = API 签名轻微变更后仍能正确适配的比例
```

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file |
| 实现路径 | 半自动：LLM 生成扰动变体 + 自动执行测试 |
| 工具链 | LLM API（生成同义改写/噪声注入）、沙箱执行环境 |
| 数据需求 | 基于现有 benchmark 自动生成扰动版本 |
| 人力需求 | 1 人开发扰动生成 + 评测 pipeline（约 1 周） |
| 落地难度 | ★★☆☆☆ |
| 优先级 | P1 |
| 参考 | EvalPlus (2305.01210); CRUXEval (2401.03065) |

### 6.6 开发者体验与可用性 [v0.2 新增]

**定义**：AI Coding 产品在实际使用中的用户侧收益量化。

**量化维度**：

```
acceptance_rate = 用户接受 AI 建议的次数 / AI 给出建议的总次数
time_saving_ratio = (人工完成时间 - AI 辅助完成时间) / 人工完成时间
human_takeover_rate = 需要人工接管/大幅修改的任务比例
interaction_rounds = 完成任务的平均交互轮数
user_satisfaction = 用户满意度评分（1-5 分，问卷采集）
```

| 要素 | 说明 |
|------|------|
| 适用粒度 | snippet / file / repo |
| 实现路径 | 半自动：产品埋点 + 用户问卷 |
| 工具链 | 产品日志分析 + 问卷平台（如 Typeform） |
| 数据需求 | 需真实用户使用数据（A/B 测试或用户研究） |
| 人力需求 | 1-2 人设计实验 + 数据分析（约 2-3 周） |
| 落地难度 | ★★★☆☆ |
| 优先级 | P1 |
| 参考 | TestGen-LLM 73% 采纳率 (Meta, FSE 2024); GitHub Copilot 用户研究 |

---

## 七、落地资源总览

### 7.1 指标实现路径汇总表

| 编号 | 指标 | 阶段 | 实现路径 | 核心工具 | 数据来源 | 人力(人天) | 难度 | 优先级 |
|------|------|------|---------|---------|---------|-----------|------|--------|
| 1.1 | SBC 语义对齐度 | 需求 | 半自动 | sentence-transformers, sacrebleu | BigCodeBench/自建 | 5 | ★★ | P1 |
| 1.2 | 指令遵循率 IFR | 需求 | 全自动 | tree-sitter + 规则引擎 | BigCodeBench | 10 | ★★ | P0 |
| 1.3 | 需求分解与设计对齐 | 需求 | 半自动 | LLM-as-Judge | DesBench | 10 | ★★★ | P2 |
| 1.4 | 需求歧义处理 | 需求 | 人工标注 | 自建数据集 | 自建 | 20-30 | ★★★★ | P2 |
| 1.5 | 跨模态需求理解 | 需求 | 半自动 | Playwright, SSIM | ComUIBench | 10 | ★★★ | P2 |
| 2.1.1 | pass@k | 实现 | 全自动 | Docker 沙箱 | HumanEval/MBPP/LiveCodeBench | 5 | ★ | P0 |
| 2.1.2 | 部分正确性 | 实现 | 全自动 | 同 pass@k | 同 pass@k | 0.5 | ★ | P0 |
| 2.2.1 | Lint 规范性 | 实现 | 全自动 | Pylint/ESLint/SonarQube | 无 | 3 | ★ | P0 |
| 2.2.2 | 复杂度 | 实现 | 全自动 | radon/lizard/SonarQube | 无 | 0.5 | ★ | P0 |
| 2.2.3 | 可读性 | 实现 | 混合 | AST 特征 + LLM-as-Judge | Buse-Weimer 数据 | 10 | ★★ | P1 |
| 2.2.4 | 代码异味密度 | 实现 | 全自动 | SonarQube/Semgrep | 无 | 7-10 | ★★ | P1 |
| 2.2.5 | 代码重复率 | 实现 | 全自动 | PMD-CPD/SonarQube | 无 | 0.5 | ★ | P1 |
| 2.2.6 | 风格一致性 | 实现 | 全自动 | radon/multimetric | 项目参考代码 | 5 | ★★ | P1 |
| 2.2.7 | 代码自然性 | 实现 | 全自动 | CodeBERT/UniXcoder | 无 | 5 | ★★ | P2 |
| 2.2.8 | 耦合与内聚 | 实现 | 全自动 | JDepend/Understand | 仓库级代码 | 0.5 | ★★ | P2 |
| 2.3.1 | 执行效率 NET/NTMU | 实现 | 全自动 | Docker + perf/tracemalloc | EffiBench | 5 | ★★ | P0 |
| 2.4.1 | 漏洞引入率 | 实现 | 全自动 | Bandit/Semgrep/CodeQL | SecurityEval | 5 | ★★ | P0 |
| 2.4.2 | 安全标准合规率 | 实现 | 全自动 | SAST + 规则映射 | 无 | 5 | ★★ | P1 |
| 2.4.3 | 抗提示词注入 | 实现 | 半自动 | 自建对抗集 + SAST | CyberSecEval 2 | 10 | ★★★ | P2 |
| 2.5 | 许可证合规 | 实现 | 半自动 | ScanCode/MOSS | SPDX 数据库 | 5 | ★★ | P1 |
| 2.6 | 跨语言/领域泛化 | 实现 | 全自动 | MultiPL-E | 多语言 benchmark | 5 | ★★ | P1 |
| 3.1 | 测试生成质量 | 测试 | 全自动 | pytest/JUnit | TestGenEval | 5 | ★ | P0 |
| 3.2 | 覆盖率增益 | 测试 | 全自动 | coverage.py/JaCoCo | 被测项目 | 0.5 | ★ | P0 |
| 3.3 | 变异测试分数 | 测试 | 全自动 | mutmut/PIT/Stryker | 无 | 3 | ★★ | P1 |
| 3.4 | 测试预言质量 | 测试 | 半自动 | tree-sitter + 变异框架 | Defects4J/BugsInPy | 5 | ★★ | P1 |
| 3.5 | 回归测试安全性 | 测试 | 全自动 | 测试框架 | SWE-bench | 0.5 | ★ | P0 |
| 3.6 | 测试可维护性 | 测试 | 半自动 | Lint + LLM-as-Judge | 自定义 rubric | 5 | ★★★ | P2 |
| 4.1 | Bug 定位准确率 | 维护 | 全自动 | git diff + 匹配脚本 | SWE-bench | 3 | ★ | P0 |
| 4.2.1 | 补丁最小性 | 维护 | 全自动 | git diff --stat | SWE-bench | 0.5 | ★ | P0 |
| 4.2.2 | State-Diff 评测 | 维护 | 全自动 | Agent-Diff 框架 | Agent-Diff | 15-20 | ★★★★ | P2 |
| 4.3.1 | 缺陷密度 | 维护 | 全自动 | SonarQube + 测试框架 | 大规模生成样本 | 5 | ★★ | P0 |
| 4.3.2 | 可维护性指数 MI | 维护 | 全自动 | radon/multimetric | 无 | 0.5 | ★ | P1 |
| 4.4 | 代码审查质量 | 维护 | 人工标注 | AACR-Bench | AACR-Bench | 20-30 | ★★★★ | P2 |
| 4.5 | 重构质量 | 维护 | 半自动 | 测试框架 + SonarQube | TestRefactoring | 5 | ★★★ | P2 |
| **5.1** | **多轮交互效率** | **Agent** | **全自动** | **Agent 日志分析** | **SWE-bench/CodeFlowBench** | **5** | **★★** | **P0** |
| **5.2** | **工具调用正确性** | **Agent** | **半自动** | **日志解析 + LLM-as-Judge** | **自建** | **10** | **★★★** | **P1** |
| **5.3** | **跨文件编辑一致性** | **Agent** | **全自动** | **AST + 类型检查** | **SWE-bench** | **5** | **★★** | **P0** |
| **5.4** | **环境搭建与依赖管理** | **Agent** | **全自动** | **Docker + 构建日志** | **开源项目集** | **15** | **★★★** | **P1** |
| **5.5** | **计划-执行-反思** | **Agent** | **半自动** | **日志 + LLM-as-Judge** | **自建** | **10** | **★★★** | **P2** |
| 6.1 | 输出一致性 | 通用 | 全自动 | 多次采样 + scipy | 任意 benchmark | 0.5 | ★ | P1 |
| 6.2 | 数据污染鲁棒性 | 通用 | 半自动 | LiveCodeBench | LiveCodeBench | 5 | ★★ | P1 |
| 6.3 | 生成成本 | 通用 | 全自动 | API 日志 | 无 | 0.5 | ★ | P0 |
| 6.4 | 价值观对齐 | 通用 | 半自动 | CyberSecEval 2 | CyberSecEval 2 | 5 | ★★ | P1 |
| **2.4.4** | **供应链与依赖安全** | **实现** | **全自动** | **Trivy/Grype/detect-secrets** | **无** | **3-5** | **★★** | **P0** |
| **3.7** | **测试稳定性** | **测试** | **全自动** | **pytest-repeat** | **无** | **0.5** | **★** | **P0** |
| **6.5** | **鲁棒性** | **通用** | **半自动** | **LLM + 沙箱** | **现有 benchmark 扰动版** | **5** | **★★** | **P1** |
| **6.6** | **开发者体验** | **通用** | **半自动** | **产品埋点 + 问卷** | **用户数据** | **15** | **★★★** | **P1** |

### 7.2 工具链采买与搭建

**开源工具（零成本）**：

| 工具 | 覆盖指标 | 语言支持 |
|------|---------|---------|
| Pylint / Ruff | Lint、异味、编码标准 | Python |
| ESLint / Biome | Lint、异味 | JS/TS |
| SonarQube CE | Lint、异味、复杂度、重复、缺陷、MI | 30+ 语言 |
| Semgrep OSS | 安全漏洞、自定义异味规则 | 30+ 语言 |
| CodeQL | 语义安全分析 | 8 语言 |
| radon | 复杂度、Halstead、MI | Python |
| lizard | 圈复杂度 | 多语言 |
| PMD / PMD-CPD | 异味、重复检测 | Java/多语言 |
| tree-sitter | AST 解析（断言提取、特征提取） | 多语言 |
| mutmut / PIT / Stryker | 变异测试 | Python / Java / JS |
| coverage.py / JaCoCo / istanbul | 覆盖率 | Python / Java / JS |
| ScanCode-toolkit | 许可证识别 | 通用 |

**需付费/采买项**：

| 项目 | 用途 | 估算成本 |
|------|------|---------|
| LLM API（GPT-4 / Claude） | LLM-as-Judge、SBC 逆向生成、可读性评分 | 按量付费，预估 $500-2000/月 |
| SonarQube EE（可选） | 企业级多分支分析、更多规则 | $15K+/年 |
| Understand（可选） | 多语言 OO 度量（CBO/LCOM） | $800+/席 |
| 标注平台（如 Label Studio） | 人工标注管理 | 开源免费 |
| 云计算资源 | 沙箱执行、变异测试（CPU 密集） | 按量，预估 $200-500/月 |

### 7.3 数据集构建与标注

**可直接复用的开源数据集**：

| 数据集 | 阶段 | 规模 | 用途 |
|--------|------|------|------|
| HumanEval / MBPP | 实现 | 164 / 974 题 | pass@k 基线 |
| EvalPlus | 实现 | 增强测试版 HumanEval/MBPP | 增强 pass@k |
| LiveCodeBench | 实现 | 持续更新 | 抗污染 pass@k |
| BigCodeBench | 实现 | 1,140 题 | 指令遵循、复杂约束 |
| EffiBench / EffiBench-X | 实现 | 1,000 题 / 多语言 | 效率评测 |
| SecurityEval | 实现 | 130 题 | 安全漏洞 |
| SWE-bench Verified | 维护 | 500 issue | 补丁质量、Bug 定位 |
| TestGenEval | 测试 | 68,647 函数 | 测试生成 |
| Defects4J / BugsInPy | 测试 | 835 / 493 缺陷 | 测试预言验证 |
| MultiPL-E | 实现 | 18+ 语言 | 跨语言泛化 |

**需自建的数据集**：

| 数据集 | 阶段 | 标注内容 | 标注人力 | 建议方式 |
|--------|------|---------|---------|---------|
| 歧义需求集 | 需求 | 需求是否有歧义、歧义类型 | 2-3 人 × 2-4 周 | 从 SO "需要更多信息"类问题采集种子，LLM 扩增后人工审核 |
| 代码审查质量标注 | 维护 | 审查意见的类别、可操作性、准确性 | 3-5 人 × 2-4 周 | 从 GitHub PR 采集，定义 5 级评分量表 |
| AI 特有异味规则 | 实现 | Semgrep 自定义规则 | 1 人 × 1-2 周 | 参考文献 (2510.03029, 2512.18020) 定义规则 |

### 7.4 实施路线图 [v0.2 重构]

> v0.2 变更：从"按指标类型分阶段"改为"基础设施优先 + 关键路径驱动"，增加验收门槛和并行任务。

```
Phase 0（第 0-1 月）：基础设施与数据版本管理
├── 定义统一任务 schema（prompt/上下文/环境/采样配置/结果格式）
├── 搭建多语言沙箱执行环境（Docker 镜像：Python/JS/Java）
├── 建立结果存储与可追溯系统（schema + 日志 + 缓存）
├── 数据版本冻结 + 污染审计（时间切分 + n-gram 检测）
├── [并行] 启动人工标注规范设计与试标（歧义需求、审查质量）
├── 验收门槛：
│   ├── 沙箱执行成功率 ≥ 95%（Python/JS/Java 三语言）
│   ├── 超时率 ≤ 5%
│   └── 同配置重复运行结果差异 ≤ 1%
└── 产出：可复现的评测基础设施

Phase 1（第 1-3 月）：核心自动化指标 + Agent 基础指标
├── 集成 pass@k / 部分正确性 / 效率指标（NET/NTMU）
├── 配置 Lint / 复杂度 / 重复率 / 缺陷密度
├── 集成 SAST 安全扫描（Bandit + Semgrep）+ SCA 依赖安全（Trivy）
├── 搭建覆盖率 / 回归测试 / 测试稳定性 pipeline
├── 集成生成成本统计 + 输出一致性采样
├── [并行] Agent 多轮交互效率 + 跨文件编辑一致性
├── [并行] IFR 规则引擎开发
├── 验收门槛：
│   ├── ≥ 20 个全自动指标上线
│   ├── 指标间相关性分析完成（识别冗余）
│   └── 端到端评测报告可自动生成
└── 产出：核心指标体系 MVP

Phase 2（第 3-5 月）：半自动指标 + LLM-as-Judge 校准
├── 开发 SBC 逆向生成 pipeline + 权重校准
├── 开发可读性评估（结构特征 + LLM-as-Judge）
├── 配置风格一致性 / 代码自然性计算
├── 搭建许可证合规扫描
├── 编写 AI 特有异味 Semgrep 规则
├── Agent 工具调用正确性 + 计划-执行-反思评估
├── 鲁棒性评测（同义改写 + 噪声注入）
├── LLM-as-Judge 校准规范：
│   ├── 固定 Judge 配置（模型版本/system prompt/温度/采样次数）
│   ├── 采用 pairwise ranking + 多评委投票
│   ├── 记录 inter-rater reliability（κ/ICC）
│   └── 引入对抗样本防止 Judge 被诱导
├── 验收门槛：
│   ├── Judge 一致性 κ ≥ 0.7
│   └── 半自动指标与人工评估相关性 ≥ 0.8
└── 产出：~15 个半自动指标上线

Phase 3（第 5-7 月）：人工标注 + 体系完善 + 开发者体验
├── 完成歧义需求标注数据集
├── 完成代码审查质量标注数据集
├── 开发重构质量评测 pipeline
├── 集成跨语言/跨领域泛化评测（扩展至 6+ 语言）
├── 开发者体验指标采集（acceptance rate / time saving）
├── 全指标体系联调与报告生成
├── 指标依赖图构建（区分诊断指标 vs 汇总指标，避免重复计分）
├── 验收门槛：
│   ├── 完整指标体系覆盖 SE 全生命周期 + Agent 场景
│   ├── 一键评测报告生成时间 ≤ 4 小时
│   └── 标注数据集 inter-annotator agreement κ ≥ 0.75
└── 产出：完整指标体系上线
```

**总人力预估**：

| 阶段 | 人力 | 工作内容 |
|------|------|---------|
| Phase 0 | 2 人 × 1 月 | 基础设施 + 数据版本管理 + 标注规范设计 |
| Phase 1 | 2-3 人 × 2 月 | 核心自动化指标 + Agent 基础 + IFR |
| Phase 2 | 3 人 × 2 月 | 半自动指标 + Judge 校准 + 鲁棒性 |
| Phase 3 | 3-5 人 × 2 月 | 标注 + 泛化 + 开发者体验 + 联调 |
| 持续运营 | 1-2 人 | 数据更新（15% 月更新率）、指标维护、新指标接入 |

---

## 附录 A：指标-阶段映射总览 [v0.2 更新]

```
              需求    实现    测试    维护    Agent   通用
SBC            ●
IFR            ●
设计对齐        ●
歧义处理        ●
跨模态          ●
pass@k                 ●
部分正确性              ●
Lint                   ●
复杂度                  ●
可读性                  ●
代码异味                ●
重复率                  ●
风格一致性              ●
自然性                  ●
耦合内聚                ●
效率 NET/NTMU           ●
漏洞引入率              ●
安全合规率              ●
抗注入                  ●
供应链安全 [新]         ●
许可证合规              ●
跨语言泛化              ●
测试生成质量                    ●
覆盖率增益                      ●
变异测试                        ●
预言质量                        ●
回归安全性                      ●
测试可维护性                    ●
测试稳定性 [新]                 ●
Bug 定位                                ●
补丁最小性                              ●
State-Diff                              ●
缺陷密度                                ●
MI 指数                                 ●
审查质量                                ●
重构质量                                ●
多轮交互效率 [新]                               ●
工具调用正确性 [新]                             ●
跨文件编辑一致性 [新]                           ●
环境搭建能力 [新]                               ●
计划-执行-反思 [新]                             ●
输出一致性                                              ●
鲁棒性 [新]                                             ●
数据污染                                                ●
生成成本                                                ●
价值观对齐                                              ●
开发者体验 [新]                                         ●
```

## 附录 B：参考文献

### 功能正确性与 Benchmark
1. Chen et al., 2021. "Evaluating Large Language Models Trained on Code" (Codex/HumanEval). arXiv:2107.03374
2. Liu et al., 2024. "Is Your Code Generated by ChatGPT Really Correct?" (EvalPlus). arXiv:2305.01210
3. Gu et al., 2024. "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution". arXiv:2401.03065
4. Jain et al., 2024. "LiveCodeBench: Holistic and Contamination Free Evaluation". arXiv:2403.07974
5. Zhuo et al., 2024. "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls". arXiv:2406.15877
6. Zhang et al., 2026. "EvoCodeBench: A Human-Performance Benchmark for Self-Evolving LLM-Driven Coding Systems". arXiv:2602.10171

### 工程质量与可读性
7. Halstead, 1977. "Elements of Software Science". Elsevier
8. Buse & Weimer, 2010. "Learning a Metric for Code Readability". IEEE TSE, 36(4):546-558
9. Hindle et al., 2012. "On the Naturalness of Software". ICSE 2012
10. Campbell, 2018. "Cognitive Complexity: An Overview and Evaluation". SonarSource
11. Scalabrino et al., 2018. "A Comprehensive Model for Code Readability". JSS, 145:240-259
12. Clark et al., 2024. "A Quantitative Analysis of Quality and Consistency in AI-generated Code". IEEE ICoSSE
13. Licorish et al., 2025. "Comparing Human and LLM Generated Code". arXiv:2501.16857
14. Cotroneo et al., 2025. "Human-Written vs. AI-Generated Code" (ISSRE 2025). arXiv:2508.21634
15. "Investigating The Smells of LLM Generated Code", Oct 2025. arXiv:2510.03029
16. "A Causal Perspective on Code Smells in LLM-Generated Code", Nov 2025. arXiv:2511.15817
17. "LLM-Specific Code Smells", Dec 2025. arXiv:2512.18020
18. "AI builds, We Analyze", Jan 2026. arXiv:2601.16839

### 效率
19. Huang et al., 2024. "EffiBench" (NeurIPS 2024 D&B). arXiv:2402.02037
20. Qing et al., 2025. "EffiBench-X: Multi-Language Efficiency Benchmark". arXiv:2505.13004

### 安全性与许可证合规
21. Siddiq & Santos, 2022. "SecurityEval Dataset: Mining Vulnerability Examples to Evaluate Machine Learning-Based Code Generation Techniques". MSR4P&S@ESEC/FSE 2022. DOI:10.1145/3549035.3561184
22. Bhatt et al., 2024. "CyberSecEval 2" (Meta). arXiv:2404.13161
23. Xu et al., 2024. "LiCoEval" (ICSE 2025). arXiv:2408.02487
24. Bifolco et al., 2025. "CodeGenLink" (ASE 2025). arXiv:2510.01077
25. "Prompt Poisoning Code", Oct 2025. arXiv:2510.22944
26. "Can Adversarial Code Comments Fool AI Security Reviewers", Feb 2026. arXiv:2602.16741
27. "CVE-Factory", Feb 2026. arXiv:2602.03012

### 稳定性与一致性
28. Ouyang et al., 2023. "Non-determinism of ChatGPT in Code Generation". arXiv:2308.02828
29. Donato et al., 2025. "How Configurations Impact Code Generation". arXiv:2502.17450

### 测试
30. Alagarsamy et al., 2023. "TestGen-LLM" (Meta)
31. Varshney et al., 2026. "TestGenEval". arXiv:2410.00752

### 补丁与维护
32. Chen & Jiang, 2024. "Evaluating Software Development Agents" (SANER 2025). arXiv:2410.12468
33. Pysklo et al., 2026. "Agent-Diff". arXiv:2602.11224
34. McConnell, 2004. "Code Complete" (2nd ed.). Microsoft Press
35. Oman & Hagemeister, 1992. "Metrics for Assessing a Software System's Maintainability"

### 需求与意图对齐
36. Ponnusamy, 2025. "Bridging LLM-Generated Code and Requirements" (SBC). arXiv:2502.07835
37. "From What to How", Feb 2026. arXiv:2602.13611

### 跨语言与跨领域
38. Cassano et al., 2023. "MultiPL-E". arXiv:2208.08227
39. "SWE-bench Mobile", Feb 2026. arXiv:2602.09540
40. "ComUIBench", Feb 2026. arXiv:2602.19276

### 代码审查
41. "AACR-Bench", Feb 2026. arXiv:2601.19494
42. "SWR-Bench", Feb 2026. arXiv:2509.01494

### 成本与效率
43. "Cost-Aware Model Selection", Feb 2026. arXiv:2602.06370
44. "EGSS: Entropy-guided Stepwise Scaling", Feb 2026. arXiv:2602.05242

### 数据污染
45. "On Inter-dataset Code Duplication and Data Leakage", Jan 2024. arXiv:2401.07930
