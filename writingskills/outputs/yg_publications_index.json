{
  "generated_at": "2026-02-10T01:57:15.573579+00:00",
  "source_publications_url": "https://ntdxyg.github.io/publications/",
  "paper_count": 39,
  "papers": [
    {
      "id": "9a47a86c3d20",
      "page_id": "9712127",
      "title": "Fine-grained Pseudo-code Generation Method via Code Feature Extraction and Transformer",
      "year": 2021,
      "abbr": "APSEC'21",
      "venue": "Proceedings of the 28th Asia-Pacific Software Engineering Conference",
      "type": "conference",
      "tags": [
        "EI",
        "CCF-C"
      ],
      "doi": "10.1109/APSEC53868.2021.00029",
      "url": null,
      "code": "https://github.com/NTDXYG/DeepPseudo",
      "abstract": "Pseudo-code written by natural language is helpful for novice developers' program comprehension. However, writing such pseudo-code is time-consuming and laborious. Motivated by the research advancements of sequence-to-sequence learning and code semantic learning, we propose a novel deep pseudo-code generation method DeepPseudo via code feature extraction and Transformer. In particular, DeepPseudo utilizes a Transformer encoder to perform encoding for source code and then use a code feature extractor to learn the knowledge of local features. Finally, it uses a pseudo-code generator to perform decoding, which can generate the corresponding pseudo-code. We choose two corpora (i.e., Django and SPoC) from real-world large-scale projects as our empirical subjects. We first compare DeepPseudo with seven state-of-the-art baselines from pseudo-code generation and neural machine translation domains in terms of four performance measures. Results show the competitiveness of DeepPseudo. Moreover, we also analyze the rationality of the component settings in DeepPseudo.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": false,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W3186152447",
        "doi": "https://doi.org/10.1109/apsec53868.2021.00029",
        "display_name": "Fine-grained Pseudo-code Generation Method via Code Feature Extraction and Transformer",
        "publication_year": 2021,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": "Pseudo-code written by natural language is helpful for novice developers' program comprehension. However, writing such pseudo-code is time-consuming and laborious. Motivated by the research advancements of sequence-to-sequence learning and code semantic learning, we propose a novel deep pseudo-code generation method DeepPseudo via code feature extraction and Transformer. In particular, DeepPseudo utilizes a Transformer encoder to perform encoding for source code and then use a code feature extractor to learn the knowledge of local features. Finally, it uses a pseudo-code generator to perform decoding, which can generate the corresponding pseudo-code. We choose two corpora (i.e., Django and SPoC) from real-world large-scale projects as our empirical subjects. We first compare DeepPseudo with seven state-of-the-art baselines from pseudo-code generation and neural machine translation domains in terms of four performance measures. Results show the competitiveness of DeepPseudo. Moreover, we also analyze the rationality of the component settings in DeepPseudo.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "017cc07844c0",
      "page_id": "9622968",
      "title": "ComFormer: Code Comment Generation via Transformer and Fusion Method-based Hybrid Code Representation",
      "year": 2021,
      "abbr": "DSA'21",
      "venue": "Proceedings of the 8th International Conference on Dependable Systems and Their Applications",
      "type": "conference",
      "tags": [
        "EI"
      ],
      "doi": "10.1109/DSA52907.2021.00013",
      "url": null,
      "code": "https://github.com/NTDXYG/ComFormer",
      "abstract": "Developers often write low-quality code comments due to the lack of programming experience, which can reduce the efficiency of developers' program comprehension. Therefore, developers hope that code comment generation tools can be developed to illustrate the functionality and purpose of the code. Recently, researchers mainly model this problem as the neural machine translation problem and tend to use deep learning-based methods. In this study, we propose a novel method ComFormer based on Transformer and fusion method-based hybrid code presentation. Moreover, to alleviate OOV (out-of-vocabulary) problem and speed up model training, we further utilize the Byte-BPE algorithm to split identifiers and Sim_SBT method to perform AST Traversal. We compare ComFormer with seven state-of-the-art baselines from code comment generation and neural machine translation domains. Comparison results show the competitiveness of ComFormer in terms of three performance measures. Moreover, we perform a human study to verify that ComFormer can generate high-quality comments.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": false,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W3182190622",
        "doi": "https://doi.org/10.1109/dsa52907.2021.00013",
        "display_name": "ComFormer: Code Comment Generation via Transformer and Fusion Method-based Hybrid Code Representation",
        "publication_year": 2021,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": "Developers often write low-quality code comments due to the lack of programming experience, which can reduce the efficiency of developers' program comprehension. Therefore, developers hope that code comment generation tools can be developed to illustrate the functionality and purpose of the code. Recently, researchers mainly model this problem as the neural machine translation problem and tend to use deep learning-based methods. In this study, we propose a novel method ComFormer based on Transformer and fusion method-based hybrid code presentation. Moreover, to alleviate OOV (out-of-vocabulary) problem and speed up model training, we further utilize the Byte-BPE algorithm to split identifiers and Sim_SBT method to perform AST Traversal. We compare ComFormer with seven state-of-the-art baselines from code comment generation and neural machine translation domains. Comparison results show the competitiveness of ComFormer in terms of three performance measures. Moreover, we perform a human study to verify that ComFormer can generate high-quality comments.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "37b76d51c840",
      "page_id": "9623014",
      "title": "EKD-BSP: Bug Report Severity Prediction by Extracting Keywords from Description",
      "year": 2021,
      "abbr": "DSA'21",
      "venue": "Proceedings of the 8th International Conference on Dependable Systems and Their Applications",
      "type": "conference",
      "tags": [
        "EI"
      ],
      "doi": "10.1109/DSA52907.2021.00014",
      "url": null,
      "code": null,
      "abstract": "Bug severity is important for triagers. Recently, the text in the summary field (i.e., bug summary) of bug reports is usually used to extract features, and then bug report severity prediction models are constructed. In some bug reports, the bug summary may not contain enough useful information. While the text field in the description (i.e., bug description) of bug reports contains detailed information of the bug (e.g., steps to reproduce the bug, stack traces, and expected behavior). However, the bug description may contain irrelevant information. Motivated by the above findings, we propose a novel method EKD-BSP (Bug Report Severity Prediction by Extracting Keywords from Description), which uses the bug summary and the keywords extracted from the bug description to perform severity prediction. Our empirical study selects two large-scale open-source projects (i.e., Eclipse and Mozilla) as the empirical subjects. The empirical results show that EKD-BSP can improve the performance of F-measure by up to 5.19% after compared with the baselines.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4206032889",
        "doi": "https://doi.org/10.1109/dsa52907.2021.00014",
        "display_name": "EKD-BSP: Bug Report Severity Prediction by Extracting Keywords from Description",
        "publication_year": 2021,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": "Bug severity is important for triagers. Recently, the text in the summary field (i.e., bug summary) of bug reports is usually used to extract features, and then bug report severity prediction models are constructed. In some bug reports, the bug summary may not contain enough useful information. While the text field in the description (i.e., bug description) of bug reports contains detailed information of the bug (e.g., steps to reproduce the bug, stack traces, and expected behavior). However, the bug description may contain irrelevant information. Motivated by the above findings, we propose a novel method EKD-BSP (Bug Report Severity Prediction by Extracting Keywords from Description), which uses the bug summary and the keywords extracted from the bug description to perform severity prediction. Our empirical study selects two large-scale open-source projects (i.e., Eclipse and Mozilla) as the empirical subjects. The empirical results show that EKD-BSP can improve the performance of F-measure by up to 5.19% after compared with the baselines.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "19893a4e0c50",
      "page_id": "DBLP:conf/seke/YangZYC21",
      "title": "DeepSCC: Source Code Classification Based on Fine-Tuned RoBERTa (S)",
      "year": 2021,
      "abbr": "SEKE'21",
      "venue": "Proceedings of the 33rd International Conference on Software Engineering and Knowledge Engineering",
      "type": "conference",
      "tags": [
        "EI",
        "CCF-C"
      ],
      "doi": "10.18293/SEKE2021-005",
      "url": "https://doi.org/10.18293/SEKE2021-005",
      "code": "https://github.com/NTDXYG/DeepSCC",
      "abstract": "In software engineering-related tasks (such as programming language tag prediction based on code snippets from Stack Overflow), the programming language classification for code snippets is a common task. In this study, we propose a novel method DeepSCC, which uses a fine-tuned RoBERTa model to classify the programming language type of the source code. In our empirical study, we choose a corpus collected from Stack Overflow, which contains 224,445 pairs of code snippets and corresponding language types. After comparing nine state-of-the-art baselines from the fields of source code classification and neural text classification in terms of four performance measures (i.e., Accuracy, Precision, Recall, and F1), we show the competitiveness of our proposed method DeepSCC.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W3198379461",
        "doi": "https://doi.org/10.18293/seke2021-005",
        "display_name": "DeepSCC: Source Code Classification Based on Fine-Tuned RoBERTa (S)",
        "publication_year": 2021,
        "host_venue": null,
        "is_oa": true,
        "oa_url": "https://doi.org/10.18293/seke2021-005",
        "best_oa_location": {
          "id": "doi:10.18293/seke2021-005",
          "is_oa": true,
          "landing_page_url": "https://doi.org/10.18293/seke2021-005",
          "pdf_url": "https://doi.org/10.18293/seke2021-005",
          "source": {
            "id": "https://openalex.org/S4220650826",
            "display_name": "Proceedings/Proceedings of the ... International Conference on Software Engineering and Knowledge Engineering",
            "issn_l": "2325-9000",
            "issn": [
              "2325-9000",
              "2325-9086"
            ],
            "is_oa": false,
            "is_in_doaj": false,
            "is_core": true,
            "host_organization": null,
            "host_organization_name": null,
            "host_organization_lineage": [],
            "host_organization_lineage_names": [],
            "type": "journal"
          },
          "license": "cc-by",
          "license_id": "https://openalex.org/licenses/cc-by",
          "version": "publishedVersion",
          "is_accepted": true,
          "is_published": true,
          "raw_source_name": "International Conferences on Software Engineering and Knowledge Engineering",
          "raw_type": "proceedings-article"
        },
        "abstract": "In software engineering-related tasks (such as programming language tag prediction based on code snippets from Stack Overflow), the programming language classification for code snippets is a common task.In this study, we propose a novel method DeepSCC, which uses a fine-tuned RoBERTa model to classify the programming language type of the source code.In our empirical study, we choose a corpus collected from Stack Overflow, which contains 224,445 pairs of code snippets and corresponding language types.After comparing nine state-of-the-art baselines from the fields of source code classification and neural text classification in terms of four performance measures (i.e., Accuracy, Precision, Recall, and F1), we show the competitiveness of our proposed method DeepSCC.",
        "error": null
      },
      "pdf": {
        "url": "https://doi.org/10.18293/seke2021-005",
        "source": "openalex",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\Desktop\\codes\\å†™ä½œå‚è€ƒ\\papers\\pdfs_yg\\2021_SEKE'21_DeepSCC_ Source Code Classification Based on Fine-Tuned RoBERTa (S)_19893a4e0c50.pdf",
        "preview_text_path": "C:\\Users\\daoge\\Desktop\\codes\\å†™ä½œå‚è€ƒ\\papers\\extracted_yg\\2021_SEKE'21_DeepSCC_ Source Code Classification Based on Fine-Tuned RoBERTa (S)_19893a4e0c50.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "1d2cd6727805",
      "page_id": "Yang2021deep",
      "title": "åŸºäºæ·±åº¦å­¦ä¹ çš„ Stack Overflow é—®é¢˜å¸–åˆ†ç±»æ–¹æ³•",
      "year": 2021,
      "abbr": "å‰æ—å¤§å­¦å­¦æŠ¥ (ç†å­¦ç‰ˆ)'21",
      "venue": "å‰æ—å¤§å­¦å­¦æŠ¥ (ç†å­¦ç‰ˆ)",
      "type": "journal",
      "tags": [
        "åŒ—æ ¸"
      ],
      "doi": "10.13413/j.cnki.jdxblxb.2020165",
      "url": null,
      "code": null,
      "abstract": "The classification methods based on regular expressions and traditional machine learning had the problems of manual extraction of patterns and performance bottleneck, we proposed deep learning-based classification methods for question post, the deep text mining model TextCNN and integrating attention mechanismâ€”TextRNN were used to construct a classification model. The experimental results show that the classification performance of deep learning-based methods is better than the existing benchmark methods on most of the question purpose categories, and the Adam optimizer is better than the SGD optimizer, and the Glove pre-trained word vector is better than randomly generated word vectors. The method classifies posts for the purpose of asking question, which can add a new dimension to the analysis of post discussion topics on Stack Overflow (SO).",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": false,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "host_venue": null,
        "is_oa": null,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "7bad0f93ebfc",
      "page_id": "JSYJ202106040",
      "title": "ORESP:åŸºäºæœ‰åºå›å½’çš„è½¯ä»¶ç¼ºé™·ä¸¥é‡ç¨‹åº¦é¢„æµ‹æ–¹æ³•",
      "year": 2021,
      "abbr": "è®¡ç®—æœºåº”ç”¨ç ”ç©¶'21",
      "venue": "è®¡ç®—æœºåº”ç”¨ç ”ç©¶",
      "type": "journal",
      "tags": [
        "åŒ—æ ¸"
      ],
      "doi": "10.19734/j.issn.1001-3695.2020.07.0249",
      "url": null,
      "code": null,
      "abstract": "ä¸ºæé«˜è½¯ä»¶ç¼ºé™·ä¸¥é‡ç¨‹åº¦çš„é¢„æµ‹æ€§èƒ½ï¼Œé€šè¿‡å……åˆ†è€ƒè™‘è½¯ä»¶ç¼ºé™·ä¸¥é‡ç¨‹åº¦æ ‡ç­¾é—´çš„æ¬¡åºæ€§ï¼Œæå‡ºä¸€ç§åŸºäºæœ‰åºå›å½’çš„è½¯ä»¶ç¼ºé™·ä¸¥é‡ç¨‹åº¦é¢„æµ‹æ–¹æ³•ORESPã€‚è¯¥æ–¹æ³•é¦–å…ˆä½¿ç”¨åŸºäºSpearmançš„ç‰¹å¾é€‰æ‹©æ–¹æ³•æ¥è¯†åˆ«å¹¶ç§»é™¤æ•°æ®é›†å†…çš„å†—ä½™ç‰¹å¾ï¼Œéšåä½¿ç”¨åŸºäºæ¯”ä¾‹ä¼˜åŠ¿æ¨¡å‹çš„ç¥ç»ç½‘ç»œæ¥æ„å»ºé¢„æµ‹æ¨¡å‹ã€‚é€šè¿‡ä¸äº”ç§ç»å…¸åˆ†ç±»æ–¹æ³•çš„æ¯”è¾ƒï¼Œæ‰€æçš„ORESPæ–¹æ³•åœ¨å››ç§ä¸åŒç±»å‹çš„åº¦é‡ä¸‹å‡å¯å–å¾—æ›´é«˜çš„é¢„æµ‹æ€§èƒ½ï¼Œå…¶ä¸­åŸºäºå¹³å‡0-1è¯¯å·®ï¼ˆMZEï¼‰è¯„æµ‹æŒ‡æ ‡ï¼Œé¢„æµ‹æ¨¡å‹æ€§èƒ½æœ€å¤§å¯æå‡10.3%ï¼›åŸºäºå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰è¯„æµ‹æŒ‡æ ‡ï¼Œé¢„æµ‹æ¨¡å‹æ€§èƒ½æœ€å¤§å¯æå‡12.3%ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå‘ç°ä½¿ç”¨åŸºäºSpearmançš„ç‰¹å¾é€‰æ‹©æ–¹æ³•å¯ä»¥æœ‰æ•ˆæå‡ORESPæ–¹æ³•çš„é¢„æµ‹æ€§èƒ½ã€‚",
      "abstract_signals": {
        "lang": "zh",
        "has_numbers": true,
        "has_propose": true,
        "has_results": false
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "host_venue": null,
        "is_oa": null,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "87810cb8a88a",
      "page_id": "RJXB202107011",
      "title": "ä»£ç æ³¨é‡Šè‡ªåŠ¨ç”Ÿæˆæ–¹æ³•ç»¼è¿°",
      "year": 2021,
      "abbr": "è½¯ä»¶å­¦æŠ¥'21",
      "venue": "è½¯ä»¶å­¦æŠ¥",
      "type": "journal",
      "tags": [
        "EI",
        "ä¸­æ–‡CCF-A",
        "åŒ—æ ¸"
      ],
      "doi": "10.13328/j.cnki.jos.006258",
      "url": null,
      "code": null,
      "abstract": "During software development and maintenance, code comments often have some problems, such as missing, insufficient, or mismatching with code content. Writing high-quality code comments takes time and effort for developers, and the quality can not be guaranteed, therefore, it is urgent for researchers to design effective automatic code comment generation methods. The automatic code comment generation issue is an active research topic in the program comprehension domain. This study conducts a systematic review of this research topic. The existing methods are divided into three categories:Template-based generation methods, information retrieval-based methods, and deep learning-based methods. Related studies are analyzed and summarizedfor each category. Then, the corpora and comment quality evaluation methods that are often used in previous studiesare analyzed, which can facilitate the experimental study for future studies. Finally, the potential research directions in the future aresummarized and discussed.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": false,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": false
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 0.03,
        "id": "https://openalex.org/W4307016191",
        "doi": "https://doi.org/10.5281/zenodo.7234722",
        "display_name": "++äººç”Ÿå¤§äº‹ç·šä¸Šçœ‹-2022-å®Œæ•´ç‰ˆ~HD",
        "publication_year": 2022,
        "host_venue": null,
        "is_oa": true,
        "oa_url": "https://zenodo.org/record/7234722",
        "best_oa_location": {
          "id": "pmh:oai:zenodo.org:7234722",
          "is_oa": true,
          "landing_page_url": "https://zenodo.org/record/7234722",
          "pdf_url": null,
          "source": null,
          "license": "cc-by",
          "license_id": "https://openalex.org/licenses/cc-by",
          "version": "submittedVersion",
          "is_accepted": false,
          "is_published": false,
          "raw_source_name": "",
          "raw_type": "info:eu-repo/semantics/article"
        },
        "abstract": "ã€Šäººç”Ÿå¤§äº‹ ã€‹-å®Œæ•´ç‰ˆ-ç·šä¸Šçœ‹é›»å½±å…è²»-ä¸­æ–‡å­—å¹•\\näººç”Ÿå¤§äº‹ç·šä¸Šçœ‹-2022-å®Œæ•´ç‰ˆ~HD\\näººç”Ÿå¤§äº‹ã€‹ ç·šä¸Šçœ‹-2022(HD-4Kå°ç£é›»å½±)å®Œæ•´ç‰ˆ((1080p)) \\näººç”Ÿå¤§äº‹~ç·šä¸Šçœ‹| å®Œæ•´ç‰ˆ2022å°æ¹¾é›»å½±(å°é´¨å½±éŸ³)FULL HD-4K\\näººç”Ÿå¤§äº‹(Lighting up the Stars)â–·ç·šä¸Šçœ‹å®Œæ•´ç‰ˆ â€” åœ¨çº¿è§‚çœ‹[[1080P]]-ZHç”µå½±\\nã€Šäººç”Ÿå¤§äº‹ã€‹â–·å®Œæ•´é›»å½±ç‰ˆHD(2022)-Lighting up the Stars ç·šä¸Šçœ‹å®Œæ•´ç‰ˆ [ğ‡ğƒ]\\n\\nâ“˜\\n\\n \\n\\n| ğŸœğ•‚ ğ•Œâ„ğ”» | ğŸ™ğŸ˜ğŸ ğŸ˜â„™ ğ”½ğ•Œğ•ƒğ•ƒ â„ğ”» | ğŸŸğŸšğŸ˜â„™ â„ğ”» | ğ•„ğ•‚ğ• | ğ•„â„™ğŸœ | ğ”»ğ•ğ”» | ğ”¹ğ•ğ•¦-â„ğ•’ğ•ª |\\n\\nğŸ…•ğŸ…¤ğŸ…›ğŸ…› ğŸ…¥ğŸ…”ğŸ…¡ğŸ…¢ğŸ…˜ğŸ…ğŸ… ğŸ‘‡ğŸ‘‡ğŸ‘‡ ğŸ…’ğŸ…›ğŸ…˜ğŸ…’ğŸ…š ğŸ…—ğŸ…”ğŸ…¡ï¿½\\n\\nğŸ¬â–¶ç«‹å³è§‚çœ‹ğŸ‘‰ [[ äººç”Ÿå¤§äº‹(Lighting up the Stars) 2022 å®Œæ•´ç‰ˆé«˜æ¸…ç‰ˆ ]] \\n\\n\\nâ­â­â­ â­â­â­â­â­â­ â­â­â­â­â­â­ â­â­â­â­â­\\n\\n \\n\\n\\n \\n\\nå¯¼æ¼”: åˆ˜æ±Ÿæ±Ÿ\\nç¼–å‰§: åˆ˜æ±Ÿæ±Ÿ\\nä¸»æ¼”: æœ±ä¸€é¾™ / æ¨æ©åˆ / ç‹æˆˆ / åˆ˜é™† / ç½—äº¬æ°‘ / æ›´å¤š...\\nç±»å‹: å‰§æƒ… / å®¶åº­\\nåˆ¶ç‰‡å›½å®¶/åœ°åŒº: ä¸­å›½å¤§é™† / ä¸­å›½é¦™æ¸¯\\nè¯­è¨€: æ±‰è¯­æ™®é€šè¯\\nä¸Šæ˜ æ—¥æœŸ: 2022-06-24(ä¸­å›½å¤§é™†)\\nç‰‡é•¿: 112åˆ†é’Ÿ\\nåˆå: Lighting Up The Stars / Funeral Family\\nIMDb: tt15801594\\n\\nIäººç”Ÿå¤§äº‹çš„å‰§æƒ…ç®€ä»‹ Â· Â· Â· Â· Â· Â·\\n æ®¡è‘¬å¸ˆè«ä¸‰å¦¹(æœ±ä¸€é¾™ é¥°)åœ¨åˆ‘æ»¡é‡Šæ”¾ä¸ä¹…åçš„ä¸€æ¬¡å‡ºæ®¡ä¸­,é‡åˆ°äº†å­¤å„¿æ­¦å°æ–‡(æ¨æ©åˆ é¥°),å°æ–‡çš„å‡ºç°,æ„å¤–åœ°æ”¹å˜äº†è«ä¸‰å¦¹å¯¹èŒä¸šå’Œç”Ÿæ´»çš„æ€åº¦ã€‚\\nTAGS:\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) æ¾³é—¨ä¸Šæ˜ \\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) 2022ä¸Šæ˜ \\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) HDçº¿ä¸Šçœ‹\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) çº¿ä¸Šçœ‹å°é¸­\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) ç”µå½±å®Œæ•´ç‰ˆ\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) çº¿ä¸Šçœ‹ä¸‹è½½\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) 2022 ä¸‹è½½\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) çº¿ä¸Šçœ‹å®Œæ•´ç‰ˆ\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) çº¿ä¸Šçœ‹å®Œæ•´ç‰ˆå°é¸­\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) (2022)å®Œæ•´ç‰ˆæœ¬\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) |1080P|å®Œæ•´ç‰ˆæœ¬\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) çº¿ä¸Šçœ‹(2022)å®Œæ•´ç‰ˆ\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) çº¿ä¸Šçœ‹(2022)å®Œæ•´ç‰ˆ\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) çº¿ä¸Šçœ‹ç”µå½±(2022)\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) 2022å¹´å†æ¬¡è§‚çœ‹ç”µå½±\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) çº¿ä¸Šçœ‹|2022ä¸Šæ˜ |å®Œæ•´ç‰ˆå°é¸­|çº¿ä¸Šçœ‹å°é¸­|\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) ä¸Šçœ‹\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) ä¸»é¢˜æ›²\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) å°é¸­å½±éŸ³\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) çº¿ä¸Šå°é¸­\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) å®Œæ•´ç‰ˆæœ¬\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) é¦™æ¸¯ä¸Šæ˜ \\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) çº¿ä¸Šçœ‹å°é¸­å½±éŸ³\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) 2022 çº¿ä¸Šçœ‹\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars)åœ¨çº¿\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) 1080P ä¸‹è½½\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) å…è´¹çº¿ä¸Šçœ‹ç”µå½±\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) ç”µå½±åœ¨çº¿2022å¹´\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) (2022)åœ¨çº¿è§‚çœ‹\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) [2022]è§‚çœ‹å’Œä¸‹è½½\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) [2022,HD]è§‚çœ‹å’Œä¸‹è½½\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) singapora(2022) å®Œæ•´ç‰ˆ\\n\\n\\näººç”Ÿå¤§äº‹ (Lighting up the Stars) çº¿ä¸Šçœ‹| 2022æœ€æ–°ç”µå½±| å°é¸­å½±éŸ³|\\n\\n \\n\\n \\n\\nâ—‡ æµåª’ä½“\\n\\n \\n\\næµåª’ä½“æ˜¯åœ¨ç”±ä¾›åº”å•†æä¾›çš„åŒæ—¶ä¸æ–­æ¥æ”¶å¹¶å‘ˆç°ç»™æœ€ç»ˆä½¿ç”¨è€…çš„å¤šåª’ä½“ã€‚åŠ¨è¯ to stream æ˜¯æŒ‡ä»¥è¿™ç§æ–¹å¼ä¼ é€’æˆ–è·å–åª’ä½“çš„è¿‡ç¨‹ã€‚ [éœ€è¦æ¾„æ¸…] æµåª’ä½“æŒ‡çš„æ˜¯åª’ä½“çš„äº¤ä»˜æ–¹å¼,è€Œä¸æ˜¯åª’ä½“æœ¬èº«ã€‚åŒºåˆ†åˆ†æ•£å¼åª’ä½“çš„ä¼ è¾“æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºç”µä¿¡ç½‘è·¯,å› ä¸ºå¤§å¤šæ•°ä¼ è¾“ç³»ç»Ÿè¦ä¹ˆæ˜¯å›ºæœ‰çš„æµåª’ä½“(ä¾‹å¦‚å¹¿æ’­ã€ç”µè§†ã€æµåª’ä½“åº”ç”¨ç¨‹å¼),è¦ä¹ˆæ˜¯å›ºæœ‰çš„éæµåª’ä½“(ä¾‹å¦‚ä¹¦ç±ã€å½•åƒå¸¦ã€éŸ³è®¯ CD)ã€‚åœ¨ç½‘è·¯ä¸Šæµå¼ä¼ è¾“å†…å®¹å­˜åœ¨æŒ‘æˆ˜ã€‚ä¾‹å¦‚,äº’è”ç½‘è¿æ¥é¢‘å®½ä¸è¶³çš„ä½¿ç”¨è€…å¯èƒ½ä¼šé‡åˆ°ä¸­æ–­ã€å»¶è¿Ÿæˆ–å†…å®¹ç¼“å†²ç¼“æ…¢çš„æƒ…å†µã€‚æ²¡æœ‰ç›¸å®¹ç¡¬ä½“æˆ–è½¯ä½“ç³»ç»Ÿçš„ä½¿ç”¨è€…å¯èƒ½æ— æ³•æµå¼ä¼ è¾“æŸäº›å†…å®¹ã€‚æµåª’ä½“ç›´æ’­æ˜¯å³æ—¶ä¼ é€äº’è”ç½‘å†…å®¹,å°±åƒç”µè§†ç›´æ’­é€šè¿‡ç”µè§†ä¿¡å·é€šè¿‡æ— çº¿æ–¹å¼å¹¿æ’­å†…å®¹ä¸€æ ·ã€‚å³æ—¶ Internet æµåª’ä½“éœ€è¦æŸç§å½¢å¼çš„æºåª’ä½“(ä¾‹å¦‚,æ‘„åƒæœºã€éŸ³è®¯ä»‹é¢ã€è¤å¹•æ•è·è½¯ä½“)ã€ç”¨äºæ•°ä½åŒ–å†…å®¹çš„ç¼–ç å™¨ã€åª’ä½“å‘è¡Œè€…ä»¥åŠç”¨äºåˆ†å‘å’Œäº¤ä»˜çš„å†…å®¹äº¤ä»˜ç½‘è·¯ã€‚ç›´æ’­ä¸éœ€è¦åœ¨èµ·ç‚¹å½•åˆ¶,å°½ç®¡é€šå¸¸æ˜¯è¿™æ ·ã€‚æµå¼ä¼ è¾“æ˜¯æ–‡ä»¶ä¸‹è½½çš„æ›¿ä»£æ–¹æ¡ˆ,æœ€ç»ˆç”¨æˆ·åœ¨è§‚çœ‹æˆ–æ”¶å¬å†…å®¹ä¹‹å‰è·å–å®Œæ•´æ–‡ä»¶çš„è¿‡ç¨‹ã€‚æµåª’ä½“å…è®¸æœ€ç»ˆä½¿ç”¨è€…åœ¨å‘é€æ•´ä¸ªæ¡£ä¹‹å‰ä½¿ç”¨ä»–ä»¬çš„åª’ä½“æ’­æ”¾æœºæ’­æ”¾æ•°ä½è§†é¢‘æˆ–æ•°ä½éŸ³è®¯å†…å®¹ã€‚æœ¯è¯­ã€Œæµåª’ä½“ã€å¯é€‚ç”¨äºè§†é¢‘å’ŒéŸ³è®¯ä»¥å¤–çš„åª’ä½“,ä¾‹å¦‚å³æ—¶å­—å¹•ã€è‡ªåŠ¨æ”¶æŠ¥æœºå’Œå³æ—¶æ–‡æœ¬,æ‰€æœ‰è¿™äº›éƒ½è¢«è§†ä¸ºã€Œæµæ–‡æœ¬ã€ã€‚ç”µæ¢¯éŸ³ä¹æ˜¯æœ€æ—©ä»¥æµåª’ä½“å½¢å¼å‡ºç°çš„æµè¡ŒéŸ³ä¹ä¹‹ä¸€;ä»Šå¤©,äº’è”ç½‘ç”µè§†æ˜¯ä¸€ç§å¸¸è§çš„æµåª’ä½“å½¢å¼ã€‚ä¸€äº›æµè¡Œçš„æµåª’ä½“æœåŠ¡åŒ…æ‹¬ Netflixã€Disney+ã€Huluã€Prime Videoã€è§†é¢‘å…±äº«ç½‘ç«™ YouTube å’Œå…¶ä»–æµåª’ä½“ç”µå½±å’Œç”µè§†èŠ‚ç›®çš„ç½‘ç«™;æµåª’ä½“éŸ³ä¹çš„ Apple Musicã€YouTube Music å’Œ Spotify;ä»¥åŠå®æ—¶è§†é¢‘æ¸¸æˆæµåª’ä½“ç½‘ç«™ Twitchã€‚æ ¼é›·ç‰¹\\n\\n \\n\\nâ—‡ ç‰ˆæƒ â—‡\\n\\n \\n\\nç‰ˆæƒæ˜¯ä¸€ç§æ™ºæ…§è´¢äº§æƒ,å®ƒèµ‹äºˆæ‹¥æœ‰è€…åˆ¶ä½œåˆ›æ„ä½œå“å‰¯æœ¬çš„ä¸“æœ‰æƒ,é€šå¸¸æ˜¯åœ¨æœ‰é™çš„æ—¶é—´å†…ã€‚åˆ›é€ æ€§ä½œå“å¯ä»¥æ˜¯æ–‡å­¦ã€è‰ºæœ¯ã€æ•™è‚²æˆ–éŸ³ä¹å½¢å¼ã€‚ç‰ˆæƒæ—¨åœ¨ä¿æŠ¤åˆ›æ„ä½œå“å½¢å¼çš„åˆ›æ„çš„åŸå§‹è¡¨è¾¾,è€Œä¸æ˜¯åˆ›æ„æœ¬èº«ã€‚ç‰ˆæƒå—åˆ°åŸºäºå…¬å…±åˆ©ç›Šè€ƒè™‘çš„é™åˆ¶,ä¾‹å¦‚ç¾å›½çš„åˆç†ä½¿ç”¨åŸåˆ™ã€‚ä¸€äº›å¸æ³•ç®¡è¾–åŒºè¦æ±‚ä»¥æœ‰å½¢å½¢å¼ã€Œå›ºå®šã€å—ç‰ˆæƒä¿æŠ¤çš„ä½œå“ã€‚å®ƒé€šå¸¸ç”±å¤šä¸ªä½œè€…å…±ç”¨,æ¯ä¸ªä½œè€…éƒ½æœ‰ä¸€ç»„ä½¿ç”¨æˆ–è®¸å¯ä½œå“çš„æƒåˆ©,é€šå¸¸è¢«ç§°ä¸ºæƒåˆ©æŒæœ‰äººã€‚ [éœ€è¦æ›´å¥½çš„æ¥æº] è¿™äº›æƒåˆ©é€šå¸¸åŒ…æ‹¬å¤åˆ¶ã€å¯¹è¡ç”Ÿä½œå“çš„æ§åˆ¶ã€å‘è¡Œã€å…¬å¼€è¡¨æ¼”å’Œè¯¸å¦‚å½’å±ä¹‹ç±»çš„ç²¾ç¥æƒåˆ©ã€‚ç‰ˆæƒå¯ä»¥ç”±å…¬æ³•æˆäºˆ,åœ¨è¿™ç§æƒ…å†µä¸‹è¢«è§†ä¸ºâ€œé¢†åœŸæƒåˆ©â€ã€‚è¿™æ„å‘³ç€ä»»ä½•ç‰¹å®šå·çš„æ³•å¾‹æˆäºˆçš„ç‰ˆæƒä¸ä¼šè¶…å‡ºè¯¥ç‰¹å®šå¸æ³•ç®¡è¾–åŒºçš„é¢†åœŸã€‚æ­¤ç±»ç‰ˆæƒå› å›½å®¶/åœ°åŒºè€Œå¼‚;è®¸å¤šå›½å®¶,æœ‰æ—¶æ˜¯ä¸€å¤§ç¾¤å›½å®¶,ä¸å…¶ä»–å›½å®¶å°±åœ¨ä½œå“è·¨è¶Šå›½ç•Œæˆ–å›½å®¶æƒåˆ©ä¸ä¸€è‡´æ—¶é€‚ç”¨çš„ç¨‹åºè¾¾æˆåå®šã€‚é€šå¸¸,ç‰ˆæƒçš„å…¬æ³•æœŸé™åœ¨åˆ›ä½œè€…å»ä¸–å 50 åˆ° 100 å¹´åˆ°æœŸ,å…·ä½“å–å†³äºå¸æ³•ç®¡è¾–åŒºã€‚ä¸€äº›å›½å®¶è¦æ±‚æŸäº›ç‰ˆæƒæ‰‹ç»­æ‰èƒ½å»ºç«‹ç‰ˆæƒ,è€Œå¦ä¸€äº›å›½å®¶åˆ™æ‰¿è®¤æœªç»æ­£å¼æ³¨å†Œçš„ä»»ä½•å·²å®Œæˆä½œå“çš„ç‰ˆæƒã€‚æ€»çš„æ¥è¯´,è®¸å¤šäººè®¤ä¸ºç‰ˆæƒçš„é•¿æœŸä¿æŠ¤å¯ä»¥æ›´å¥½åœ°ä¿æŠ¤ä½œå“ã€‚ç„¶è€Œ,ä¸€äº›å­¦è€…è®¤ä¸º,æ›´é•¿çš„æŒç»­æ—¶é—´å¹¶æ²¡æœ‰æé«˜ä½œè€…çš„æ”¶å…¥,åŒæ—¶é˜»ç¢äº†æ–‡åŒ–åˆ›é€ åŠ›å’Œå¤šæ ·æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹,ç¼©çŸ­ç‰ˆæƒæœŸé™å¯ä»¥å¢åŠ ä½œè€…ä»å…¶ä½œå“ä¸­è·å¾—çš„æ”¶å…¥,å¹¶å¢åŠ æ–‡åŒ–å¤šæ ·æ€§å’Œåˆ›é€ åŠ›ã€‚\\n\\n \\n\\nâ—‡ ç”µå½±æˆ–ç”µå½± â—‡\\n\\n \\n\\nç”µå½±æˆ–ç”µå½±æ˜¯ä¸€ç§è§†è§‰äº¤æµå½¢å¼,å®ƒä½¿ç”¨ç§»åŠ¨çš„å›¾åƒå’Œå£°éŸ³æ¥è®²æ•…äº‹æˆ–æ•™äººã€‚å¤§å¤šæ•°äººçœ‹(çœ‹)ç”µå½±æ˜¯ä¸€ç§å¨±ä¹å½¢å¼æˆ–ä¸€ç§å¨±ä¹æ–¹å¼ã€‚å¯¹äºæŸäº›äººæ¥è¯´,æœ‰è¶£çš„ç”µå½±å¯ä»¥æ˜¯è®©ä»–ä»¬å‘ç¬‘çš„ç”µå½±,è€Œå¯¹å¦ä¸€äº›äººæ¥è¯´,å®ƒå¯ä»¥æ˜¯è®©ä»–ä»¬å“­æ³£æˆ–å®³æ€•çš„ç”µå½±ã€‚äººä»¬æ™®éè®¤ä¸º,ç‰ˆæƒæ˜¯ä¿ƒè¿›æ–‡åŒ–å¤šæ ·æ€§å’Œåˆ›é€ åŠ›çš„å¿…è¦æ¡ä»¶ã€‚ç„¶è€Œ,Parc è®¤ä¸º,ä¸æµè¡Œè§‚ç‚¹ç›¸å,æ¨¡ä»¿å’Œå¤åˆ¶å¹¶ä¸é™åˆ¶æ–‡åŒ–åˆ›é€ åŠ›æˆ–å¤šæ ·æ€§,è€Œæ˜¯è¿›ä¸€æ­¥æ”¯æŒå®ƒã€‚è¿™ä¸€è®ºç‚¹å¾—åˆ°äº†è®¸å¤šä¾‹å­çš„æ”¯æŒ,ä¾‹å¦‚å°ç±³å’Œæ¢µé«˜ã€æ¯•åŠ ç´¢ã€é©¬å¥ˆå’Œè«å¥ˆç­‰ã€‚å¤§å¤šæ•°ç”µå½±çš„åˆ¶ä½œæ–¹å¼å¯ä»¥åœ¨ç”µå½±é™¢å’Œå®¶é‡Œçš„å±å¹•ä¸Šæ”¾æ˜ ã€‚ç”µå½±åœ¨ç”µå½±é™¢æ”¾æ˜ æ•°å‘¨æˆ–æ•°æœˆå,å¯ä»¥é€šè¿‡å…¶ä»–å„ç§åª’ä½“è¿›è¡Œè¥é”€ã€‚å®ƒä»¬åœ¨ä»˜è´¹ç”µè§†æˆ–æœ‰çº¿ç”µè§†ä¸Šæ’­æ”¾,å¹¶åœ¨ DVD å…‰ç›˜æˆ–å½•åƒå¸¦ä¸Šå‡ºå”®æˆ–å‡ºç§Ÿ,ä»¥ä¾¿äººä»¬å¯ä»¥åœ¨å®¶ä¸­è§‚çœ‹ç”µå½±ã€‚æ‚¨è¿˜å¯ä»¥ä¸‹è½½æˆ–æµå¼ä¼ è¾“ç”µå½±ã€‚è¾ƒæ—§çš„ç”µå½±åœ¨ç”µè§†é¢‘é“æ’­æ”¾ã€‚èƒ¶ç‰‡ç›¸æœºæˆ–æ‘„åƒæœºæ‹æ‘„ç…§ç‰‡çš„é€Ÿåº¦éå¸¸å¿«,é€šå¸¸ä¸ºæ¯ç§’ 24 æˆ– 25 å¼ å›¾ç‰‡(å¸§)ã€‚å½“ç”µå½±æ”¾æ˜ æœºã€è®¡ç®—æœºæˆ–ç”µè§†ä»¥è¿™ç§é€Ÿåº¦æ˜¾ç¤ºå›¾åƒæ—¶,å›¾åƒåºåˆ—ä¸­æ˜¾ç¤ºçš„äº‹ç‰©ä¼¼ä¹å®é™…ä¸Šåœ¨ç§»åŠ¨ã€‚å£°éŸ³æ˜¯åŒæ—¶å½•åˆ¶æˆ–ç¨åæ·»åŠ çš„ã€‚ç”µå½±ä¸­çš„å£°éŸ³é€šå¸¸åŒ…æ‹¬äººä»¬è¯´è¯çš„å£°éŸ³(ç§°ä¸ºå¯¹è¯)ã€éŸ³ä¹(ç§°ä¸ºâ€œé…ä¹â€)å’ŒéŸ³æ•ˆã€ç”µå½±ä¸­å‘ç”Ÿçš„æ´»åŠ¨çš„å£°éŸ³(ä¾‹å¦‚å¼€é—¨æˆ–å¼€æª)ã€‚ )ã€‚åœ¨ 20 ä¸–çºª,ç…§ç›¸æœºä½¿ç”¨ç…§ç›¸èƒ¶å·ã€‚è¯¥äº§å“é€šå¸¸ä»è¢«ç§°ä¸ºâ€œç”µå½±â€,å°½ç®¡é€šå¸¸æ²¡æœ‰ç”µå½±ã€‚æµæ´¾æ˜¯ä¸€ç§ç”µå½±ç±»å‹æˆ–ç”µå½±é£æ ¼çš„è¯ã€‚ç”µå½±å¯ä»¥æ˜¯è™šæ„çš„(ç¼–é€ çš„)ã€çºªå½•ç‰‡(å±•ç¤ºâ€œçœŸå®ç”Ÿæ´»â€)æˆ–ä¸¤è€…çš„æ··åˆã€‚è™½ç„¶æ¯å¹´åˆ¶ä½œæ•°ç™¾éƒ¨ç”µå½±,ä½†å¾ˆå°‘æœ‰ä¸éµå¾ªå°‘é‡åœºæ™¯æˆ–æ•…äº‹çš„ç”µå½±ã€‚æœ‰äº›ç”µå½±ç»“åˆäº†ä¸¤ç§æˆ–å¤šç§ç±»å‹ã€‚åŠ¨ä½œç‰‡æœ‰å¾ˆå¤šæƒŠå¿ƒåŠ¨é­„çš„æ•ˆæœ,æ¯”å¦‚æ±½è½¦è¿½é€å’Œæ¶‰åŠç‰¹æŠ€æ¼”å‘˜çš„æªæˆ˜ã€‚å®ƒé€šå¸¸æ¶‰åŠâ€œå¥½äººâ€å’Œâ€œåäººâ€,å› æ­¤æˆ˜äº‰å’ŒçŠ¯ç½ªæ˜¯å¸¸è§çš„è¯é¢˜ã€‚ç”±äºæƒ…èŠ‚é€šå¸¸å¾ˆç®€å•,å› æ­¤åŠ¨ä½œç”µå½±é€šå¸¸ä¸éœ€è¦è´¹åŠ›å³å¯è§‚çœ‹ã€‚ä¾‹å¦‚,åœ¨ã€Šè™èƒ†é¾™å¨ã€‹ä¸­,ææ€–åˆ†å­æ§åˆ¶äº†ä¸€åº§æ‘©å¤©å¤§æ¥¼å¹¶ç´¢è¦å¤§ç¬”èµé‡‘,ä»¥æ¢å–ä¸æ€å®³è¢«æ‰£ä¸ºäººè´¨çš„å·¥äººã€‚ä¸€ä½è‹±é›„ä»¥æŸç§æ–¹å¼è®¾æ³•æ‹¯æ•‘äº†æ‰€æœ‰äººã€‚åŠ¨ä½œç‰‡é€šå¸¸ä¸ä¼šè®©äººå“­,ä½†å¦‚æœåŠ¨ä½œç‰‡ä¹Ÿæ˜¯å‰§æƒ…ç‰‡,é‚£å°±æ˜¯æœ‰æƒ…æ„Ÿçš„ã€‚å†’é™©ç”µå½±é€šå¸¸æ¶‰åŠä¸€ä¸ªå¯»æ±‚æ‹¯æ•‘ä¸–ç•Œæˆ–æ‰€çˆ±ä¹‹äººçš„è‹±é›„ã€‚è®²æ¼«ç”»è®²æ•…äº‹ã€‚è¿™äº›ç”µå½±è¿‡å»æ˜¯é€å¸§æ‰‹ç»˜çš„,ä½†ç°åœ¨æ˜¯åœ¨ç”µè„‘ä¸Šåˆ¶ä½œçš„ã€‚å¥½å‹ç”µå½±æ¶‰åŠ2ä¸ªè‹±é›„,ä¸€ä¸ªè¦æ‹¯æ•‘å¦ä¸€ä¸ª,ä¸¤ä¸ªéƒ½å¿…é¡»å…‹æœéšœç¢ã€‚å“¥ä»¬ç”µå½±ç»å¸¸å‘ˆç°å–œå‰§,ä½†ä¹Ÿæœ‰ä¸€äº›æƒ…æ„Ÿ,å› ä¸ºâ€œå“¥ä»¬â€ä¹‹é—´çš„äº²å¯†å‹è°Šã€‚å–œå‰§æ˜¯å…³äºæ„šè ¢æˆ–ä¸å¯»å¸¸çš„äº‹æƒ…çš„æœ‰è¶£ç”µå½±,æˆ–è€…å‘ç°è‡ªå·±å¤„äºæ„šè ¢æˆ–ä¸å¯»å¸¸çš„æƒ…å†µ,è®©è§‚ä¼—å‘ç¬‘ã€‚çºªå½•ç‰‡æ˜¯(æˆ–å£°ç§°æ˜¯)å…³äºçœŸå®äººç‰©å’ŒçœŸå®äº‹ä»¶çš„ç”µå½±ã€‚ä»–ä»¬å‡ ä¹æ€»æ˜¯ä¸¥è‚ƒçš„,å¹¶ä¸”å¯èƒ½æ¶‰åŠé«˜åº¦æƒ…ç»ªåŒ–çš„è¯é¢˜,ä¾‹å¦‚æ®‹å¿ã€‚æˆå‰§æ˜¯ä¸¥è‚ƒçš„,é€šå¸¸æ˜¯å…³äºäººä»¬å å…¥çˆ±æ²³æˆ–ä¸å¾—ä¸åœ¨ç”Ÿæ´»ä¸­åšå‡ºé‡è¦å†³å®šã€‚ä»–ä»¬è®²è¿°äººä¸äººä¹‹é—´å…³ç³»çš„æ•…äº‹ã€‚ä»–ä»¬é€šå¸¸éµå¾ªä¸€ä¸ªåŸºæœ¬æƒ…èŠ‚,å…¶ä¸­ä¸€ä¸¤ä¸ªä¸»è¦è§’è‰²(æ¯ä¸ªæ¼”å‘˜æ‰®æ¼”ä¸€ä¸ªè§’è‰²)å¿…é¡»â€œå…‹æœâ€(è¶Šè¿‡)éšœç¢(é˜»ç¢ä»–ä»¬å‰è¿›çš„ä¸œè¥¿)æ‰èƒ½å¾—åˆ°ä»–ä»¬æƒ³è¦çš„ä¸œè¥¿ã€‚æ‚²å‰§æ€»æ˜¯æˆå‰§æ€§çš„,éƒ½æ˜¯å…³äºé™·å…¥å›°å¢ƒçš„äººçš„ã€‚ä¾‹å¦‚,ç¦»å©šçš„ä¸ˆå¤«å’Œå¦»å­å¿…é¡»å„è‡ªåŠªåŠ›åœ¨æ³•åº­ä¸Šè¯æ˜ä»–ä»¬æ˜¯ç…§é¡¾å­©å­çš„æœ€ä½³äººé€‰ã€‚æƒ…æ„Ÿ(æ„Ÿè§‰)æ˜¯ç”µå½±çš„é‡è¦ç»„æˆéƒ¨åˆ†,è§‚ä¼—(è§‚çœ‹ç”µå½±çš„äºº)å¯èƒ½ä¼šæ„Ÿåˆ°æ²®ä¸§ç”šè‡³å“­æ³£ã€‚é»‘è‰²ç”µå½±æ˜¯ 1940 å¹´ä»£å…³äºçŠ¯ç½ªå’Œæš´åŠ›çš„ä¾¦æ¢å‰§ã€‚å®¶åº­ç”µå½±æ˜¯ä¸ºæ•´ä½“è€Œåˆ¶ä½œçš„å®¶åº­ã€‚å®ƒä»¬ä¸»è¦æ˜¯ä¸ºå„¿ç«¥åˆ¶ä½œçš„,ä½†ä¹Ÿç»å¸¸ä¸ºæˆäººæä¾›å¨±ä¹ã€‚è¿ªå£«å°¼ä»¥å…¶å®¶åº­ç”µå½±è€Œé—»åã€‚ææ€–ç”µå½±åˆ©ç”¨ææƒ§æ¥åˆºæ¿€è§‚ä¼—ã€‚éŸ³ä¹ã€ç¯å…‰å’Œå¸ƒæ™¯(åˆ¶ä½œç”µå½±çš„ç”µå½±åˆ¶ç‰‡å‚ä¸­çš„äººé€ åœºæ‰€)éƒ½æ—¨åœ¨å¢å¼ºæ„Ÿè§‰ã€‚æµªæ¼«å–œå‰§(Rom-Coms)é€šå¸¸æ˜¯å…³äºä¸¤ä¸ªæ¥è‡ªä¸åŒä¸–ç•Œçš„äººçš„çˆ±æƒ…æ•…äº‹,ä»–ä»¬å¿…é¡»å…‹æœéšœç¢æ‰èƒ½èµ°åˆ°ä¸€èµ·ã€‚ Rom-Coms é€šå¸¸è½»æ¾æ„‰å¿«,ä½†ä¹Ÿå¯èƒ½åŒ…å«ä¸€äº›æƒ…æ„Ÿã€‚å–œå‰§ææ€–ç‰‡åœ¨æƒ…èŠ‚ä¸­ç»“åˆäº†ææ€–å’Œå–œå‰§ä¸»é¢˜ã€‚è¿™ç§ç±»å‹çš„ç”µå½±æœ‰æ—¶ä¼šä½¿ç”¨é»‘è‰²å–œå‰§ä½œä¸ºä¸»è¦çš„å¹½é»˜å½¢å¼ã€‚ç§‘å¹»ç”µå½±ä»¥æœªæ¥æˆ–å¤ªç©ºä¸ºèƒŒæ™¯ã€‚æœ‰äº›äººåˆ©ç”¨ä»–ä»¬çš„æœªæ¥æˆ–é™Œç”Ÿçš„ç¯å¢ƒæ¥è¯¢é—®æœ‰å…³ç”Ÿå‘½æ„ä¹‰æˆ–æˆ‘ä»¬åº”è¯¥å¦‚ä½•æ€è€ƒç”Ÿå‘½çš„é—®é¢˜ã€‚ç§‘å¹»ç”µå½±ç»å¸¸ä½¿ç”¨ç‰¹æ•ˆæ¥åˆ›é€ å¤–æ˜Ÿä¸–ç•Œã€å¤–å¤ªç©ºã€å¤–æ˜Ÿäººå’Œé£èˆ¹çš„å›¾åƒã€‚å¥‡å¹»ç”µå½±åŒ…å«çœŸäººæ— æ³•åšåˆ°çš„ç¥å¥‡å’Œä¸å¯èƒ½çš„äº‹æƒ…ã€‚æƒŠæ‚šç‰‡é€šå¸¸æ˜¯å…³äºéœ€è¦è§£å†³çš„ç¥ç§˜ã€å¥‡æ€ªçš„äº‹ä»¶æˆ–çŠ¯ç½ªã€‚è§‚ä¼—ä¸€ç›´åœ¨çŒœæµ‹ç›´åˆ°æœ€åå‡ åˆ†é’Ÿ,é‚£æ—¶æƒ…èŠ‚ä¸­é€šå¸¸ä¼šæœ‰â€œè½¬æŠ˜â€(æƒŠå–œ)ã€‚æƒŠå¿ƒåŠ¨é­„çš„ç”µå½±è®©æ‚¨ååœ¨åº§ä½è¾¹ä¸Šã€‚å®ƒä»¬é€šå¸¸æœ‰å¤šç§æ›²æŠ˜,ä½¿è§‚ä¼—æ„Ÿåˆ°å›°æƒ‘ã€‚è¥¿æ–¹ç”µå½±è®²è¿°äº†1870å¹´ä»£å’Œ1880å¹´ä»£ç¾å›½è¥¿éƒ¨ç‰›ä»”çš„æ•…äº‹,å¤§å¤šæ˜¯åŠ¨ä½œç‰‡,ä½†æœ‰å¤è£…ã€‚æœ‰äº›ä¸ç¾æ´²åŸä½æ°‘æœ‰å…³ã€‚å¹¶éæ‰€æœ‰ä»¥ç¾å›½è¥¿éƒ¨ä¸ºèƒŒæ™¯çš„ç”µå½±éƒ½æ˜¯åœ¨é‚£é‡Œåˆ¶ä½œçš„ã€‚ä¾‹å¦‚,åœ¨æ„å¤§åˆ©åˆ¶ä½œçš„è¥¿æ–¹ç”µå½±è¢«ç§°ä¸ºæ„å¤§åˆ©é¢æ¡è¥¿éƒ¨ç‰‡ã€‚ä¸€äº›ç”µå½±ä¹Ÿå¯èƒ½ä½¿ç”¨è¥¿æ–¹æƒ…èŠ‚,å³ä½¿å®ƒä»¬æ˜¯åœ¨å…¶ä»–åœ°æ–¹è®¾ç½®çš„ã€‚\\n\\n\\nâââ FI ç”µè§†ç”µå½± âââ\\n\\n \\n\\nç¬¬ä¸€ä¸ªç”µè§†èŠ‚ç›®æ˜¯å®éªŒæ€§çš„,å¶å°”ä¼šå‡ºç°ã€‚è‡ª 1930 å¹´ä»£ä»¥æ¥,å®ƒä»¬åªèƒ½åœ¨é è¿‘æ¡…æ†çš„åœ°æ–¹è§‚çœ‹ã€‚ç”µè§†èŠ‚ç›®,ä¾‹å¦‚ 1936 å¹´åœ¨å¾·å›½ä¸¾è¡Œçš„å¤å­£å¥¥è¿ä¼š,ä¹”æ²»å…­ä¸–å›½ç‹åœ¨é‚£é‡ŒåŠ å†•ã€‚åœ¨1940å¹´çš„è‹±å›½,ä»¥åŠ1939å¹´çº½çº¦ä¸–ç•Œåšè§ˆä¼šä¸Šå¤§åé¼é¼çš„å¤§å«Â·è¨è¯ºå¤«(David Sarnoff)çš„æ¨å‡º,è¿™ç§åª’ä»‹ç»§ç»­å‘å±•,ä½†æˆ˜åç¬¬äºŒæ¬¡ä¸–ç•Œå¤§æˆ˜åœæ»äº†å®ƒçš„å‘å±•ã€‚ .è¿™éƒ¨ 19440 å¹´çš„ä¸–ç•Œç”µå½±å¯å‘äº†è®¸å¤šç¾å›½äºº,ä»–ä»¬è´­ä¹°äº†ç¬¬ä¸€å°ç”µè§†æœºã€‚ 1948 å¹´,æµè¡Œçš„å¹¿æ’­ç”µå°å¾·å£«å¤å‰§é™¢æˆä¸ºç¬¬ä¸€ä¸ªæ¯å‘¨ä¸€æ¬¡çš„ç”µè§†ç»¼è‰ºèŠ‚ç›®ã€‚è¯¥èŠ‚ç›®ç”±ç±³å°”é¡¿ä¼¯åˆ©ä¸»æŒ,å¹¶è·å¾—äº†â€œå…ˆç”Ÿâ€çš„ç§°å·ã€‚ TV',è¡¨æ˜åª’ä½“æ˜¯ç¨³å®šçš„,æ˜¯ä¸€ç§å¯ä»¥å¸å¼•å¹¿å‘Šå•†çš„ç°ä»£å¨±ä¹å½¢å¼ã€‚ 1951 å¹´ 9 æœˆ 4 æ—¥,è¿™æ˜¯ç¾å›½ç¬¬ä¸€ä¸ªå…¨å›½ç›´æ’­ç”µè§†èŠ‚ç›®ã€‚å½“å“ˆé‡Œæœé²é—¨æ€»ç»Ÿå°±æ—§é‡‘å±±-æ—¥æœ¬å’Œå¹³æ¡çº¦å°± AT&amp;T çš„è·¨å¤§é™†ç”µç¼†å’Œå¾®æ³¢ä¸­ç»§ç³»ç»Ÿå‘è¡¨æ¼”è®²æ—¶,ä»–å·²ç»å‘å½“åœ°å¸‚åœºå…¬å¸è¿›è¡Œäº†å¹¿æ’­ã€‚æ˜¯çš„ã€‚ 1954 å¹´ 1 æœˆ 1 æ—¥,ç¾å›½ä¸¾è¡Œäº†ç¬¬ä¸€æ¬¡å…¨å›½è‰²å½©ç§€(1954 ç«ç‘°æ¸¸è¡Œé”¦æ ‡èµ›)ã€‚åœ¨æ¥ä¸‹æ¥çš„åå¹´ä¸­,å¤§å¤šæ•°äº’è”ç½‘å¹¿æ’­å’Œå‡ ä¹æ‰€æœ‰æœ¬åœ°å¹¿æ’­éƒ½æ˜¯é»‘ç™½å¹¿æ’­ã€‚ 1965å¹´ç§‹å¤©å®£å¸ƒæ¢è‰²,äº’è”ç½‘ä¸Šä¸€åŠä»¥ä¸Šçš„é»„é‡‘æ—¶æ®µèŠ‚ç›®éƒ½æ˜¯å½©è‰²æ’­æ”¾çš„ã€‚ä¸€å¹´å,ç¬¬ä¸€ä¸ªå…¨å½©æ—ºå­£å¼€å§‹äº†ã€‚ 19402å¹´,æœ€åä¸€æ¬¡æ”¯æŒæ—¥é—´ç½‘ç»œèŠ‚ç›®å˜æˆäº†ç¬¬ä¸€ä¸ªå…¨å½©ç½‘ç»œå­£ã€‚\\n\\n \\n\\nâââ æ ¼å¼å’Œæµæ´¾ âââ\\n\\n \\n\\nå¦è¯·å‚é˜…:æµæ´¾æ¸…å• Â§ ç”µå½±å’Œç”µè§†æ ¼å¼å’Œæµæ´¾ å¯ä»¥å‘ˆç°çš„å„ç§æ ¼å¼å’Œç±»å‹ä½¿ç”µè§†èŠ‚ç›®æ¯”å¤§å¤šæ•°å…¶ä»–åª’ä½“æ›´åŠ å¤šæ ·åŒ–ã€‚æè¿°å¯ä»¥æ˜¯è™šæ„çš„(å¦‚å–œå‰§å’Œæˆå‰§)æˆ–éè™šæ„çš„(å¦‚çºªå½•ç‰‡ã€æ–°é—»å’ŒçœŸäººç§€)ã€‚å®ƒå¯ä»¥æ˜¯æœ€æ–°çš„(å¦‚æœ¬åœ°æ–°é—»èŠ‚ç›®å’Œä¸€äº›ç”µè§†ç”µå½±)æˆ–å†å²çš„(å¦‚è®¸å¤šçºªå½•ç‰‡å’Œå°è¯´ç”µå½±)ã€‚å®ƒä»¬å¯ä»¥å…·æœ‰æ•™è‚²æ„ä¹‰æˆ–å¨±ä¹æ€§,å°±åƒæƒ…èŠ‚å–œå‰§å’Œæ¸¸æˆèŠ‚ç›®ä¸€æ ·ã€‚ [å¼•ç”¨éœ€è¦] æˆå‰§èŠ‚ç›®é€šå¸¸ç”±ä¸€ç³»åˆ—æ¼”å‘˜åœ¨å†å²æˆ–ç°ä»£ç¯å¢ƒä¸­æ‰®æ¼”è§’è‰²ç»„æˆã€‚è¯¥è®¡åˆ’éµå¾ªä»–ä»¬çš„ç”Ÿæ´»å’Œå†’é™©ã€‚ 1980å¹´ä»£ä¹‹å‰,è¡¨æ¼”(è‚¥çš‚å‰§é™¤å¤–)å¤§å¤šä¿æŒé™æ­¢,æ²¡æœ‰æ•…äº‹æƒ…èŠ‚,ä¸»è¦äººç‰©å’Œå»ºç­‘ç‰©å‡ ä¹æ²¡æœ‰å˜åŒ–ã€‚ [å¼•ç”¨éœ€è¦] å¦‚æœå‰§é›†ä¸­è§’è‰²çš„ç”Ÿæ´»æœ‰ä»»ä½•å˜åŒ–,é€šå¸¸ä¼šåœ¨æœ€åè¢«æ’¤é”€ã€‚å› æ­¤,å‰§é›†å¯ä»¥æŒ‰ä»»ä½•é¡ºåºæ’­æ”¾ã€‚ [éœ€è¦å¼•è¯] è‡ª1980å¹´ä»£ä»¥æ¥,è®¸å¤šç”µå½±é€æ¸æ”¹å˜äº†æƒ…èŠ‚ã€äººç‰©æˆ–ä¸¤è€…ã€‚ä¾‹å¦‚,å¸Œå°”è¡—å¸ƒé²æ–¯å’Œåœ£åˆ«å¤„æ˜¯ç¾å›½ç¬¬ä¸€éƒ¨å…·æœ‰è¿™ç§æˆå‰§ç»“æ„çš„é»„é‡‘æ—¶æ®µç”µè§†å‰§[4][éœ€è¦æ›´å¥½çš„èµ„æº],åæ¥çš„FILM Babylon 5è¿›ä¸€æ­¥è§£é‡Šäº†è¿™ç§ç»“æ„çš„äº”äººç»„ä¸è®¡åˆ’ç›¸å…³çš„å­£èŠ‚ã€‚ ã€å¼•æ–‡ã€‘æ®æŠ¥å¯¼,2019å¹´å„å¤§åª’ä½“å…¬å¸æ”¶å…¥ä¸­ç”µè§†æ”¶å…¥å æ¯”è¶…è¿‡ç”µå½±ã€‚æœ‰äº›äººè¿˜æ³¨æ„åˆ°æŸäº›ç”µè§†èŠ‚ç›®çš„å“è´¨æœ‰æ‰€æé«˜ã€‚ 2019 å¹´,å¥¥æ–¯å¡è·å¥–ç”µå½±å¯¼æ¼”å²è’‚æ–‡Â·ç´¢å¾·ä¼¯æ ¼ (Steven Soderbergh) å®£å¸ƒäº†äººç‰©å’Œæ•…äº‹çš„æ¨¡ç³Šæ€§å’Œå¤æ‚æ€§:\\n\\n \\n\\nâââ è°¢è°¢å¤§å®¶çš„ä»˜å‡º,ç¥å¤§å®¶è§‚å½±æ„‰å¿«\\n\\n \\n\\nåœ¨è¿™é‡Œæ‚¨å¯ä»¥æ‰¾åˆ°æ‰€æœ‰å¯ä»¥åœ¨çº¿æ’­æ”¾çš„ç”µå½±,åŒ…æ‹¬æœ¬å‘¨ä¸Šæ˜ çš„ç”µå½±ã€‚å¦‚æœä½ æƒ³åœ¨è¿™ä¸ªç½‘ç«™ä¸Šçœ‹åˆ°ä»»ä½•ä¸œè¥¿,ä½ åº”è¯¥çŸ¥é“å®ƒæ¶µç›–çš„ç±»å‹æ˜¯çŠ¯ç½ªã€ç§‘å­¦ã€ç”µå½±ã€ç”µå½±ã€æµªæ¼«ã€æƒŠæ‚šã€å–œå‰§ã€æˆå‰§å’ŒåŠ¨æ¼«ç”µå½±ã€‚éå¸¸æ„Ÿè°¢ä½ ã€‚æˆ‘ä»¬å°†é€šçŸ¥ä»»ä½•æƒ³æ¥æ”¶æœ‰å…³ä»Šå¹´ç”µå½±èŠ‚ç›®çš„æ–°é—»æˆ–èµ„è®¯ä»¥åŠå¦‚ä½•è§‚çœ‹æ‚¨æœ€å–œæ¬¢çš„ç”µå½±çš„äººã€‚å¸Œæœ›æˆ‘ä»¬èƒ½æˆä¸ºæ‚¨ä¸ºæ‚¨æœ€å–œçˆ±çš„ç”µå½±å¯»æ‰¾æ¨èçš„æœ€ä½³åˆä½œä¼™ä¼´ã€‚è¿™æ˜¯æˆ‘ä»¬çš„,é—®å€™!æ„Ÿè°¢æ”¶çœ‹ä»Šå¤©çš„è§†é¢‘ã€‚æˆ‘å¸Œæœ›ä½ å–œæ¬¢æˆ‘åˆ†äº«çš„è§†é¢‘ã€‚å¦‚æœæ‚¨å–œæ¬¢æˆ‘ä»¬åˆ†äº«çš„å†…å®¹,è¯·ç»™æˆ‘ä»¬ä¸€ä¸ªå¤§æ‹‡æŒ‡,è®©æˆ‘ä»¬çŸ¥é“æ‚¨å–œæ¬¢æˆ–åˆ†äº«å®ƒ,è¿™å°†ä½¿",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "56fc71c3dc96",
      "page_id": "DBLP:conf/icbase/TianWY22",
      "title": "BUG-T5: A Transformer-based Automatic Title Generation Method for Bug Reports",
      "year": 2022,
      "abbr": "ICBASE'22",
      "venue": "Proceedings of the 3rd International Conference on Big Data & Artificial Intelligence & Software Engineering",
      "type": "conference",
      "tags": [
        "EI"
      ],
      "doi": null,
      "url": "https://ceur-ws.org/Vol-3304/paper06.pdf",
      "code": null,
      "abstract": "In Github, developers may not clarify and summarize the critical problems in the bug report titles due to a lack of domain knowledge or poor writing skills. Therefore, it is essential to help practitioners draft high-quality titles. In this study, we propose the BUG-T5 method automatically generating titles by fine-tuning the T5 model. In our empirical analysis, we choose a publicly available corpus from Github. After comparing BUG-T5 with four state-ofthe-art baselines (i.e., TextRank, NMT, Transformer, and iTAPE) on ROUGE metrics, we demonstrate the competitiveness of our proposed method, BUG-T5.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "openalex": {
        "score": 0.5164864864864865,
        "id": "https://openalex.org/W4385840344",
        "doi": "https://doi.org/10.3390/electronics12163456",
        "display_name": "A Comparison of Summarization Methods for Duplicate Software Bug Reports",
        "publication_year": 2023,
        "host_venue": null,
        "is_oa": true,
        "oa_url": "https://www.mdpi.com/2079-9292/12/16/3456/pdf?version=1692092615",
        "best_oa_location": {
          "id": "doi:10.3390/electronics12163456",
          "is_oa": true,
          "landing_page_url": "https://doi.org/10.3390/electronics12163456",
          "pdf_url": "https://www.mdpi.com/2079-9292/12/16/3456/pdf?version=1692092615",
          "source": {
            "id": "https://openalex.org/S4210202905",
            "display_name": "Electronics",
            "issn_l": "2079-9292",
            "issn": [
              "2079-9292"
            ],
            "is_oa": true,
            "is_in_doaj": false,
            "is_core": true,
            "host_organization": "https://openalex.org/P4310310987",
            "host_organization_name": "Multidisciplinary Digital Publishing Institute",
            "host_organization_lineage": [
              "https://openalex.org/P4310310987"
            ],
            "host_organization_lineage_names": [
              "Multidisciplinary Digital Publishing Institute"
            ],
            "type": "journal"
          },
          "license": "cc-by",
          "license_id": "https://openalex.org/licenses/cc-by",
          "version": "publishedVersion",
          "is_accepted": true,
          "is_published": true,
          "raw_source_name": "Electronics",
          "raw_type": "journal-article"
        },
        "abstract": "Bug reports vary in length, while some bug reports are lengthy, others are too brief to describe bugs in detail. In such a case, duplicate bug reports can serve as valuable resources for enriching bug descriptions. However, existing bug summarization methods mainly focused on summarizing a single bug report. In this paper, we focus on summarizing duplicate bug reports. By doing so, we aim to obtain an informative summary of bug reports while reducing redundant sentences in the summary. We apply several text summarization methods to duplicate bug reports. We then compare summarization results generated by different summarization methods and identify the most effective method for summarizing duplicate bug reports. Our comparative experiment reveals that the extractive multi-document method based on TF-IDF is the most effective in the summarization. This method successfully captures the relevant information from duplicate bug reports, resulting in comprehensive summaries. These results contribute to the advancement of bug summarization techniques, especially in summarizing duplicate bug reports.",
        "error": null
      },
      "pdf": {
        "url": "https://ceur-ws.org/Vol-3304/paper06.pdf",
        "source": "yg_page_url",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\Desktop\\codes\\å†™ä½œå‚è€ƒ\\papers\\pdfs_yg\\2022_ICBASE'22_BUG-T5_ A Transformer-based Automatic Title Generation Method for Bug Reports_56fc71c3dc96.pdf",
        "preview_text_path": "C:\\Users\\daoge\\Desktop\\codes\\å†™ä½œå‚è€ƒ\\papers\\extracted_yg\\2022_ICBASE'22_BUG-T5_ A Transformer-based Automatic Title Generation Method for Bug Reports_56fc71c3dc96.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "a47ca78e0b35",
      "page_id": "9978203",
      "title": "BashExplainer: Retrieval-Augmented Bash Code Comment Generation based on Fine-tuned CodeBERT",
      "year": 2022,
      "abbr": "ICSME'22",
      "venue": "Proceedings of the 2022 IEEE International Conference on Software Maintenance and Evolution",
      "type": "conference",
      "tags": [
        "EI",
        "CCF-B"
      ],
      "doi": "10.1109/ICSME55016.2022.00016",
      "url": null,
      "code": "https://github.com/NTDXYG/BashExplainer",
      "abstract": "Developers use shell commands for many tasks, such as file system management, network control, and process management. Bash is one of the most commonly used shells and plays an important role in Linux system development and maintenance. Due to the language flexibility of Bash code, developers who are not familiar with Bash often have difficulty understanding the purpose and functionality of Bash code. In this study, we study Bash code comment generation problem and proposed an automatic method BASHEXPLAINER based on two-stage training strategy. In the first stage, we train a Bash encoder by fine-tuning CodeBERT on our constructed Bash code corpus. In the second stage, we first retrieve the most similar code from the code repository for the target code based on semantic and lexical similarity. Then we use the trained Bash encoder to generate two vector representations. Finally, we fuse these two vector representations via the fusion layer and generate the code comment through the decoder. To show the competitiveness of our proposed method, we construct a high-quality corpus by combining the corpus shared in the previous NL2Bash study and the corpus shared in the NLC2CMD competition. This corpus contains 10,592 Bash codes and corresponding comments. Then we selected ten baselines from previous studies on automatic code comment generation, which cover information retrieval methods, deep learning methods, and hybrid methods. The experimental results show that in terms of the performance measures BLEU-3/4, METEOR, and ROUGR-L, BASHEXPLAINER can outperform all baselines by at least 8.75%, 9.29%, 4.77% and 3.86%. Then we design ablation experiments to show the component setting rationality of BASHEXPLAINER. Later, we conduct a human study to further show the competitiveness of BASHEXPLAINER. Finally, we develop a browser plug-in based on BASHEXPLAINER to facilitate the understanding of the Bash code for developers.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4312794920",
        "doi": "https://doi.org/10.1109/icsme55016.2022.00016",
        "display_name": "BashExplainer: Retrieval-Augmented Bash Code Comment Generation based on Fine-tuned CodeBERT",
        "publication_year": 2022,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": "Developers use shell commands for many tasks, such as file system management, network control, and process management. Bash is one of the most commonly used shells and plays an important role in Linux system development and maintenance. Due to the language flexibility of Bash code, developers who are not familiar with Bash often have difficulty understanding the purpose and functionality of Bash code. In this study, we study Bash code comment generation problem and proposed an automatic method BASHEXPLAINER based on two-stage training strategy. In the first stage, we train a Bash encoder by fine-tuning CodeBERT on our constructed Bash code corpus. In the second stage, we first retrieve the most similar code from the code repository for the target code based on semantic and lexical similarity. Then we use the trained Bash encoder to generate two vector representations. Finally, we fuse these two vector representations via the fusion layer and generate the code comment through the decoder. To show the competitiveness of our proposed method, we construct a high-quality corpus by combining the corpus shared in the previous NL2Bash study and the corpus shared in the NLC2CMD competition. This corpus contains 10,592 Bash codes and corresponding comments. Then we selected ten baselines from previous studies on automatic code comment generation, which cover information retrieval methods, deep learning methods, and hybrid methods. The experimental results show that in terms of the performance measures BLEU-3/4, METEOR, and ROUGR-L, BASHEXPLAINER can outperform all baselines by at least 8.75%, 9.29%, 4.77% and 3.86%. Then we design ablation experiments to show the component setting rationality of BASHEXPLAINER. Later, we conduct a human study to further show the competitiveness of BASHEXPLAINER. Finally, we develop a browser plug-in based on BASHEXPLAINER to facilitate the understanding of the Bash code for developers.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "cfa8d4a50803",
      "page_id": "10.1145/3545258.3545260",
      "title": "EL-CodeBert: Better Exploiting CodeBert to Support Source Code-Related Classification Tasks",
      "year": 2022,
      "abbr": "Internetware'22",
      "venue": "Proceedings of the 13th Asia-Pacific Symposium on Internetware",
      "type": "conference",
      "tags": [
        "EI",
        "CCF-C"
      ],
      "doi": "10.1145/3545258.3545260",
      "url": "https://doi.org/10.1145/3545258.3545260",
      "code": "https://github.com/NTDXYG/EL-CodeBert",
      "abstract": "With the development of deep learning and natural language processing techniques, the performance of many source code-related tasks can be improved by using pre-trained models. Of these pre-trained models, CodeBert is a bi-modal pre-trained model for programming languages and natural languages, which has been successfully used in current source code-related tasks. These previous studies mainly use the output vector of CodeBertâ€™s last layer as the code semantic representation for fine-tuning downstream source code-related tasks. However, this setting may miss the valuable representational information, which may be captured by other layers of CodeBert. To better exploit the representational information in each layer of CodeBert for fine-tuning downstream source code-related tasks, we propose an approach EL-CodeBert. Our approach first extracts the representational information in each layer of CodeBert and views them as a representational information sequence. Then our approach learns the importance of representational information in each layer through the bidirectional recurrent neural network (i.e., Bi-LSTM) and the attention mechanism. To verify the effectiveness of our proposed approach, we select four downstream source code-related classification tasks (i.e., code smell classification, code language classification, technical debt classification, and code comment classification). After compared with state-of-the-art baselines for these tasks, EL-CodeBert can achieve better performance in most performance measures. Finally, we also conduct ablation studies to verify the rationality of the component setting in our proposed approach.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": false,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4295919046",
        "doi": "https://doi.org/10.1145/3545258.3545260",
        "display_name": "EL-CodeBert: Better Exploiting CodeBert to Support Source Code-Related Classification Tasks",
        "publication_year": 2022,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": "With the development of deep learning and natural language processing techniques, the performance of many source code-related tasks can be improved by using pre-trained models. Of these pre-trained models, CodeBert is a bi-modal pre-trained model for programming languages and natural languages, which has been successfully used in current source code-related tasks. These previous studies mainly use the output vector of CodeBert's last layer as the code semantic representation for fine-tuning downstream source code-related tasks. However, this setting may miss the valuable representational information, which may be captured by other layers of CodeBert.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "54e61f001437",
      "page_id": "YANG2022107858",
      "title": "CCGIR: Information retrieval-based code comment generation method for smart contracts",
      "year": 2022,
      "abbr": "KBS'22",
      "venue": "Knowledge-Based Systems",
      "type": "journal",
      "tags": [
        "SCI-Q1",
        "CCF-C"
      ],
      "doi": "https://doi.org/10.1016/j.knosys.2021.107858",
      "url": "https://www.sciencedirect.com/science/article/pii/S0950705121010406",
      "code": "https://github.com/NTDXYG/CCGIR",
      "abstract": "A smart contract is a computer program, which is intended to automatically execute, control or document legally relevant events and actions according to the terms of a contract. About 10% of the security vulnerabilities in smart contracts are caused by misuse of codes without comments. Therefore, there is a need to design effective automatic code comment generation methods for smart contracts. In this study, we propose an information retrieval-based code comment generation method CCGIR for smart contracts. Since code clones are common in smart contract development, CCGIR finds the most similar code in the code repository and reuses its comment through an information retrieval approach from three aspects: semantic similarity, lexical similarity, and syntactic similarity of smart contract codes. We select a corpus, which contains 57,676 unique pairs of <method, comment> from 40,932 real-world smart contracts, as our experimental subject. Then we conduct empirical studies to evaluate the effectiveness of our proposed method. Experimental results show that CCGIR can outperform nine state-of-the-art baselines in terms of three performance measures. Moreover, we perform a human study to further verify that CCGIR can generate higher quality comments. Finally, we find CCGIR can achieve promising performance on the other two code comment generation tasks (i.e., code comment generation for Java and code comment generation for Python). Due to the simplicity and effectiveness of our proposed method, we recommend researchers can use our proposed method as the baseline when evaluating their proposed novel code comment generation methods.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4200276311",
        "doi": "https://doi.org/10.1016/j.knosys.2021.107858",
        "display_name": "CCGIR: Information retrieval-based code comment generation method for smart contracts",
        "publication_year": 2021,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "465189774e3b",
      "page_id": "9825869",
      "title": "DualSC: Automatic Generation and Summarization of Shellcode via Transformer and Dual Learning",
      "year": 2022,
      "abbr": "SANER'22",
      "venue": "2022 IEEE International Conference on Software Analysis, Evolution and Reengineering",
      "type": "conference",
      "tags": [
        "EI",
        "CCF-B"
      ],
      "doi": "10.1109/SANER53432.2022.00052",
      "url": null,
      "code": "https://github.com/NTDXYG/DualSC",
      "abstract": "A shellcode is a small piece of code and it is executed to exploit a software vulnerability, which allows the target computer to execute arbitrary commands from the attacker through a code injection attack. Similar to the purpose of automated vulnerability generation techniques, the automated generation of shellcode can generate attack instructions, which can be used to detect vulnerabilities and implement defensive measures. While the automated summarization of shellcode can help users unfamiliar with shellcode and network information security understand the intent of shellcode attacks. In this study, we propose a novel approach DualSC to solve the automatic shellcode generation and summarization tasks. Specifically, we formalize automatic shellcode generation and summarization as dual tasks, use a shallow Transformer for model construction, and design a normalization method Adjust_QKNorm to adapt these low-resource tasks (i.e., insufficient training data). Finally, to alleviate the out-of-vocabulary problem, we propose a rule-based repair component to improve the performance of automatic shellcode generation. In our empirical study, we select a high-quality corpus Shellcode_IA32 as our empirical subject. This corpus was gathered from two real-world projects based on the line-by-line granularity. We first compare DualSC with six state-of-the-art baselines from the code generation and code summarization domains in terms of four performance measures. The comparison results show the competitiveness of DualSC. Then, we verify the effectiveness of the component setting in DualSC. Finally, we conduct a human study to further verify the effectiveness of DualSC.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4221146651",
        "doi": "https://doi.org/10.1109/saner53432.2022.00052",
        "display_name": "DualSC: Automatic Generation and Summarization of Shellcode via Transformer and Dual Learning",
        "publication_year": 2022,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": "A shellcode is a small piece of code and it is executed to exploit a software vulnerability, which allows the target computer to execute arbitrary commands from the attacker through a code injection attack. Similar to the purpose of automated vulnerability generation techniques, the automated generation of shellcode can generate attack instructions, which can be used to detect vulnerabilities and implement defensive measures. While the automated summarization of shellcode can help users unfamiliar with shellcode and network information security understand the intent of shellcode attacks. In this study, we propose a novel approach DualSC to solve the automatic shellcode generation and summarization tasks. Specifically, we formalize automatic shellcode generation and summarization as dual tasks, use a shallow Transformer for model construction, and design a normalization method Adjust_QKNorm to adapt these low-resource tasks (i.e., insufficient training data). Finally, to alleviate the out-of-vocabulary problem, we propose a rule-based repair component to improve the performance of automatic shellcode generation. In our empirical study, we select a high-quality corpus Shellcode_IA32 as our empirical subject. This corpus was gathered from two real-world projects based on the line-by-line granularity. We first compare DualSC with six state-of-the-art baselines from the code generation and code summarization domains in terms of four performance measures. The comparison results show the competitiveness of DualSC. Then, we verify the effectiveness of the component setting in DualSC. Finally, we conduct a human study to further verify the effectiveness of DualSC.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "49f3756f7984",
      "page_id": "9825881",
      "title": "SOTitle: A Transformer-based Post Title Generation Approach for Stack Overflow",
      "year": 2022,
      "abbr": "SANER'22",
      "venue": "2022 IEEE International Conference on Software Analysis, Evolution and Reengineering",
      "type": "conference",
      "tags": [
        "EI",
        "CCF-B"
      ],
      "doi": "10.1109/SANER53432.2022.00075",
      "url": null,
      "code": "https://github.com/NTDXYG/SOTitle",
      "abstract": "On Stack Overflow, developers can not only browse question posts to solve their programming problems but also gain expertise from the question posts to help improve their programming skills. Therefore, improving the quality of question posts in Stack Overflow has attracted the wide attention of researchers. A concise and precise title can play an important role in helping developers understand the key information of the question post, which can improve the post quality. How-ever, the quality of the generated title is not high due to the lack of professional knowledge related to their questions or the poor presentation ability of developers. A previous study aimed to automatically generate the title by analyzing the code snippets in the question post. However, this study ignored the useful information in the corresponding problem description. Therefore, we propose an approach SOTitle for automatic post title generation by leveraging the code snippets and the problem description in the question post (i.e., the multi-modal input). SOTitle follows the Transformer structure, which can effectively capture long-term dependencies through a multi-head attention mechanism. To verify the effectiveness of SOTitle, we construct a large-scale high-quality corpus from Stack Overflow, which includes 1,168,257 high-quality question posts for four popular programming languages. Experimental results show that SOTitle can significantly outperform six state-of-the-art baselines in both automatic evaluation and human evaluation. To encourage follow-up studies, we make our corpus and approach publicly available.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4221162673",
        "doi": "https://doi.org/10.1109/saner53432.2022.00075",
        "display_name": "SOTitle: A Transformer-based Post Title Generation Approach for Stack Overflow",
        "publication_year": 2022,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": "On Stack Overflow, developers can not only browse question posts to solve their programming problems but also gain expertise from the question posts to help improve their programming skills. Therefore, improving the quality of question posts in Stack Overflow has attracted the wide attention of researchers. A concise and precise title can play an important role in helping developers understand the key information of the question post, which can improve the post quality. How-ever, the quality of the generated title is not high due to the lack of professional knowledge related to their questions or the poor presentation ability of developers. A previous study aimed to automatically generate the title by analyzing the code snippets in the question post. However, this study ignored the useful information in the corresponding problem description. Therefore, we propose an approach SOTitle for automatic post title generation by leveraging the code snippets and the problem description in the question post (i.e., the multi-modal input). SOTitle follows the Transformer structure, which can effectively capture long-term dependencies through a multi-head attention mechanism. To verify the effectiveness of SOTitle, we construct a large-scale high-quality corpus from Stack Overflow, which includes 1,168,257 high-quality question posts for four popular programming languages. Experimental results show that SOTitle can significantly outperform six state-of-the-art baselines in both automatic evaluation and human evaluation. To encourage follow-up studies, we make our corpus and approach publicly available.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "0339f511a134",
      "page_id": "10196969",
      "title": "EDP-BGCNN: Effective Defect Prediction via BERT-based Graph Convolutional Neural Network",
      "year": 2023,
      "abbr": "COMPSAC'23",
      "venue": "Proceedings of the 47th Annual Computers, Software, and Applications Conference",
      "type": "conference",
      "tags": [
        "EI",
        "CCF-C"
      ],
      "doi": "10.1109/COMPSAC57700.2023.00114",
      "url": null,
      "code": null,
      "abstract": "Software defect prediction (SDP) is a critical task that aims to identify potential defects and allocate resources for testing to enhance software reliability. In this study, we present a novel defect prediction framework called EDP-BGCNN, which leverages the power of BERT and graph convolutional neural networks to represent code. Our approach first extracts the codeâ€™s structural semantic features based on its abstract syntax tree (AST), followed by applying BERT for embedded learning to extract the codeâ€™s semantic features. We then use latent Dirichlet allocation (LDA) to extract descriptive semantic features and convert them into a numeric vector. The code and descriptive semantic features are then combined and processed by GraphSMOTE to address the class imbalance problem. Finally, we obtain a more comprehensive representation using graph convolutional neural networks. We evaluated our approach on five open-source projects and compared it with three state-of-the-art deep-learning methods. Our experimental results demonstrate that EDP-BGCNN can achieve significant improvements in AUC (4.9% - 23%) and F1-measure (6.6% - 10.7%) on average.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4385477976",
        "doi": "https://doi.org/10.1109/compsac57700.2023.00114",
        "display_name": "EDP-BGCNN: Effective Defect Prediction via BERT-based Graph Convolutional Neural Network",
        "publication_year": 2023,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "be5d808eb27a",
      "page_id": "zhang-etal-2023-syntax",
      "title": "Syntax-Aware Retrieval Augmented Code Generation",
      "year": 2023,
      "abbr": "EMNLP'23",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP",
      "type": "conference",
      "tags": [
        "CCF-B",
        "EI"
      ],
      "doi": "10.18653/v1/2023.findings-emnlp.90",
      "url": "https://aclanthology.org/2023.findings-emnlp.90/",
      "code": "https://github.com/NUAAZXY/kNN-TRANX",
      "abstract": "Neural code generation models are nowadays widely adopted to generate code from natural language descriptions automatically. Recently, pre-trained neural models equipped with token-level retrieval capabilities have exhibited great potentials in neural machine translation. However, applying them directly to code generation experience challenges: the use of the retrieval-based mechanism inevitably introduces extraneous noise to the generation process, resulting in even syntactically incorrect code. Computationally, such models necessitate frequent searches of the cached datastore, which turns out to be time-consuming. To address these issues, we propose $k$NN-TRANX, a token-level retrieval augmented code generation method. $k$NN-TRANX allows for searches in smaller datastores tailored for the code generation task. It leverages syntax constraints for the retrieval of datastores, which reduces the impact of retrieve noise. We evaluate $k$NN-TRANX on two public datasets and the experimental results confirm the effectiveness of our approach.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": false,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4389520043",
        "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.90",
        "display_name": "Syntax-Aware Retrieval Augmented Code Generation",
        "publication_year": 2023,
        "host_venue": null,
        "is_oa": true,
        "oa_url": "https://aclanthology.org/2023.findings-emnlp.90.pdf",
        "best_oa_location": {
          "id": "doi:10.18653/v1/2023.findings-emnlp.90",
          "is_oa": true,
          "landing_page_url": "https://doi.org/10.18653/v1/2023.findings-emnlp.90",
          "pdf_url": "https://aclanthology.org/2023.findings-emnlp.90.pdf",
          "source": null,
          "license": "cc-by",
          "license_id": "https://openalex.org/licenses/cc-by",
          "version": "publishedVersion",
          "is_accepted": true,
          "is_published": true,
          "raw_source_name": "Findings of the Association for Computational Linguistics: EMNLP 2023",
          "raw_type": "proceedings-article"
        },
        "abstract": "Neural code generation models are nowadays widely adopted to generate code from natural language descriptions automatically. Recently, pre-trained neural models equipped with token-level retrieval capabilities have exhibited great potentials in neural machine translation. However, applying them directly to code generation experience challenges: the use of the retrieval-based mechanism inevitably introduces extraneous noise to the generation process, resulting in even syntactically incorrect code. Computationally, such models necessitate frequent searches of the cached datastore, which turns out to be time-consuming. To address these issues, we propose kNN-TRANX, a token-level retrieval augmented code generation method. kNN-TRANX allows for searches in smaller datastores tailored for the code generation task. It leverages syntax constraints for the retrieval of datastores, which reduces the impact of retrieve noise. We evaluate kNN-TRANX on two public datasets and the experimental results confirm the effectiveness of our approach.",
        "error": null
      },
      "pdf": {
        "url": "https://aclanthology.org/2023.findings-emnlp.90.pdf",
        "source": "openalex",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\Desktop\\codes\\å†™ä½œå‚è€ƒ\\papers\\pdfs_yg\\2023_EMNLP'23_Syntax-Aware Retrieval Augmented Code Generation_be5d808eb27a.pdf",
        "preview_text_path": "C:\\Users\\daoge\\Desktop\\codes\\å†™ä½œå‚è€ƒ\\papers\\extracted_yg\\2023_EMNLP'23_Syntax-Aware Retrieval Augmented Code Generation_be5d808eb27a.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "4627f6011e7b",
      "page_id": "yang2023syntax",
      "title": "A syntax-guided multi-task learning approach for Turducken-style code generation",
      "year": 2023,
      "abbr": "EMSE'23",
      "venue": "Empirical Software Engineering",
      "type": "journal",
      "tags": [
        "SCI-Q2",
        "CCF-B"
      ],
      "doi": "10.1007/s10664-023-10372-1",
      "url": null,
      "code": "https://github.com/NTDXYG/TurduckenGen",
      "abstract": "Due to the development of pre-trained language models, automated code generation techniques have shown great promise in recent years. However, the generated code will not always adhere to syntactic constraints of the target language, especially in the case of Turducken-style code, where declarative code snippets are embedded within imperative programs. In this study, we summarize three significant challenges in regards to syntactic constraints: (1) the efficient representation of syntactic constraints, (2) the effective integration of syntactic information, and (3) the scalable syntax-first decoding algorithm. To address these challenges, we propose a syntax-guided multi-task learning approach TurduckenGen. Specifically, we first explicitly append the type information to the code tokens to capture the representation of syntactic constraints. Then we formalize code generation with syntactic constraint representation as an auxiliary task to enable the model to learn the syntactic constraints of the code. Finally, the syntactically correct code is selected accurately from the multiple candidates with the help of the compiler feedback. Extensive experiments and comprehensive analysis demonstrate the effectiveness and general applicability of our approach after being compared with six state-of-the-art baselines on two Turducken-style code datasets. Finally, we conducted a human study and found the code quality generated by our approach is better than baselines in terms of code readability and semantic similarity.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4387642109",
        "doi": "https://doi.org/10.1007/s10664-023-10372-1",
        "display_name": "A syntax-guided multi-task learning approach for Turducken-style code generation",
        "publication_year": 2023,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "205b03e9c9bf",
      "page_id": "YANG2023111577",
      "title": "ExploitGen: Template-augmented exploit code generation based on CodeBERT",
      "year": 2023,
      "abbr": "JSS'23",
      "venue": "Journal of Systems and Software",
      "type": "journal",
      "tags": [
        "SCI-Q2",
        "CCF-B"
      ],
      "doi": "https://doi.org/10.1016/j.jss.2022.111577",
      "url": "https://www.sciencedirect.com/science/article/pii/S0164121222002539",
      "code": "https://github.com/NTDXYG/ExploitGen",
      "abstract": "Exploit code is widely used for detecting vulnerabilities and implementing defensive measures. However, automatic generation of exploit code for security assessment is a challenging task. In this paper, we propose a novel template-augmented exploit code generation approach ExploitGen based on CodeBERT. Specifically, we first propose a rule-based Template Parser to generate template-augmented natural language descriptions (NL). Both the raw and template-augmented NL sequences are encoded to context vectors by the respective encoders. For better learning semantic information, ExploitGen incorporates a semantic attention layer, which uses the attention mechanism to extract and calculate each layerâ€™s representational information. In addition, ExploitGen computes the interaction information between the template information and the semantics of the raw NL and designs a residual connection to append the template information into the semantics of the raw NL. Comprehensive experiments on two datasets show the effectiveness of ExploitGen after comparison with six state-of-the-art baselines. Apart from the automatic evaluation, we conduct a human study to evaluate the quality of generated code in terms of syntactic and semantic correctness. The results also confirm the effectiveness of ExploitGen.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": false,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4311423650",
        "doi": "https://doi.org/10.1016/j.jss.2022.111577",
        "display_name": "ExploitGen: Template-augmented exploit code generation based on CodeBERT",
        "publication_year": 2022,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "b6f9f464f2b4",
      "page_id": "DBLP:conf/seke/ShenJ0Y23",
      "title": "An Empirical Study of Adversarial Training in Code Comment Generation",
      "year": 2023,
      "abbr": "SEKE'23",
      "venue": "Proceedings of the 35th International Conference on Software Engineering and Knowledge Engineering",
      "type": "conference",
      "tags": [
        "EI",
        "CCF-C"
      ],
      "doi": "10.18293/SEKE2023-108",
      "url": "https://doi.org/10.18293/SEKE2023-108",
      "code": null,
      "abstract": "The code comment generation task is designed for developers to understand programs more quickly during development and maintenance. However, the existing automatic code comment generation models can not generate valuable comments for developers. It is necessary to explore a technology that can optimize the performance of code comment generation models without changing the model. We consider adversarial training as the experimental object, which can improve the robustness and generalization of the model. We present a large-scale study to experimentally validate the performance of gradient-based adversarial training methods in the code comment generation task. The results show that adversarial training can improve the model performance by generating adversarial examples without changing the model. Our empirical study can provide a new perspective for researchers to improve the performance of code comment generation models.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": false,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4386365667",
        "doi": "https://doi.org/10.18293/seke2023-108",
        "display_name": "An Empirical Study of Adversarial Training in Code Comment Generation",
        "publication_year": 2023,
        "host_venue": null,
        "is_oa": true,
        "oa_url": "https://doi.org/10.18293/seke2023-108",
        "best_oa_location": {
          "id": "doi:10.18293/seke2023-108",
          "is_oa": true,
          "landing_page_url": "http://doi.org/10.18293/seke2023-108",
          "pdf_url": "https://doi.org/10.18293/seke2023-108",
          "source": {
            "id": "https://openalex.org/S4220650826",
            "display_name": "Proceedings/Proceedings of the ... International Conference on Software Engineering and Knowledge Engineering",
            "issn_l": "2325-9000",
            "issn": [
              "2325-9000",
              "2325-9086"
            ],
            "is_oa": false,
            "is_in_doaj": false,
            "is_core": true,
            "host_organization": null,
            "host_organization_name": null,
            "host_organization_lineage": [],
            "host_organization_lineage_names": [],
            "type": "journal"
          },
          "license": null,
          "license_id": null,
          "version": "publishedVersion",
          "is_accepted": true,
          "is_published": true,
          "raw_source_name": "International Conferences on Software Engineering and Knowledge Engineering",
          "raw_type": "proceedings-article"
        },
        "abstract": "The code comment generation task is designed for developers to understand programs more quickly during development and maintenance.However, the existing automatic code comment generation models can not generate valuable comments for developers.It is necessary to explore a technology that can optimize the performance of code comment generation models without changing the model.We consider adversarial training as the experimental object, which can improve the robustness and generalization of the model.We present a large-scale study to experimentally validate the performance of gradient-based adversarial training methods in the code comment generation task.The results show that adversarial training can improve the model performance by generating adversarial examples without changing the model.Our empirical study can provide a new perspective for researchers to improve the performance of code comment generation models.",
        "error": null
      },
      "pdf": {
        "url": "https://doi.org/10.18293/seke2023-108",
        "source": "openalex",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\Desktop\\codes\\å†™ä½œå‚è€ƒ\\papers\\pdfs_yg\\2023_SEKE'23_An Empirical Study of Adversarial Training in Code Comment Generation_b6f9f464f2b4.pdf",
        "preview_text_path": "C:\\Users\\daoge\\Desktop\\codes\\å†™ä½œå‚è€ƒ\\papers\\extracted_yg\\2023_SEKE'23_An Empirical Study of Adversarial Training in Code Comment Generation_b6f9f464f2b4.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": true,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "0dd06bcf5f27",
      "page_id": "Zhang2023CCGRA",
      "title": "CCGRA: Smart Contract Code Comment Generation with Retrieval-enhanced Approach",
      "year": 2023,
      "abbr": "SEKE'23",
      "venue": "Proceedings of the 35th International Conference on Software Engineering and Knowledge Engineering",
      "type": "conference",
      "tags": [
        "EI",
        "CCF-C"
      ],
      "doi": "10.18293/SEKE2023-090",
      "url": "https://doi.org/10.18293/SEKE2023-090",
      "code": "https://github.com/ZZHbible/CCGRA",
      "abstract": "Smart contracts are self-executing programs on the blockchain that are critical to a range of industries, including finance, supply chain management, and healthcare. However, comprehending smart contracts can be challenging due to a lack of effective comments in most user-defined code. To address this challenge, we propose a novel retrieval-enhanced approach CCGRA that leverages retrieval knowledge to generate high-quality comments for Solidity language code. Our approach carefully eliminates duplicated data and template data in the widely-used smart contract dataset to ensure a high-quality corpus. Extensive experiments and comprehensive analysis demonstrate the effectiveness applicability of our approach after being compared with eight state-of-the-art baselines. Finally, we conduct a human study and find the comment quality generated by our approach is better than baselines in terms of similarity, naturalness, and informativeness.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": false,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4386380948",
        "doi": "https://doi.org/10.18293/seke2023-090",
        "display_name": "CCGRA: Smart Contract Code Comment Generation with Retrieval-enhanced Approach",
        "publication_year": 2023,
        "host_venue": null,
        "is_oa": true,
        "oa_url": "https://doi.org/10.18293/seke2023-090",
        "best_oa_location": {
          "id": "doi:10.18293/seke2023-090",
          "is_oa": true,
          "landing_page_url": "http://doi.org/10.18293/seke2023-090",
          "pdf_url": "https://doi.org/10.18293/seke2023-090",
          "source": {
            "id": "https://openalex.org/S4220650826",
            "display_name": "Proceedings/Proceedings of the ... International Conference on Software Engineering and Knowledge Engineering",
            "issn_l": "2325-9000",
            "issn": [
              "2325-9000",
              "2325-9086"
            ],
            "is_oa": false,
            "is_in_doaj": false,
            "is_core": true,
            "host_organization": null,
            "host_organization_name": null,
            "host_organization_lineage": [],
            "host_organization_lineage_names": [],
            "type": "journal"
          },
          "license": null,
          "license_id": null,
          "version": "publishedVersion",
          "is_accepted": true,
          "is_published": true,
          "raw_source_name": "International Conferences on Software Engineering and Knowledge Engineering",
          "raw_type": "proceedings-article"
        },
        "abstract": "Smart contracts are self-executing programs on the blockchain that are critical to a range of industries, including finance, supply chain management, and healthcare.However, comprehending smart contracts can be challenging due to a lack of effective comments in most user-defined code.To address this challenge, we propose a novel retrieval-enhanced approach CC-GRA that leverages retrieval knowledge to generate high-quality comments for Solidity language code.Our approach carefully eliminates duplicated data and template data in the widely-used smart contract dataset to ensure a high-quality corpus.Extensive experiments and comprehensive analysis demonstrate the effectiveness applicability of our approach after being compared with eight state-of-the-art baselines.Finally, we conduct a human study and find the comment quality generated by our approach is better than baselines in terms of similarity, naturalness, and informativeness.",
        "error": null
      },
      "pdf": {
        "url": "https://doi.org/10.18293/seke2023-090",
        "source": "openalex",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\Desktop\\codes\\å†™ä½œå‚è€ƒ\\papers\\pdfs_yg\\2023_SEKE'23_CCGRA_ Smart Contract Code Comment Generation with Retrieval-enhanced Approach_0dd06bcf5f27.pdf",
        "preview_text_path": "C:\\Users\\daoge\\Desktop\\codes\\å†™ä½œå‚è€ƒ\\papers\\extracted_yg\\2023_SEKE'23_CCGRA_ Smart Contract Code Comment Generation with Retrieval-enhanced Approach_0dd06bcf5f27.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": true,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "ad2fae6ccf3e",
      "page_id": "20231310",
      "title": "åŸºäºåŒé‡ä¿¡æ¯æ£€ç´¢çš„Bashä»£ç æ³¨é‡Šç”Ÿæˆæ–¹æ³•",
      "year": 2023,
      "abbr": "è½¯ä»¶å­¦æŠ¥'23",
      "venue": "è½¯ä»¶å­¦æŠ¥",
      "type": "journal",
      "tags": [
        "EI",
        "ä¸­æ–‡CCF-A",
        "åŒ—æ ¸"
      ],
      "doi": "10.13328/j.cnki.jos.006690",
      "url": null,
      "code": "https://github.com/ExplainBash/explainbash",
      "abstract": "Bash is the default shell command language for Linux, which plays an important role in the development and maintenance of Linux systems. Nevertheless, understanding the purpose and functionality of the Bash code is a challenging task. Therefore, an automatic method ExplainBash is proposed based on dual information retrieval for automatic Bash code comment generation. Specifically, the proposed method is based on semantic similarity and lexical similarity to perform dual information retrieval, which aims to generate high-quality code comments. For semantic similarity, CodeBERT and BERT-whitening operator are used to learn the code semantic representation, and Euclidean distance is resorted to compute semantic similarity; while for lexical similarity, code is represented as a set of code tokens, then the edit distance is resorted to compute lexical similarity. A high-quality corpus is constructed based on the corpus shared in the NL2Bash study and the data shared in the NLC2CMD competition. After that, nine state-of-the-art baselines are selected from the automatic code comment generation domain, which cover the information retrieval-based methods and deep learning-based methods. Results of empirical study and human study verify the effectiveness of the proposed method. Ablation experiments are also designed to analyze the rationality of the settings (such as retrieval strategy, BERT-whitening operator) in the proposed method. Finally, a browser plug-in is developed based on the proposed method to facilitate the code comprehension of the Bash code.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": false
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 0.016511627906976745,
        "id": "https://openalex.org/W3208793882",
        "doi": "https://doi.org/10.5281/zenodo.4604023",
        "display_name": "ESPnet2 pretrained model, Emiru Tsunoo/aishell_asr_train_asr_streaming_transformer_raw_zh_char_sp_valid.acc.ave, fs=16k, lang=zh",
        "publication_year": 2021,
        "host_venue": null,
        "is_oa": true,
        "oa_url": "https://zenodo.org/record/4604023",
        "best_oa_location": {
          "id": "pmh:oai:zenodo.org:4604023",
          "is_oa": true,
          "landing_page_url": "https://zenodo.org/record/4604023",
          "pdf_url": null,
          "source": null,
          "license": "cc-by",
          "license_id": "https://openalex.org/licenses/cc-by",
          "version": "submittedVersion",
          "is_accepted": false,
          "is_published": false,
          "raw_source_name": "",
          "raw_type": "info:eu-repo/semantics/other"
        },
        "abstract": "This model was trained by Emiru Tsunoo using aishell recipe in espnet. <strong>Python API</strong><pre><code class=\"language-python\">See https://github.com/espnet/espnet_model_zoo</code></pre> <strong>Evaluate in the recipe</strong><pre><code class=\"language-bash\">git clone https://github.com/espnet/espnet cd espnet git checkout 006a7f71394510b04817aaa4593dda2ae6f6d508 pip install -e . cd egs2/aishell/asr1 ./run.sh --skip_data_prep false --skip_train true --download_model Emiru Tsunoo/aishell_asr_train_asr_streaming_transformer_raw_zh_char_sp_valid.acc.ave</code> </pre> <strong>Results</strong><pre><code> # RESULTS ## Environments - date: `Sun Feb 21 01:15:01 JST 2021` - python version: `3.8.5 (default, Sep 4 2020, 07:30:14) [GCC 7.3.0]` - espnet version: `espnet 0.9.7` - pytorch version: `pytorch 1.4.0` - Git hash: `006a7f71394510b04817aaa4593dda2ae6f6d508` - Commit date: `Thu Feb 18 10:27:42 2021 +0900` ## asr_train_asr_streaming_transformer_raw_zh_char_sp ### WER |dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err| |---|---|---|---|---|---|---|---|---| |decode_asr_streaming_lm_lm_train_lm_zh_char_valid.loss.ave_asr_model_valid.acc.ave/dev|14326|14326|57.8|42.2|0.0|0.0|42.2|42.2| |decode_asr_streaming_lm_lm_train_lm_zh_char_valid.loss.ave_asr_model_valid.acc.ave/test|7176|7176|56.2|43.8|0.0|0.0|43.8|43.8| ### CER |dataset|Snt|Wrd|Corr|Sub|Del|Ins|Err|S.Err| |---|---|---|---|---|---|---|---|---| |decode_asr_streaming_lm_lm_train_lm_zh_char_valid.loss.ave_asr_model_valid.acc.ave/dev|14326|205341|94.0|5.9|0.1|0.1|6.1|42.2| |decode_asr_streaming_lm_lm_train_lm_zh_char_valid.loss.ave_asr_model_valid.acc.ave/test|7176|104765|93.6|6.3|0.2|0.1|6.6|43.8|</code></pre> <strong>ASR config</strong><pre><code>config: conf/train_asr_streaming_transformer.yaml print_config: false log_level: INFO dry_run: false iterator_type: sequence output_dir: exp/asr_train_asr_streaming_transformer_raw_zh_char_sp ngpu: 1 seed: 0 num_workers: 1 num_att_plot: 0 dist_backend: nccl dist_init_method: env:// dist_world_size: 4 dist_rank: 0 local_rank: 0 dist_master_addr: localhost dist_master_port: 35575 dist_launcher: null multiprocessing_distributed: true cudnn_enabled: true cudnn_benchmark: false cudnn_deterministic: true collect_stats: false write_collected_feats: false max_epoch: 20 patience: null val_scheduler_criterion: - valid - acc early_stopping_criterion: - valid - loss - min best_model_criterion: - - valid - acc - max keep_nbest_models: 10 grad_clip: 5 grad_clip_type: 2.0 grad_noise: false accum_grad: 2 no_forward_run: false resume: true train_dtype: float32 use_amp: false log_interval: null unused_parameters: false use_tensorboard: true use_wandb: false wandb_project: null wandb_id: null pretrain_path: null init_param: [] freeze_param: [] num_iters_per_epoch: null batch_size: 80 valid_batch_size: null batch_bins: 1000000 valid_batch_bins: null train_shape_file: - exp/asr_stats_raw_zh_char_sp/train/speech_shape - exp/asr_stats_raw_zh_char_sp/train/text_shape.char valid_shape_file: - exp/asr_stats_raw_zh_char_sp/valid/speech_shape - exp/asr_stats_raw_zh_char_sp/valid/text_shape.char batch_type: folded valid_batch_type: null fold_length: - 51200 - 150 sort_in_batch: descending sort_batch: descending multiple_iterator: false chunk_length: 500 chunk_shift_ratio: 0.5 num_cache_chunks: 1024 train_data_path_and_name_and_type: - - dump/raw/train_sp/wav.scp - speech - sound - - dump/raw/train_sp/text - text - text valid_data_path_and_name_and_type: - - dump/raw/dev/wav.scp - speech - sound - - dump/raw/dev/text - text - text allow_variable_data_keys: false max_cache_size: 0.0 max_cache_fd: 32 valid_max_cache_size: null optim: adam optim_conf: lr: 0.001 scheduler: warmuplr scheduler_conf: warmup_steps: 25000 token_list: - - - çš„ - ä¸€ - åœ¨ - å - ä¸­ - æ˜¯ - äºº - æœ‰ - äºŒ - ä¸Š - äº† - ä¸ - å›½ - å¸‚ - å¤§ - ä¸š - ä¸º - å¹´ - ä¸‰ - å‘ - ä¸ª - åˆ† - å‡º - ä¼š - å…¬ - è¡Œ - åœ° - æˆ - è¿™ - å’Œ - åˆ° - äº” - äº§ - æ—¶ - å¯¹ - æˆ¿ - ç™¾ - èƒ½ - åœº - æ¥ - ä»¥ - æ–° - ä¹‹ - æ—¥ - è€… - å°† - ç° - å›› - è¦ - å®¶ - èµ„ - å¤š - æœˆ - ä¹Ÿ - æ–¹ - å - æœº - ä¸‹ - å‰ - é›¶ - æ¯” - äº - ç”Ÿ - ç‚¹ - å¼€ - åŠ¨ - é«˜ - ç» - è¿› - æŠ¥ - ä½“ - èµ› - å­ - ä¸‡ - è½¦ - ç”¨ - é‡‘ - å¸ - å¯ - è¢« - è¿‡ - æ‰‹ - æœ¬ - ä½œ - è‡ª - å…¨ - å…« - å…­ - æœ€ - ä»· - ç›® - ç”µ - éƒ¨ - äº¤ - ä¹ - ä¸ƒ - é¢ - æˆ‘ - ä¼ - åŠ  - å° - åº¦ - å® - åŒ - åŸ - å·¥ - å…¶ - åŠ› - å®š - è€Œ - å…ƒ - åˆ - å·² - å†… - ä¸ - æ³• - è¿˜ - å…³ - ç½‘ - å¾— - ä»– - å°± - å…¥ - å - å“ - å¥³ - è®° - ç† - äº‹ - é•¿ - ä¸¤ - å•† - éƒ½ - ä»¬ - äº¬ - å¹¶ - ä½† - å¹³ - åˆ¶ - ä¿ - æ® - æœŸ - åŒ– - ä¸» - é‡ - è¡¨ - æ¬¡ - ç›¸ - é‡ - é€š - é“ - æ”¿ - æ‰€ - å¤© - ç¬¬ - åˆ© - é—´ - æµ· - æ•° - åŠ¡ - æ - åŒ— - å±• - å‘˜ - ç®¡ - æŠ• - å›  - å»º - å¥½ - å¤– - åŒº - æ›´ - ç¤º - å¢ - ä» - è®¡ - ä¿¡ - æ€§ - ç­‰ - è¿ - é¡¹ - åº” - å½“ - æ”¶ - ä½ - ç€ - èµ· - å­¦ - å° - æ°‘ - æŒ - è§„ - è®¾ - æ˜ - è‚¡ - æ­£ - æ²¡ - å¿ƒ - ç„¶ - å¾ˆ - ä»Š - è°ƒ - å» - å®‰ - æ­¤ - ä¸œ - é˜Ÿ - å¦‚ - çº¿ - ç§‘ - ä¸– - æ—  - è¾¾ - èº« - æœ - è¯ - åŸº - å— - ç”· - éœ€ - æ ‡ - å¸ƒ - æƒ… - æ ¼ - è¿‘ - æ­¥ - æœª - è´¹ - æ±‚ - å¼ - æ¶ˆ - åƒ - ç¾ - äº› - é‡Œ - ç±³ - å‘ - çœ‹ - ç»­ - æ¯ - æ„ - æ¥ - é—¨ - å› - åŠ - é”€ - è€ - è· - æ€» - ç›‘ - æ‰“ - è” - è‡³ - äº¿ - è¯´ - è®¯ - ä½ - ç¯ - ä»¶ - æ•´ - æ°´ - æŠ€ - è·¯ - é™¢ - å±€ - ç‰¹ - è¯¥ - ç»Ÿ - ç”± - å”® - è´­ - å¼º - æ”¹ - é—® - ä¹ - æ¥¼ - æ¶¨ - å¤„ - å†³ - è®© - ç³» - æˆ· - é¢˜ - æ¨ - å°‘ - å¹¿ - æ˜¾ - é™ - è·‘ - å½± - åª - é€‰ - ç§° - åˆ› - æ˜“ - æˆ˜ - é¦– - å®Œ - æ¡ˆ - ç­– - å¸¸ - æŸ¥ - å‚ - ç§ - ç‰Œ - ç¨‹ - é“¶ - å¤‡ - è®¤ - è¥ - ç«‹ - åŠ¿ - ç»“ - é€  - è¶… - å·± - å‡† - å­˜ - é™© - çƒ - å„ - ä»£ - ä½ - å† - åš - çº§ - æ¬¾ - æ”¾ - ç‰© - å‘Š - åŸ - å‹ - è½¬ - è­¦ - å‘¨ - ç•Œ - å¼  - æ · - ä¼  - è¾ƒ - é£ - å• - ç»™ - å¥¹ - å· - è§£ - åˆ™ - è§† - æŒ‡ - é¢„ - å‡ - å - ä¾› - èµ° - æ¯ - å– - å¯¼ - æœ - é›† - æ–‡ - å˜ - å®¢ - æ’ - ç‰‡ - å¤´ - ä»» - ç§¯ - æœ¯ - ç‡ - å‹ - å†› - æ–¯ - ç ” - åˆ« - é - ç›´ - æ™º - é€Ÿ - ç»„ - æ˜Ÿ - é¢† - å£ - ä»½ - å² - é©¬ - ç‹ - å¿« - ä¸“ - ç¤¾ - ä½¿ - å›¢ - æ¨¡ - å™¨ - éš¾ - æ´» - æ‹‰ - æˆ– - çº¦ - æ–½ - æº - æ„ - æ”¯ - åŒ» - å„¿ - å¸¦ - æœ - å…ˆ - æƒ³ - å¼• - ä¹ˆ - åŠ - ç…§ - ç‹ - æƒ - å¾® - å— - å§‹ - è - æ·± - å£« - æ¸¸ - ç»© - ä»… - å†µ - åª’ - éš - åŠ - è¶Š - å¹… - ç¡® - æ³¨ - ç±» - äº‰ - ç¨ - é™ - æµ - å‡ - æ§ - å…… - é¢ - æœ› - è¿ - åˆ’ - å¥¥ - äºš - åŒ… - å¨± - è¥¿ - è´¢ - å€¼ - ä¼¤ - æŸ - è‡´ - ç»ˆ - ç©º - æµ - ä¼— - é™… - åœŸ - ä¹° - ä» - è‚² - å¸ˆ - æ±½ - çŸ¥ - è´¨ - æ€ - å…· - æ - è´£ - ç©¶ - éœ² - æ¡ - å‡  - å±… - å…± - å“ - å - ç«™ - å†  - èŠ‚ - å­£ - ä¼˜ - å§” - å®… - è§‚ - äº’ - è§ - èŒƒ - å¢ƒ - æ„Ÿ - è´Ÿ - æ®µ - å¤± - é‡‡ - å¥— - åŸŸ - å°” - ä¸¾ - ä½• - å…‰ - æ°” - è½ - åš - æ•™ - é”¦ - æ— - å±± - ä¾ - ç»§ - æ - å½¢ - å›¾ - å®¡ - ç« - ç›Š - æ–­ - è´· - æ•ˆ - åºœ - å¤ - è®¸ - å®¹ - å¥ - å‡» - è¶³ - åˆ - è¯‰ - åŠ© - å­© - è‰² - åœ - ç¥¨ - åŒ - æ‹¿ - æ¿ - æ¾ - çƒ­ - é‚£ - æŠŠ - å´ - æ¸… - åˆ˜ - è®® - è€ƒ - å‡ - æ›¾ - ç–‘ - ä¾‹ - é™¤ - åŠŸ - å  - ä½  - è¯• - æ ¹ - æ¸¯ - å¤ª - ç¦» - æ‰ - è´§ - çª - æ¶‰ - ä¸” - åˆ¸ - é… - ç›˜ - å³ - åº“ - ä»˜ - ç ´ - èŒ - æ¼” - å†œ - ç½® - çºª - è®º - çœŸ - é¾™ - æ™š - è£… - çˆ± - å· - ç»ƒ - æ­» - å‹ - äº² - ä¸¥ - è¯„ - ç”° - è¯ - æ‰˜ - æŠ¤ - ç« - å - çº¢ - æ±Ÿ - å…‹ - å– - è¨€ - ç§Ÿ - å–„ - é¢‘ - æ™® - é£ - éªŒ - è¡¥ - è¾¹ - æ»¡ - è±¡ - è½¯ - ç®— - é­ - é¦€ - é—» - ç¨³ - å‚ - è¿œ - è‹¹ - é’± - æ‹… - åˆ¤ - å®˜ - è™½ - æ¹¾ - æŒ‰ - æ˜¨ - æ ¡ - å¿… - å›­ - ç•¥ - æ•‘ - å¸Œ - åº• - æ‰§ - å¤Ÿ - å¾ - æ‹ - å† - åƒ - æ¶¦ - å±‚ - å€º - ä¾¿ - éšœ - å›´ - åº· - åº— - å¾€ - åˆ— - æ—© - æµ‹ - å½• - å¦ - é¦™ - å® - é˜³ - ç´¢ - æ ¸ - å…´ - æ£€ - çŠ¶ - è‹± - æ‘ - æ–™ - äº‘ - ç•™ - å¤« - ç§» - å¥– - ç—… - ä¸´ - è½» - çœ - ç§’ - æ¿€ - è¯· - é© - å± - é‡ - è·Œ - ç»´ - æ‰¹ - å¾· - æ‰¿ - ç«¯ - ä»‹ - ç²¾ - å¤º - ç¾¤ - åˆ - èƒœ - å¡ - å°½ - èŠ± - è¾† - å®ƒ - æ•… - ç¥ - å±Š - æ²» - é€ - æ™¯ - ç™½ - å‰¯ - ä»€ - å®£ - é“ - æ¨ - è·³ - å‡ - ç™» - ç¦ - é’ - è¯ - å©š - å…» - å¹• - è¿ - çŸ­ - è®¿ - ä¿® - çº· - å¾‹ - å·¦ - è§’ - é…’ - æ‹¬ - çˆ† - å«Œ - å¾„ - å® - è‘£ - é€‚ - é€ - åˆš - é˜² - é™ˆ - åˆ - å·® - åº­ - ç‹¬ - æ³¢ - é£Ÿ - è¯† - ä¼¼ - å€™ - é»„ - äº¡ - è®­ - ä¹¦ - é€€ - å¾… - èˆª - å— - å†² - æ‰© - å´ - ç”š - ç”³ - ä¼Ÿ - çœ¼ - å·´ - è§‰ - æ‰¾ - æ¢ - ä¹‰ - è½® - æ»‘ - å¸­ - å¤® - é€ - å³ - å« - ä¹˜ - çŸ³ - å­— - ç½ª - ç½— - æ³³ - å­™ - æ - å¿— - å¦ - æ¯ - ç»¿ - æŠ¢ - æ­¢ - ä»¤ - ç«¥ - å¦ˆ - å² - åˆ‘ - æ´² - è¿° - ç©¿ - å¿µ - çº³ - æŸ - å¯Œ - å… - æ¯’ - ç»œ - ç´§ - å¦» - ä¹ - è±ª - ç´  - å®³ - å€’ - å¸ - è¡— - ä¿ƒ - æ‹© - æ€ - è¿½ - å·¨ - çŠ¯ - å£° - æ„¿ - æ™¨ - æ€ - è°ˆ - æ²³ - é•‡ - å°¼ - è·Ÿ - åº† - é“¾ - æª - å€Ÿ - èµ” - å¯† - åœ³ - è´´ - è‹ - æ¸© - éª— - ä¹  - æ‘„ - ç‰ˆ - å¸® - å¸ - é˜¶ - é˜¿ - è¿ - é©¾ - é»‘ - è¶‹ - å¿ - ç§ - åƒ - ç–— - ç»† - è™‘ - è„‘ - éŸ© - äº® - æ—… - æŠ“ - ç½š - è‰¯ - èƒŒ - è„¸ - ç» - ç­ - å± - ç¡€ - æˆ - æˆ´ - æ‹› - å‘½ - å°š - ç¼º - ä¼™ - é¡» - çˆ¶ - å¤œ - åˆ‡ - æ“ - æŒ¥ - æ´¾ - å»¶ - æ’ - æŠ« - è¡£ - å‰§ - é™† - ç«Ÿ - ç­¾ - æ¬§ - äº« - æ˜¥ - å¾½ - è£ - å¿ - å¯ - è‰º - å®— - å‘³ - å¯Ÿ - ä¼° - å‡€ - å‹Ÿ - æ‹¥ - é‡Š - å–œ - é¡º - åŠ± - é  - æ¸ - å…° - æ²¹ - ä½³ - å›° - é’ˆ - è¿· - å†™ - æ - ç¡¬ - æ¡¥ - åš - è®¢ - æ‹³ - ç´¯ - ç›– - å®¤ - æŸ - æˆª - è· - é©¶ - æ—¬ - æ­Œ - æ‚‰ - çƒˆ - åº - æ‚£ - å¹² - æ±¡ - åœˆ - æ° - é¡¶ - è´¥ - ä¼´ - å½’ - æ¢ - æ› - æ€€ - æ€¥ - æ±  - ç»‡ - ç§€ - å§ - å³° - é¡¾ - è¯¯ - é”® - ä¸° - ç© - æ±‰ - å¤ - å½© - è®¨ - æœ‹ - æŠ— - åˆº - æŒ‘ - è¡€ - å‡Œ - æ—§ - æ‹Ÿ - æ™’ - é™„ - æƒŠ - æ¬¢ - åŠ³ - ä¸ˆ - æ’­ - å¾ - å— - æ¹– - ç¬‘ - é¦† - éŸ³ - é˜µ - å - è°· - å¼‚ - æ€ - å¤ - é¾„ - ç†Ÿ - è‹¥ - æƒ  - ä¼‘ - æ°¸ - å“ª - æš‚ - è¾“ - ç» - å° - å†° - ç¼“ - æš– - å¬ - é¿ - å˜‰ - å¯» - åŸ¹ - ç­¹ - ä¼¦ - é›ª - è´¦ - æš´ - ç®€ - äºˆ - ä¸½ - æ³½ - åˆ» - é‡ - å¨ - å®½ - ç¬” - è¯­ - æ­¦ - ç‚’ - è™š - æ¶ - å¥‡ - å“¥ - å°¤ - åº§ - è¿… - ç²‰ - å€ - æœ± - å±‹ - èˆ¬ - é”™ - æ´¥ - å¼Ÿ - æ±‡ - æ¦‚ - é¼“ - æ‰ - éƒ‘ - é’Ÿ - å¬ - ç¤¼ - ç¦ - æŠ˜ - ç¼© - é” - æ¶› - ä¹¡ - è‚¥ - å¹¸ - é›¨ - æ¢¦ - è‚‰ - æ”» - å†¬ - å‘¼ - è“ - ç»¼ - ç  - æ¯ - æ˜  - åˆ€ - è°¢ - ç¼– - è„š - æ™“ - é - æœ - å‰ - æ´— - ç›— - ä¸¹ - å± - ç›› - ç§˜ - æ‹˜ - æŸ“ - æ¸  - æ‰£ - æ´‹ - æ¢¯ - æª - ä¹… - è¯ˆ - å· - æ‘© - ä¿„ - è¿ª - æ¯› - èµ - ç¬¦ - ç”» - ç¿» - å¦¹ - ç­‘ - èš - å“ˆ - å…µ - è‚¯ - èƒ - æ½® - è‹¦ - é€ƒ - è®² - æˆ - æ…¢ - é¡¿ - é— - ä¸ - å‘ˆ - æ­ - æŒ‚ - å° - æ…§ - è·¨ - è¯¢ - æ‹† - æ£® - å­• - è„± - è¯» - æš - æ - æ¡© - è·ƒ - åˆ· - èŠ¯ - æ–— - æ˜† - å‚¨ - å®ˆ - è§¦ - æœ¨ - çš® - é¥­ - æ·» - è - éœ‡ - è½½ - è´µ - ä¾µ - æ’‘ - çˆ¸ - å†Œ - èˆ - ä¸ - è´¸ - å¥¶ - éš - å¦‡ - æ¦œ - ç¡ - é™· - è‰ - æ‰¬ - è¢­ - å· - ç£ - äº - å• - ç  - èµ¶ - æ‰¶ - ç›ˆ - æ¡£ - è¯º - è¿” - æ—¢ - æœ« - æ²™ - è° - å® - æ‘˜ - å…¸ - åºŠ - é—­ - å¼ƒ - é›· - æ¯• - éƒ­ - ç² - éƒ - èŠ - èƒ¡ - ç‘ - ç›Ÿ - å… - æŠ± - ç‡ƒ - é“œ - æ—— - è£ - é¤ - ç‰™ - çˆ· - è¿¹ - å®‡ - é€” - æ½œ - æŠµ - éª¨ - æ´ - æµª - ç‰ - ç¥– - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "74ad62749f29",
      "page_id": "shen2024bash",
      "title": "Bash comment generation via data augmentation and semantic-aware CodeBERT",
      "year": 2024,
      "abbr": "ASEJ'24",
      "venue": "Automated Software Engineering",
      "type": "journal",
      "tags": [
        "SCI-Q2",
        "CCF-B"
      ],
      "doi": "10.1007/s10515-024-00431-2",
      "url": null,
      "code": "https://github.com/syhstudy/Bash2Com",
      "abstract": "Understanding Bash code is challenging for developers due to its syntax flexibility and unique features. Bash lacks sufficient training data compared to comment generation tasks in popular programming languages. Furthermore, collecting more real Bash code and corresponding comments is time-consuming and labor-intensive. In this study, we propose a two-module method named Bash2Com for Bash code comments generation. The first module, NP-GD, is a gradient-based automatic data augmentation component that enhances normalization stability when generating adversarial examples. The second module, MASA, leverages CodeBERT to learn the rich semantics of Bash code. Specifically, MASA considers the representations learned at each layer of CodeBERT as a set of semantic information that captures recursive relationships within the code. To generate comments for different Bash snippets, MASA employs LSTM and attention mechanisms to dynamically concentrate on relevant representational information. Then, we utilize the Transformer decoder and beam search algorithm to generate code comments. To evaluate the effectiveness of Bash2Com, we consider a corpus of 10,592 Bash code and corresponding comments. Compared with the state-of-the-art baselines, our experimental results show that Bash2Com can outperform all baselines by at least 10.19%, 11.81%, 2.61%, and 6.13% in terms of the performance measures BLEU-3/4, METEOR, and ROUGR-L. Moreover, the rationality of NP-GD and MASA in Bash2Com are verified by ablation studies. Finally, we conduct a human evaluation to illustrate the effectiveness of Bash2Com from practitioners' perspectives.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4393190262",
        "doi": "https://doi.org/10.1007/s10515-024-00431-2",
        "display_name": "Bash comment generation via data augmentation and semantic-aware CodeBERT",
        "publication_year": 2024,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "baf2abfe0925",
      "page_id": "yang2024automatic",
      "title": "Automatic bi-modal question title generation for Stack Overflow with prompt learning",
      "year": 2024,
      "abbr": "EMSE'24",
      "venue": "Empirical Software Engineering",
      "type": "journal",
      "tags": [
        "SCI-Q2",
        "CCF-B"
      ],
      "doi": "10.1007/s10664-024-10466-4",
      "url": null,
      "code": "https://github.com/shaoyuyoung/SOTitlePlus",
      "abstract": "When drafting question posts for Stack Overflow, developers may not accurately summarize the core problems in the question titles, which can cause these questions to not get timely help. Therefore, improving the quality of question titles has attracted the wide attention of researchers. An initial study aimed to automatically generate the titles by only analyzing the code snippets in the question body. However, this study ignored the helpful information in their corresponding problem descriptions. Therefore, we propose an approach SOTitle+ by considering bi-modal information (i.e., the code snippets and the problem descriptions) in the question body. Then we formalize the title generation for different programming languages as separate but related tasks and utilize multi-task learning to solve these tasks. Later we fine-tune the pre-trained language model CodeT5 to automatically generate the titles. Unfortunately, the inconsistent inputs and optimization objectives between the pre-training task and our investigated task may make fine-tuning hard to fully explore the knowledge of the pre-trained model. To solve this issue, SOTitle+ further prompt-tunes CodeT5 with hybrid prompts (i.e., mixture of hard and soft prompts). To verify the effectiveness of SOTitle+, we construct a large-scale high-quality corpus from recent data dumps shared by Stack Overflow. Our corpus includes 179,119 high-quality question posts for six popular programming languages. Experimental results show that SOTitle+ can significantly outperform four state-of-the-art baselines in both automatic evaluation and human evaluation. In addition, our ablation studies also confirm the effectiveness of component settings (such as bi-modal information, prompt learning, hybrid prompts, and multi-task learning) of SOTitle+. Our work indicates that considering bi-modal information and prompt learning in Stack Overflow title generation is a promising exploration direction.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4396610514",
        "doi": "https://doi.org/10.1007/s10664-024-10466-4",
        "display_name": "Automatic bi-modal question title generation for Stack Overflow with prompt learning",
        "publication_year": 2024,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "e7f04bb341bc",
      "page_id": "10928415",
      "title": "RegexExplainer: Automatic Description Generation for Regular Expressions via Transformer",
      "year": 2024,
      "abbr": "ICCTIT'24",
      "venue": "Proceedings of the 4th International Conference on Communication Technology and Information Technology",
      "type": "conference",
      "tags": [
        "EI"
      ],
      "doi": "10.1109/ICCTIT64404.2024.10928415",
      "url": null,
      "code": null,
      "abstract": "Regular expressions can describe specific matching rules, which can be used to determine the string format or extract the string content. Until now, regular expressions have been widely used in various operating systems and have been supported by most programming languages. However, the understanding of regular expressions is challenging for end-users or developers, who may not familiar with the syntax of regular expressions. Therefore, it is pressing for researchers to put forward effective methods to automatically generate functional descriptions for regular expressions. In this study, we formalize the description generation of the regular expression as a neural machine translation task. Then we propose a fully data-driven method RegexExplainer based on Transformer. Specifically, it uses Regex Encoder and AST Encoder to jointly learn the encoding information of regular expressions. In order to confirm the effectiveness of RegexExplainer, we build a high-quality corpus that consists of 107,851 pairs of regular expressions along with their corresponding descriptions. The final experimental results demonstrate that RegexExplainer is able to attain better performance compared to the state-of-the-art baselines when it comes to three performance measures.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4408792851",
        "doi": "https://doi.org/10.1109/icctit64404.2024.10928415",
        "display_name": "RegexExplainer: Automatic Description Generation for Regular Expressions via Transformer",
        "publication_year": 2024,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "bd41ca71b092",
      "page_id": "zhao2024automatic",
      "title": "Automatic smart contract comment generation via large language models and in-context learning",
      "year": 2024,
      "abbr": "IST'24",
      "venue": "Information and Software Technology",
      "type": "journal",
      "tags": [
        "SCI-Q2",
        "CCF-B"
      ],
      "doi": "10.1016/j.infsof.2024.107405",
      "url": null,
      "code": "https://github.com/jun-jie-zhao/SCCLLM",
      "abstract": "Context: Designing effective automatic smart contract comment generation approaches can facilitate developersâ€™ comprehension, boosting smart contract development and improving vulnerability detection. The previous approaches can be divided into two categories: fine-tuning paradigm-based approaches and information retrieval-based approaches. Objective: However, for the fine-tuning paradigm-based approaches, the performance may be limited by the quality of the gathered dataset for the downstream task and they may have knowledge-forgetting issues, which can reduce the generality of the fine-tuned model. While for the information retrieval-based approaches, it is difficult for them to generate high-quality comments if similar code does not exist in the historical repository. Therefore we want to utilize the domain knowledge related to smart contract code comment generation in large language models (LLMs) to alleviate the disadvantages of these two types of approaches. Method: In this study, we propose an approach SCCLLM based on LLMs and in-context learning. Specifically, in the demonstration selection phase, SCCLLM retrieves the top- code snippets from the historical corpus by considering syntax, semantics, and lexical information. In the in-context learning phase, SCCLLM utilizes the retrieved code snippets as demonstrations for in-context learning, which can help to utilize the related knowledge for this task in the LLMs. In the LLMs inference phase, the input is the target smart contract code snippet, and the output is the corresponding comment generated by the LLMs. Results: We select a large corpus from a smart contract community Etherscan.io as our experimental subject. Extensive experimental results show the effectiveness of SCCLLM when compared with baselines in automatic evaluation and human evaluation. We also show the rationality of our customized demonstration selection strategy in SCCLLM by ablation studies. Conclusion: Our study shows using LLMs and in-context learning is a promising direction for automatic smart contract comment generation, which calls for more follow-up studies.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": false,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4390813110",
        "doi": "https://doi.org/10.1016/j.infsof.2024.107405",
        "display_name": "Automatic smart contract comment generation via large language models and in-context learning",
        "publication_year": 2024,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "96557ff2ee5f",
      "page_id": "ZHANG2024112066",
      "title": "Context-aware code generation with synchronous bidirectional decoder",
      "year": 2024,
      "abbr": "JSS'24",
      "venue": "Journal of Systems and Software",
      "type": "journal",
      "tags": [
        "SCI-Q2",
        "CCF-B"
      ],
      "doi": "https://doi.org/10.1016/j.jss.2024.112066",
      "url": "https://www.sciencedirect.com/science/article/pii/S0164121224001110",
      "code": null,
      "abstract": "Code generation aims to map natural language descriptions to code snippets. Recent approaches using sequence-to-tree models have shown promising results. However, they generally adopt an autoregressive way to predict the next token based on previous ones and do not consider potential future tokens. To address this issue, we propose Contextor, a novel context-sensitive model employing a bidirectional decoder to generate tokens in two different orders synchronously and interactively. Specifically, we employ two decoders to generate two sequences of different traversals and share their context knowledge via the attention mechanism. As a result, our model can synthesize both previous and future information simultaneously. To alleviate the information leakage problem caused by the teacher-forcing training strategy and bidirectional decoding, we propose an adapted scheduled sampling technique to prevent the decoders from contacting the actual label. Furthermore, Contextor also features a bidirectional beam search algorithm to better interact with both decoders. Experimental results demonstrate that our approach outperforms the state-of-the-art baselines.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": false,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4394859286",
        "doi": "https://doi.org/10.1016/j.jss.2024.112066",
        "display_name": "Context-aware code generation with synchronous bidirectional decoder",
        "publication_year": 2024,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "cdebb6799ffc",
      "page_id": "yang2024important",
      "title": "How important are good method names in neural code generation? a model robustness perspective",
      "year": 2024,
      "abbr": "TOSEM'24",
      "venue": "ACM Transactions on Software Engineering and Methodology",
      "type": "journal",
      "tags": [
        "SCI-Q1",
        "CCF-A"
      ],
      "doi": "10.1145/3630010",
      "url": null,
      "code": "https://github.com/NTDXYG/RADAR",
      "abstract": "Pre-trained code generation models (PCGMs) have been widely applied in neural code generation, which can generate executable code from functional descriptions in natural languages, possibly together with signatures. Despite substantial performance improvement of PCGMs, the role of method names in neural code generation has not been thoroughly investigated. In this article, we study and demonstrate the potential of benefiting from method names to enhance the performance of PCGMs from a model robustness perspective. Specifically, we propose a novel approach, named neuRAl coDe generAtor Robustifier (RADAR). RADAR consists of two components: RADAR-Attack and RADAR-Defense. The former attacks a PCGM by generating adversarial method names as part of the input, which are semantic and visual similar to the original input but may trick the PCGM to generate completely unrelated code snippets. As a countermeasure to such attacks, RADAR-Defense synthesizes a new method name from the functional description and supplies it to the PCGM. Evaluation results show that RADAR-Attack can reduce the CodeBLEU of generated code by 19.72% to 38.74% in three state-of-the-art PCGMs (i.e., CodeGPT, PLBART, and CodeT5) in the fine-tuning code generation task and reduce the Pass@1 of generated code by 32.28% to 44.42% in three state-of-the-art PCGMs (i.e., Replit, CodeGen, and CodeT5+) in the zero-shot code generation task. Moreover, RADAR-Defense is able to reinstate the performance of PCGMs with synthesized method names. These results highlight the importance of good method names in neural code generation and implicate the benefits of studying model robustness in software engineering.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": true
      },
      "title_signals": {
        "has_colon": false,
        "has_question": true,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4387869031",
        "doi": "https://doi.org/10.1145/3630010",
        "display_name": "How Important Are Good Method Names in Neural Code Generation? A Model Robustness Perspective",
        "publication_year": 2023,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": "Pre-trained code generation models (PCGMs) have been widely applied in neural code generation, which can generate executable code from functional descriptions in natural languages, possibly together with signatures. Despite substantial performance improvement of PCGMs, the role of method names in neural code generation has not been thoroughly investigated. In this article, we study and demonstrate the potential of benefiting from method names to enhance the performance of PCGMs from a model robustness perspective. Specifically, we propose a novel approach, named neu RA l co D e gener A tor R obustifier (RADAR). RADAR consists of two components: RADAR -Attack and RADAR -Defense. The former attacks a PCGM by generating adversarial method names as part of the input, which are semantic and visual similar to the original input but may trick the PCGM to generate completely unrelated code snippets. As a countermeasure to such attacks, RADAR -Defense synthesizes a new method name from the functional description and supplies it to the PCGM. Evaluation results show that RADAR -Attack can reduce the CodeBLEU of generated code by 19.72% to 38.74% in three state-of-the-art PCGMs (i.e., CodeGPT, PLBART, and CodeT5) in the fine-tuning code generation task and reduce the Pass@1 of generated code by 32.28% to 44.42% in three state-of-the-art PCGMs (i.e., Replit, CodeGen, and CodeT5+) in the zero-shot code generation task. Moreover, RADAR -Defense is able to reinstate the performance of PCGMs with synthesized method names. These results highlight the importance of good method names in neural code generation and implicate the benefits of studying model robustness in software engineering.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "57e799d73ba6",
      "page_id": "10634302",
      "title": "Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models",
      "year": 2024,
      "abbr": "TSE'24",
      "venue": "IEEE Transactions on Software Engineering",
      "type": "journal",
      "tags": [
        "SCI-Q1",
        "CCF-A"
      ],
      "doi": "10.1109/TSE.2024.3440503",
      "url": null,
      "code": "https://github.com/NTDXYG/COTTON",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in code generation. The integration of Chain of Thought (CoT) reasoning can further boost their performance. However, current CoT methods often require manual writing or LLMs with over 100 billion parameters to generate, impeding their applicability in resource-constrained scenarios. In this study, we investigate lightweight Language Models ( â„“LMs), which are defined to have fewer than 10 billion parameters. Empirically, we find that most â„“LMs cannot generate high-quality CoTs when prompted by the few-shot method, but can take advantage of high-quality CoTs generated elsewhere to improve their performance in code generation. Based on these findings, we design a novel approach COTTON which can leverage â„“LMs to automatically generate CoTs for code generation. We synthesize new datasets and conduct extensive experiments on various benchmarks. The results show that the CoTs generated by COTTON outperform the baselines in terms of automated and human evaluation metrics. In particular, the CoTs generated by COTTON boost various â„“LMs to achieve higher performance gains than those generated by LLMs such as ChatGLM (130B), and are competitive with those generated by Gemini and gpt-3.5-turbo. The results also reveal that COTTON not only improves the performance of â„“LMs, but also enhances the performance of LLMs. Our study showcases the potential of â„“LMs in software engineering applications.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4401508141",
        "doi": "https://doi.org/10.1109/tse.2024.3440503",
        "display_name": "Chain-of-Thought in Neural Code Generation: From and for Lightweight Language Models",
        "publication_year": 2024,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "c616c8d692cb",
      "page_id": "2023-30715",
      "title": "CodeScore-Rï¼šç”¨äºè¯„ä¼°ä»£ç åˆæˆåŠŸèƒ½å‡†ç¡®æ€§çš„è‡ªåŠ¨åŒ–é²æ£’æŒ‡æ ‡",
      "year": 2024,
      "abbr": "è®¡ç®—æœºç ”ç©¶ä¸å‘å±•'24",
      "venue": "è®¡ç®—æœºç ”ç©¶ä¸å‘å±•",
      "type": "journal",
      "tags": [
        "EI",
        "ä¸­æ–‡CCF-A",
        "åŒ—æ ¸"
      ],
      "doi": "10.7544/issn1000-1239.202330715",
      "url": "https://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330715",
      "code": null,
      "abstract": "Evaluation metrics are crucial in the field of code synthesis. Commonly used code evaluation metrics can be classified into three types: match-based, semantic-based, and execution-based. Among them, the execution-based Pass@k metric accurately assesses the functionality of predicted code by executing test cases. However, calculating this metric requires a significant amount of overhead, necessitating the design of an automated evaluation metric that can assess the functionality of predicted code without the need for test cases. Additionally, a good evaluation metric should be robust, that is the metric can maintain its accuracy even when the predicted code undergoes minor changes. To address these challenges, we propose an automated robust metric, called CodeScore-R, based on UniXcoder and contrastive learning, for evaluating the functionality of code synthesis. CodeScore-R employs techniques such as sketch-based processing, syntactic-equivalent transformations, and mutation testing to effectively mitigate the interference caused by identifiers, syntax structures, and operators on evaluation results. Experimental results demonstrate that in the tasks of code generation and migration in Java and Python, CodeScore-R outperforms other evaluation metrics and is more closely aligned with the Pass@k metric, while exhibiting stronger robustness.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": false,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "host_venue": null,
        "is_oa": null,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://crad.ict.ac.cn/cn/article/pdf/preview/10.7544/issn1000-1239.202330715.pdf",
        "source": "yg_url_discovered",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\Desktop\\codes\\å†™ä½œå‚è€ƒ\\papers\\pdfs_yg\\2024_è®¡ç®—æœºç ”ç©¶ä¸å‘å±•'24_CodeScore-Rï¼šç”¨äºè¯„ä¼°ä»£ç åˆæˆåŠŸèƒ½å‡†ç¡®æ€§çš„è‡ªåŠ¨åŒ–é²æ£’æŒ‡æ ‡_c616c8d692cb.pdf",
        "preview_text_path": "C:\\Users\\daoge\\Desktop\\codes\\å†™ä½œå‚è€ƒ\\papers\\extracted_yg\\2024_è®¡ç®—æœºç ”ç©¶ä¸å‘å±•'24_CodeScore-Rï¼šç”¨äºè¯„ä¼°ä»£ç åˆæˆåŠŸèƒ½å‡†ç¡®æ€§çš„è‡ªåŠ¨åŒ–é²æ£’æŒ‡æ ‡_c616c8d692cb.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "5e542ed91db9",
      "page_id": "zhang2025beyond",
      "title": "Beyond Sequences: Two-dimensional Representation and Dependency Encoding for Code Generation",
      "year": 2025,
      "abbr": "ACL'25",
      "venue": "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics",
      "type": "conference",
      "tags": [
        "CCF-A",
        "EI"
      ],
      "doi": "10.18653/v1/2025.acl-long.308",
      "url": null,
      "code": null,
      "abstract": "The advent of large language models has significantly advanced automatic code generation, transforming the way programmers writing code. Inspired by natural language processing, mainstream code generation approaches represent code as a linear sequence of tokens. In this paper, we propose to represent code snippets as two-dimensional entities, where both code lines and tokens within lines are explicitly modeled. This representation allows us to capture the hierarchical and spatial structure of code, especially the dependencies between code lines. Our method CoDE introduces a dependency encoding approach that leverages dictionary learning to perform semantic matching between code lines. As such, it avoids the reliance on strict position indices, leading to better generalization to code with diverse context and lengths. We thoroughly evaluate CoDE based on four categories of tasks. The experimental results showcase its generalizability, context understanding and retrieval, as well as interpretability in code generation.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": false,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4412945538",
        "doi": "https://doi.org/10.18653/v1/2025.acl-long.308",
        "display_name": "Beyond Sequences: Two-dimensional Representation and Dependency Encoding for Code Generation",
        "publication_year": 2025,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "54eaab2e0e01",
      "page_id": "yang2025codediting",
      "title": "Code-DiTing: Automatic Evaluation of Code Generation without References or Test Cases",
      "year": 2025,
      "abbr": "ASE'25",
      "venue": "Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering",
      "type": "conference",
      "tags": [
        "CCF-A",
        "EI"
      ],
      "doi": "10.1109/ASE63991.2025.00021",
      "url": null,
      "code": "https://github.com/CODE-DITING/CODE-DITING",
      "abstract": "Trustworthy evaluation methods for code snippets play a crucial role in neural code generation. Traditional methods, which either rely on reference solutions or require executable test cases, have inherent limitation in flexibility and scalability. The recent LLM-as-Judge methodology offers a promising alternative by directly evaluating functional consistency between the problem description and the generated code. To systematically understand the landscape of these LLM-as-Judge methods, we conduct a comprehensive empirical study across three diverse datasets. Our investigation reveals the pros and cons of two categories of LLM-as-Judge methods: the methods based on general foundation models can achieve good performance but require complex prompts and lack explainability, while the methods based on reasoning foundation models provide better explainability with simpler prompts but demand substantial computational resources due to their large parameter sizes. To address these limitations, we propose CODE-DITING, a novel code evaluation method that balances accuracy, efficiency and explainability. We develop a data distillation framework that effectively transfers reasoning capabilities from DeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing evaluation explainability and reducing the computational cost. With the majority vote strategy in the inference process, CODE-DITING 1.5B outperforms all models with the same magnitude of parameters and achieves performance which would normally exhibit in a model with 5 times of parameter scale. CODE-DITING 7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the parameter volume of these large models. Further experiments show that CODEDITING is robust to preference leakage and can serve as a promising alternative for code evaluation.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W7125971982",
        "doi": "https://doi.org/10.1109/ase63991.2025.00021",
        "display_name": "Code-DiTing: Automatic Evaluation of Code Generation without References or Test Cases",
        "publication_year": 2025,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "b0d35bc1f631",
      "page_id": "liu2025evaluating",
      "title": "Evaluating and Improving Framework-based Parallel Code Completion with Large Language Models",
      "year": 2025,
      "abbr": "ASE'25",
      "venue": "Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering",
      "type": "conference",
      "tags": [
        "CCF-A",
        "EI"
      ],
      "doi": null,
      "url": null,
      "code": null,
      "abstract": "Modern computing architectures (e.g., multi-core CPUs, GPUs, distributed systems) rely on parallel code implemented via frameworks such as OpenMP, MPI, and CUDA. While large language models (LLMs) have shown strong performance in general code generation, they struggle with the structured reasoning required for parallel programming, such as handling concurrency, synchronization, and framework-specific semantics. In practical parallel code development, a common workflow begins with sequential code and incrementally introduces parallel directive codes. We formalize this process as the task of textbfframework-based parallel code completion (FPCC), which involves three subtasks: identifying insertion points, selecting parallel frameworks, and completing parallel directive codes. To support this task, we construct a high-quality dataset of 16,615 framework-based parallel code pairs across six widely used frameworks, labeled with directive points, parallel frameworks, and the code of parallel directives. Empirical results show that six popular LLMs perform poorly on FPCC, particularly struggling with identifying insertion points and completing correct directive codes. To address these limitations, we propose HPCL, a curriculum-based fine-tuning framework that progressively improves model capabilities in insertion point identification, parallel framework selection, and parallel directive code completion. Our approach achieves substantial improvements, yielding an 17.82% increase in EM and a 5.43% improvement in DIR scores over LLM-based baselines. Finally, expert-guided error analysis reveals common failure patterns and suggests future directions in retrieval-augmented completion and consistency-aware training.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W7125955781",
        "doi": "https://doi.org/10.1109/ase63991.2025.00204",
        "display_name": "Evaluating and Improving Framework-based Parallel Code Completion with Large Language Models",
        "publication_year": 2025,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "95141fcb8e08",
      "page_id": "zhou2025sejury",
      "title": "SE-Jury: An LLM-as-Ensemble-Judge Metric for Narrowing the Gap with Human Evaluation in SE",
      "year": 2025,
      "abbr": "ASE'25",
      "venue": "Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering",
      "type": "conference",
      "tags": [
        "CCF-A",
        "EI"
      ],
      "doi": null,
      "url": null,
      "code": "https://github.com/Xin-Zhou-smu/LLMJudge",
      "abstract": "Large Language Models (LLMs) and other automated techniques have been increasingly used to support software developers by generating software artifacts such as code snippets, patches, and comments. However, accurately assessing the correctness of these generated artifacts remains a significant challenge. On one hand, human evaluation provides high accuracy but is labor-intensive and lacks scalability. On the other hand, many automatic evaluation metrics are scalable and require minimal human effort, but they often fail to accurately reflect the actual correctness of generated software artifacts. In this paper, we present SE-Jury, the first evaluation metric for LLM-as-Ensemble-Judge specifically designed to accurately assess the correctness of generated software artifacts. SE-Jury first defines five distinct evaluation strategies, each implemented as an independent judge. A dynamic team selection mechanism then identifies the most appropriate subset of judges as a team to produce a final correctness score through ensembling. We evaluate SE-Jury across a diverse set of software engineering (SE) benchmarks, including CoNaLa, Card2Code, HumanEval-X, APPS, APR-Assess, and Summary-Assess, which span three popular SE tasks: code generation, automated program repair, and code summarization. Experimental results demonstrate that SE-Jury consistently achieves a higher correlation with human judgments, with improvements ranging from 34.4% to 113.0% over existing automatic metrics. Furthermore, SE-Jury reaches agreement levels with human annotators that are close to inter-annotator agreement in code generation and program repair tasks. These findings underscore SE-Juryâ€™s potential as a scalable and reliable alternative to human evaluation in these SE tasks.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": true
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W7125977933",
        "doi": "https://doi.org/10.1109/ase63991.2025.00214",
        "display_name": "SE-Jury: An LLM-as-Ensemble-Judge Metric for Narrowing the Gap with Human Evaluation in SE",
        "publication_year": 2025,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "621275eada48",
      "page_id": "gao2025resource",
      "title": "Resource-efficient automatic software vulnerability assessment via knowledge distillation and particle swarm optimization",
      "year": 2025,
      "abbr": "EAAI'25",
      "venue": "Engineering Applications of Artificial Intelligence",
      "type": "journal",
      "tags": [
        "SCI-Q1",
        "CCF-C"
      ],
      "doi": "10.1016/j.engappai.2025.111914",
      "url": null,
      "code": "https://github.com/judeomg/PSO-KDVA",
      "abstract": "The increasing complexity of software systems has led to a surge in cybersecurity vulnerabilities, necessitating efficient and scalable solutions for vulnerability assessment. However, the deployment of large pre-trained models in real-world scenarios is hindered by their substantial computational and storage demands. To address this challenge, we propose a novel resource-efficient framework that integrates knowledge distillation and particle swarm optimization to enable automated vulnerability assessment. Our framework employs a two-stage approach: First, particle swarm optimization is utilized to optimize the architecture of a compact student model, balancing computational efficiency and model capacity. Second, knowledge distillation is applied to transfer critical vulnerability assessment knowledge from a large teacher model to the optimized student model. This process significantly reduces the model size while maintaining high performance. Experimental results on an enhanced MegaVul dataset, comprising 12,071 CVSS (Common Vulnerability Scoring System) v3 annotated vulnerabilities, demonstrate the effectiveness of our approach. Our approach achieves a 99.4% reduction in model size while retaining 89.3% of the original modelâ€™s accuracy. Furthermore, it outperforms state-of-the-art baselines by 1.7% in accuracy with 60% fewer parameters. The framework also reduces training time by 72.1% and architecture search time by 34.88% compared to traditional genetic algorithms.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W7077052573",
        "doi": "https://doi.org/10.1016/j.engappai.2025.111914",
        "display_name": "Resource-efficient automatic software vulnerability assessment via knowledge distillation and particle swarm optimization",
        "publication_year": 2025,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "6912e2fdc373",
      "page_id": "YANG2025107699",
      "title": "Assessing and improving syntactic adversarial robustness of pre-trained models for code translation",
      "year": 2025,
      "abbr": "IST'25",
      "venue": "Information and Software Technology",
      "type": "journal",
      "tags": [
        "SCI-Q2",
        "CCF-B"
      ],
      "doi": "https://doi.org/10.1016/j.infsof.2025.107699",
      "url": "https://www.sciencedirect.com/science/article/pii/S0950584925000382",
      "code": "https://github.com/NTDXYG/CoTR",
      "abstract": "Context: Pre-trained models (PTMs) have demonstrated significant potential in automatic code translation. However, the vulnerability of these models in translation tasks, particularly in terms of syntax, has not been extensively investigated. Objective: To fill this gap, our study aims to propose a novel approach CoTR to assess and improve the syntactic adversarial robustness of PTMs in code translation. Methods: CoTR consists of two components: CoTR-A and CoTR-D. CoTR-A generates adversarial examples by transforming programs, while CoTR-D proposes a semantic distance-based sampling data augmentation method and adversarial training method to improve the modelâ€™s robustness and generalization capabilities. The Pass@1 metric is used by CoTR to assess the performance of PTMs, which is more suitable for code translation tasks and offers a more precise evaluation in real-world scenarios. Results: The effectiveness of CoTR is evaluated through experiments on real-world Javaâ†”Python datasets. The results demonstrate that CoTR-A can significantly reduce the performance of existing PTMs, while CoTR-D effectively improves the robustness of PTMs. Conclusion: Our study identifies the limitations of current PTMs, including large language models, in code translation tasks. It highlights the potential of CoTR as an effective solution to enhance the robustness of PTMs for code translation tasks.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": true,
        "has_results": false
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4407989189",
        "doi": "https://doi.org/10.1016/j.infsof.2025.107699",
        "display_name": "Assessing and improving syntactic adversarial robustness of pre-trained models for code translation",
        "publication_year": 2025,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "54322119ac8b",
      "page_id": "yang2025large",
      "title": "Large Language Model for Verilog Code Generation: Literature Review and the Road Ahead",
      "year": 2025,
      "abbr": "Preprints'25",
      "venue": "Preprints",
      "type": "journal",
      "tags": [
        "Preprint"
      ],
      "doi": "10.20944/preprints202511.0656.v2",
      "url": null,
      "code": null,
      "abstract": "Code generation represents a critical intersection of Software Engineering (SE) and Artificial Intelligence (AI). Within this broader landscape, Verilog, as a representative hardware description language (HDL), is fundamental to Electronic Design Automation (EDA), recent research has increasingly focused on leveraging Large Language Models (LLMs) to automate Verilog code generation, particularly at the Register Transfer Level (RTL) design. Despite growing interest, a comprehensive survey of this domain remains absent. This review fill addresses this gap by providing a systematic literature review of LLM-based Verilog code generation, analyzing 102 papers (70 published and 32 high-quality preprints) from SE, AI, and EDA venues. We structure our analysis around four key research questions: (1) identifying the LLMs utilized, (2) examining evaluation datasets and metrics, (3) categorizing generation techniques, and (4) analyzing alignment approaches. Furthermore, we synthesize findings to identify critical limitations in current studies regarding effectiveness and integration. Finally, we outline a roadmap highlighting potential opportunities for future research in LLM-assisted hardware design.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": true,
        "has_results": false
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4416231639",
        "doi": "https://doi.org/10.20944/preprints202511.0656.v2",
        "display_name": "Large Language Model for Verilog Code Generation: Literature Review and the Road Ahead",
        "publication_year": 2025,
        "host_venue": null,
        "is_oa": true,
        "oa_url": "https://www.preprints.org/frontend/manuscript/dc6ee317d1bdce80d0dc654f17dba784/download_pub",
        "best_oa_location": {
          "id": "doi:10.20944/preprints202511.0656.v2",
          "is_oa": true,
          "landing_page_url": "https://doi.org/10.20944/preprints202511.0656.v2",
          "pdf_url": "https://www.preprints.org/frontend/manuscript/dc6ee317d1bdce80d0dc654f17dba784/download_pub",
          "source": {
            "id": "https://openalex.org/S6309402219",
            "display_name": "Preprints.org",
            "issn_l": null,
            "issn": null,
            "is_oa": true,
            "is_in_doaj": false,
            "is_core": false,
            "host_organization": null,
            "host_organization_name": null,
            "host_organization_lineage": [
              "https://openalex.org/P4310310987"
            ],
            "host_organization_lineage_names": [
              "Multidisciplinary Digital Publishing Institute"
            ],
            "type": "repository"
          },
          "license": "cc-by",
          "license_id": "https://openalex.org/licenses/cc-by",
          "version": "acceptedVersion",
          "is_accepted": true,
          "is_published": false,
          "raw_source_name": null,
          "raw_type": "posted-content"
        },
        "abstract": "Code generation has emerged as a critical research area at the intersection of Software Engineering (SE) and Artificial Intelligence (AI), attracting significant attention from both academia and industry. Within this broader landscape, Verilog, as a representative hardware description language (HDL), plays a fundamental role in digital circuit design and verification, making its automated generation particularly significant for Electronic Design Automation (EDA). Consequently, recent research has increasingly focused on applying Large Language Models (LLMs) to Verilog code generation, particularly at the Register Transfer Level (RTL), exploring how these AI-driven techniques can be effectively integrated into hardware design workflows. Despite substantial research efforts have been invested to explore LLM applications in this domain, a comprehensive survey synthesizing these developments remains absent from the literature. This review fill addresses this gap by providing a systematic literature review of LLM-based methods for Verilog code generation, examining their effectiveness, limitations, and potential for advancing automated hardware design. The review encompasses research work from conferences and journals in the fields of SE, AI, and EDA, encompassing 70 published papers, along with 32 high-quality preprint papers, bringing the total to 102 papers. By answering four key research questions, we aim to (1) identify the LLMs used for Verilog generation, (2) examine the datasets and metrics employed in evaluation, (3) categorize the techniques proposed for Verilog generation, and (4) analyze LLM alignment approaches for Verilog generation. Based on our findings, we have identified a series of limitations of existing studies. Finally, we have outlined a roadmap highlighting potential opportunities for future research endeavors in LLM-assisted hardware design.",
        "error": null
      },
      "pdf": {
        "url": "https://www.preprints.org/frontend/manuscript/dc6ee317d1bdce80d0dc654f17dba784/download_pub",
        "source": "openalex",
        "downloaded": false,
        "error": "HTTPError: HTTP Error 403: Forbidden",
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "0c97495f947a",
      "page_id": "11005718",
      "title": "Anchor Attention, Small Cache: Code Generation With Large Language Models",
      "year": 2025,
      "abbr": "TSE'25",
      "venue": "IEEE Transactions on Software Engineering",
      "type": "journal",
      "tags": [
        "SCI-Q1",
        "CCF-A"
      ],
      "doi": "10.1109/TSE.2025.3570680",
      "url": null,
      "code": "https://github.com/NUAAZXY/Anchor_Coder",
      "abstract": "The development of large language models (LLMs) has revolutionized automated code generation. However, their high demand of computation resources has hindered a broader deployment and raised environmental concerns. A common strategy for diminishing computational demands is to cache Key-Value (KV) states from the attention mechanism which is adopted predominately by mainstream LLMs. It can mitigate the need of repeated attention computations, but brings significant memory overhead. Current practices in NLP often use sparse attention which may, unfortunately, lead to substantial inaccuracies, or hallucinations, in code generation tasks. In this paper, we analyze the attention weights distribution within code generation models via an empirical study, uncovering a sparsity pattern, i.e., the aggregation of information at specific anchor points. Based on this observation, we propose a novel approach, AnchorCoder, which features token-wise anchor attention designed to extract and compress the contextual information, and layer-wise anchor attention enabling cross-layer communication to mitigate the issue of excessive superposition caused by the compression. The extensive experiments across multiple benchmark datasets confirm the effectiveness of AnchorCoder, which can consistently achieve a significant (at least 70%) reduction in KV cache requirements, while preserving the majority of model's performance.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4410394327",
        "doi": "https://doi.org/10.1109/tse.2025.3570680",
        "display_name": "Anchor Attention, Small Cache: Code Generation With Large Language Models",
        "publication_year": 2025,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "1125b86e2c4c",
      "page_id": "yang2026moregreencodelarge",
      "title": "Less is More: Towards Green Code Large Language Models via Unified Structural Pruning",
      "year": 2026,
      "abbr": "IPM'26",
      "venue": "Information Processing & Management",
      "type": "journal",
      "tags": [
        "SCI-Q1",
        "CCF-B"
      ],
      "doi": "https://doi.org/10.1016/j.ipm.2025.104580",
      "url": "https://www.sciencedirect.com/science/article/pii/S0306457325005217",
      "code": "https://github.com/Flab-Pruner/Flab-Pruner",
      "abstract": "The extensive application of Large Language Models (LLMs) in generative coding tasks has raised concerns due to their high computational demands and energy consumption. Unlike previous structural pruning methods designed for classification models that deal with lowdimensional classification logits, generative Code LLMs produce high-dimensional token logit sequences, making traditional pruning objectives inherently limited. Moreover, existing single component pruning approaches further constrain the effectiveness when applied to generative Code LLMs. In response, we propose Flab-Pruner, an innovative unified structural pruning method that combines vocabulary, layer, and Feed-Forward Network (FFN) pruning. This approach effectively reduces model parameters while maintaining performance. Additionally, we introduce a customized code instruction data strategy for coding tasks to enhance the performance recovery efficiency of the pruned model. Through extensive evaluations on three state-of-the-art Code LLMs across multiple generative coding tasks, the results demonstrate that Flab-Pruner retains 97% of the original performance after pruning 22% of the parameters and achieves the same or even better performance after post-training. The pruned models exhibit significant improvements in storage, GPU usage, computational efficiency, and environmental impact, while maintaining well robustness. Our research provides a sustainable solution for green software engineering and promotes the efficient deployment of LLMs in real-world generative coding intelligence applications.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": true,
        "has_less_is_more": true,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W7117892983",
        "doi": "https://doi.org/10.1016/j.ipm.2025.104580",
        "display_name": "Less is more: Towards green code large language models via unified structural pruning",
        "publication_year": 2026,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "b89bec61cc0b",
      "page_id": "10.1145/3728639",
      "title": "Defending Code Language Models against Backdoor Attacks with Deceptive Cross-Entropy Loss",
      "year": 2026,
      "abbr": "TOSEM'26",
      "venue": "ACM Transactions on Software Engineering and Methodology",
      "type": "journal",
      "tags": [
        "SCI-Q1",
        "CCF-A"
      ],
      "doi": "10.1145/3728639",
      "url": "https://doi.org/10.1145/3728639",
      "code": "https://github.com/NTDXYG/DeCE",
      "abstract": "Code Language Models (CLMs), particularly those leveraging deep learning, have achieved significant success in code intelligence domain. However, the issue of security, particularly backdoor attacks, is often overlooked in this process. The previous research has focused on designing backdoor attacks for CLMs, but effective defenses have not been adequately addressed. In particular, existing defense methods from natural language processing, when directly applied to CLMs, are not effective enough and lack generality, working well in some models and scenarios but failing in others, thus fall short in consistently mitigating backdoor attacks. To bridge this gap, we first confirm the phenomenon of â€œearly learningâ€ as a general occurrence during the training of CLMs. This phenomenon refers to that a model initially focuses on the main features of training data but may become more sensitive to backdoor triggers over time, leading to overfitting and susceptibility to backdoor attacks. We then analyze that overfitting to backdoor triggers results from the use of the cross-entropy loss function, where the unboundedness of cross-entropy leads the model to increasingly concentrate on the features of the poisoned data. Based on this insight, we propose a general and effective loss function DeCE (Deceptive Cross-Entropy) by blending deceptive distributions and applying label smoothing to limit the gradient to bounded, which prevents the model from overfitting to backdoor triggers and then enhances the security of CLMs against backdoor attacks. To evaluate the effectiveness of our defense method, we select four code-related tasks as our experiments scenes and conduct experimental analyses on both natural language and two programming languages (Java and Python). Our experiments across multiple models with different sizes (from 125M to 7B) and poisoning ratios demonstrate the applicability and effectiveness of DeCE in enhancing the security of CLMs. The findings emphasize the potential of DeCE as a novel defense mechanism for CLMs, effectively tackling the challenge of securing models against backdoor threats.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4410090047",
        "doi": "https://doi.org/10.1145/3728639",
        "display_name": "Defending Code Language Models against Backdoor Attacks with Deceptive Cross-Entropy Loss",
        "publication_year": 2025,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": "Code Language Models (CLMs), particularly those leveraging deep learning, have achieved significant success in code intelligence domain. However, the issue of security, particularly backdoor attacks, is often overlooked in this process. The previous research has focused on designing backdoor attacks for CLMs, but effective defenses have not been adequately addressed. In particular, existing defense methods from natural language processing, when directly applied to CLMs, are not effective enough and lack generality, working well in some models and scenarios but failing in others, thus fall short in consistently mitigating backdoor attacks. To bridge this gap, we first confirm the phenomenon of â€œearly learningâ€ as a general occurrence during the training of CLMs. This phenomenon refers to that a model initially focuses on the main features of training data but may become more sensitive to backdoor triggers over time, leading to overfitting and susceptibility to backdoor attacks. We then analyze that overfitting to backdoor triggers results from the use of the cross-entropy loss function, where the unboundedness of cross-entropy leads the model to increasingly concentrate on the features of the poisoned data. Based on this insight, we propose a general and effective loss function DeCE (Deceptive Cross-Entropy) by blending deceptive distributions and applying label smoothing to limit the gradient to bounded, which prevents the model from overfitting to backdoor triggers and then enhances the security of CLMs against backdoor attacks. To evaluate the effectiveness of our defense method, we select four code-related tasks as our experimental scenes and conduct experimental analyses on both natural language and two programming languages (Java and Python). Our experiments across multiple models with different sizes (from 125 millions to 7 billions) and poisoning ratios demonstrate the applicability and effectiveness of DeCE in enhancing the security of CLMs. The findings emphasize the potential of DeCE as a novel defense mechanism for CLMs, effectively tackling the challenge of securing models against backdoor threats.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "f158be582aec",
      "page_id": "10.1145/3735636",
      "title": "Less is More: DocString Compression in Code Generation",
      "year": 2026,
      "abbr": "TOSEM'26",
      "venue": "ACM Transactions on Software Engineering and Methodology",
      "type": "journal",
      "tags": [
        "SCI-Q1",
        "CCF-A"
      ],
      "doi": "10.1145/3735636",
      "url": "https://doi.org/10.1145/3735636",
      "code": "https://github.com/NTDXYG/ShortenDoc",
      "abstract": "The widespread use of Large Language Models (LLMs) in software engineering has intensified the need for improved model and resource efficiency. In particular, for neural code generation, LLMs are used to translate function/method signature and DocString to executable code. DocStrings, which capture user requirements for the code and are typically used as the prompt for LLMs, often contain redundant information. Recent advancements in prompt compression have shown promising results in Natural Language Processing (NLP), but their applicability to code generation remains uncertain. Our empirical study show that the state-of-the-art prompt compression methods achieve only about 10% reduction, as further reductions would cause significant performance degradation. In our study, we propose a novel compression method, ShortenDoc, dedicated to DocString compression for code generation. Our experiments on six code generation datasets, five open-source LLMs (1B to 10B parameters) and one closed-source LLM GPT-4o confirm that ShortenDoc achieves 25â€“40% compression while preserving the quality of generated code, outperforming other baseline methods at similar compression levels. The benefit of this method is to improve efficiency and reduce the token processing cost while maintaining the quality of the generated code, especially when calling third-party APIs.",
      "abstract_signals": {
        "lang": "en",
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": false
      },
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": true,
        "has_acronym": false
      },
      "openalex": {
        "score": 2.0,
        "id": "https://openalex.org/W4410361283",
        "doi": "https://doi.org/10.1145/3735636",
        "display_name": "Less Is More: DocString Compression in Code Generation",
        "publication_year": 2025,
        "host_venue": null,
        "is_oa": false,
        "oa_url": null,
        "best_oa_location": null,
        "abstract": "The widespread use of Large Language Models (LLMs) in software engineering has intensified the need for improved model and resource efficiency. In particular, for neural code generation, LLMs are used to translate function/method signature and DocString to executable code. DocStrings, which capture user requirements for the code and are typically used as the prompt for LLMs, often contain redundant information. Recent advancements in prompt compression have shown promising results in Natural Language Processing (NLP), but their applicability to code generation remains uncertain. Our empirical study shows that the state-of-the-art prompt compression methods achieve only about 10% reduction, as further reductions would cause significant performance degradation. In our study, we propose a novel compression method, ShortenDoc, dedicated to DocString compression for code generation. Our experiments on six code generation datasets, five open source LLMs (1B to 10B parameters), and one closed-source LLM GPT-4o confirm that ShortenDoc achieves 25â€“40% compression while preserving the quality of generated code, outperforming other baseline methods at similar compression levels. The benefit of this method is to improve efficiency and reduce the token processing cost while maintaining the quality of the generated code, especially when calling third-party APIs.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    }
  ]
}