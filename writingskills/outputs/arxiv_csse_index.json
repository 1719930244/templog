{
  "generated_at": "2026-02-10T13:08:41.582303+00:00",
  "source": "arXiv API",
  "query": "cat:cs.SE",
  "since_year": 2024,
  "start": 0,
  "max_results": 50,
  "paper_count": 50,
  "papers": [
    {
      "arxiv_id": "2602.08949v1",
      "title": "Digital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room",
      "authors": [
        "Mohammad Morsali",
        "Siavash H. Khajavi"
      ],
      "published": "2026-02-09T17:44:52Z",
      "updated": "2026-02-09T17:44:52Z",
      "year": 2026,
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08949v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08949v1",
      "abstract": "According to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster management frameworks rely on static simulations and passive data acquisition, hindering their ability to adapt to arbitrarily evolving wildfire episodes in real-time. To address these limitations, we introduce the Intelligent Virtual Situation Room (IVSR), a bidirectional Digital Twin (DT) platform augmented by autonomous AI agents. The IVSR continuously ingests multisource sensor imagery, weather data, and 3D forest models to create a live virtual replica of the fire environment. A similarity engine powered by AI aligns emerging conditions with a precomputed Disaster Simulation Library, retrieving and calibrating intervention tactics under the watchful eyes of experts. Authorized action-ranging from UAV redeployment to crew reallocation-is cycled back through standardized procedures to the physical layer, completing the loop between response and analysis. We validate IVSR through detailed case-study simulations provided by an industrial partner, demonstrating capabilities in localized incident detection, privacy-preserving playback, collider-based fire-spread projection, and site-specific ML retraining. Our results indicate marked reductions in detection-to-intervention latency and more effective resource coordination versus traditional systems. By uniting real-time bidirectional DTs with agentic AI, IVSR offers a scalable, semi-automated decision-support paradigm for proactive, adaptive wildfire disaster management.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08949v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08915v1",
      "title": "Comparing AI Coding Agents: A Task-Stratified Analysis of Pull Request Acceptance",
      "authors": [
        "Giovanni Pinna",
        "Jingzhi Gong",
        "David Williams",
        "Federica Sarro"
      ],
      "published": "2026-02-09T17:14:46Z",
      "updated": "2026-02-09T17:14:46Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08915v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08915v1",
      "abstract": "The rapid adoption of AI-powered coding assistants is transforming software development practices, yet systematic comparisons of their effectiveness across different task types and over time remain limited. This paper presents an empirical study comparing five popular agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, and Claude Code), analyzing 7,156 pull requests (PRs) from the AIDev dataset. Temporal trend analysis reveals heterogeneous evolution patterns: Devin exhibits the only consistent positive trend in acceptance rate (+0.77% per week over 32 weeks), whereas other agents remain largely stable. Our analysis suggests that the PR task type is a dominant factor influencing acceptance rates: documentation tasks achieve 82.1% acceptance compared to 66.1% for new features - a 16 percentage point gap that exceeds typical inter-agent variance for most tasks. OpenAI Codex achieves consistently high acceptance rates across all nine task categories (59.6%-88.6%), with stratified Chi-square tests confirming statistically significant advantages over other agents in several task categories. However, no single agent performs best across all task types: Claude Code leads in documentation (92.3%) and features (72.6%), while Cursor excels in fix tasks (80.4%).",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08915v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08887v1",
      "title": "DeepQuali: Initial results of a study on the use of large language models for assessing the quality of user stories",
      "authors": [
        "Adam Trendowicz",
        "Daniel Seifert",
        "Andreas Jedlitschka",
        "Marcus Ciolkowski",
        "Anton Strahilov"
      ],
      "published": "2026-02-09T16:49:54Z",
      "updated": "2026-02-09T16:49:54Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08887v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08887v1",
      "abstract": "Generative artificial intelligence (GAI), specifically large language models (LLMs), are increasingly used in software engineering, mainly for coding tasks. However, requirements engineering - particularly requirements validation - has seen limited application of GAI. The current focus of using GAI for requirements is on eliciting, transforming, and classifying requirements, not on quality assessment. We propose and evaluate the LLM-based (GPT-4o) approach \"DeepQuali\", for assessing and improving requirements quality in agile software development. We applied it to projects in two small companies, where we compared LLM-based quality assessments with expert judgments. Experts also participated in walkthroughs of the solution, provided feedback, and rated their acceptance of the approach. Experts largely agreed with the LLM's quality assessments, especially regarding overall ratings and explanations. However, they did not always agree with the other experts on detailed ratings, suggesting that expertise and experience may influence judgments. Experts recognized the usefulness of the approach but criticized the lack of integration into their workflow. LLMs show potential in supporting software engineers with the quality assessment and improvement of requirements. The explicit use of quality models and explanatory feedback increases acceptance.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08887v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08866v1",
      "title": "ArkEval: Benchmarking and Evaluating Automated CodeRepair for ArkTS",
      "authors": [
        "Bang Xie",
        "Senjian Zhang",
        "Zhiyuan Peng",
        "Wei Chen",
        "Chenhao Ying",
        "Yuan Luo"
      ],
      "published": "2026-02-09T16:28:29Z",
      "updated": "2026-02-09T16:28:29Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08866v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08866v1",
      "abstract": "Large language models have transformed code generation, enabling unprecedented automation in software development. As mobile ecosystems evolve, HarmonyOS has emerged as a critical platform requiring robust development tools. Software development for the HarmonyOS ecosystem relies heavily on ArkTS, a statically typed extension of TypeScript. Despite its growing importance, the ecosystem lacks robust tools for automated code repair, primarily due to the absence of a high-quality benchmark for evaluation. To address this gap, we present ArkEval, a unified framework for ArkTS automated repair workflow evaluation and benchmark construction. It provides the first comprehensive benchmark specifically designed for ArkTS automated program repair. We constructed this benchmark by mining issues from a large-scale official Huawei repository containing over 400 independent ArkTS applications. Through a rigorous multi-stage filtering process, we curated 502 reproducible issues. To ensure testability, we employed a novel LLM-based test generation and voting mechanism involving Claude and other models. Furthermore, we standardized problem statements to facilitate fair evaluation. Finally, we evaluated four state-of-the-art Large Language Models (LLMs) on our benchmark using a retrieval-augmented repair workflow. Our results highlight the current capabilities and limitations of LLMs in repairing ArkTS code, paving the way for future research in this low-resource language domain.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08866v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08816v1",
      "title": "Permissive-Washing in the Open AI Supply Chain: A Large-Scale Audit of License Integrity",
      "authors": [
        "James Jewitt",
        "Gopi Krishnan Rajbahadur",
        "Hao Li",
        "Bram Adams",
        "Ahmed E. Hassan"
      ],
      "published": "2026-02-09T15:51:36Z",
      "updated": "2026-02-09T15:51:36Z",
      "year": 2026,
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY",
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08816v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08816v1",
      "abstract": "Permissive licenses like MIT, Apache-2.0, and BSD-3-Clause dominate open-source AI, signaling that artifacts like models, datasets, and code can be freely used, modified, and redistributed. However, these licenses carry mandatory requirements: include the full license text, provide a copyright notice, and preserve upstream attribution, that remain unverified at scale. Failure to meet these conditions can place reuse outside the scope of the license, effectively leaving AI artifacts under default copyright for those uses and exposing downstream users to litigation. We call this phenomenon ``permissive washing'': labeling AI artifacts as free to use, while omitting the legal documentation required to make that label actionable. To assess how widespread permissive washing is in the AI supply chain, we empirically audit 124,278 dataset $\\rightarrow$ model $\\rightarrow$ application supply chains, spanning 3,338 datasets, 6,664 models, and 28,516 applications across Hugging Face and GitHub. We find that an astonishing 96.5\\% of datasets and 95.8\\% of models lack the required license text, only 2.3\\% of datasets and 3.2\\% of models satisfy both license text and copyright requirements, and even when upstream artifacts provide complete licensing evidence, attribution rarely propagates downstream: only 27.59\\% of models preserve compliant dataset notices and only 5.75\\% of applications preserve compliant model notices (with just 6.38\\% preserving any linked upstream notice). Practitioners cannot assume permissive labels confer the rights they claim: license files and notices, not metadata, are the source of legal truth. To support future research, we release our full audit dataset and reproducible pipeline.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": true,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08816v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08801v1",
      "title": "Verifying DNN-based Semantic Communication Against Generative Adversarial Noise",
      "authors": [
        "Thanh Le",
        "Hai Duong",
        "ThanhVu Nguyen",
        "Takeshi Matsumura"
      ],
      "published": "2026-02-09T15:40:13Z",
      "updated": "2026-02-09T15:40:13Z",
      "year": 2026,
      "categories": [
        "cs.LO",
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08801v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08801v1",
      "abstract": "Safety-critical applications like autonomous vehicles and industrial IoT are adopting semantic communication (SemCom) systems using deep neural networks to reduce bandwidth and increase transmission speed by transmitting only task-relevant semantic features. However, adversarial attacks against these DNN-based SemCom systems can cause catastrophic failures by manipulating transmitted semantic features. Existing defense mechanisms rely on empirical approaches provide no formal guarantees against the full spectrum of adversarial perturbations. We present VSCAN, a neural network verification framework that provides mathematical robustness guarantees by formulating adversarial noise generation as mixed integer programming and verifying end-to-end properties across multiple interconnected networks (encoder, decoder, and task model). Our key insight is that realistic adversarial constraints (power limitations and statistical undetectability) can be encoded as logical formulae to enable efficient verification using state-of-the-art DNN verifiers. Our evaluation on 600 verification properties characterizing various attacker's capabilities shows VSCAN matches attack methods in finding vulnerabilities while providing formal robustness guarantees for 44% of properties -- a significant achievement given the complexity of multi-network verification. Moreover, we reveal a fundamental security-efficiency tradeoff: compact 16-dimensional latent spaces achieve 50% verified robustness compared to 64-dimensional spaces.",
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08801v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08765v1",
      "title": "Taming Scylla: Understanding the multi-headed agentic daemon of the coding seas",
      "authors": [
        "Micah Villmow"
      ],
      "published": "2026-02-09T15:06:24Z",
      "updated": "2026-02-09T15:06:24Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08765v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08765v1",
      "abstract": "LLM-based tools are automating more software development tasks at a rapid pace, but there is no rigorous way to evaluate how different architectural choices -- prompts, skills, tools, multi-agent setups -- materially affect both capability and cost. This paper introduces Scylla, an evaluation framework for benchmarking agentic coding tools through structured ablation studies that uses seven testing tiers (T0-T6) progressively adding complexity to isolate what directly influences results and how. The key metric is Cost-of-Pass (CoP): the expected dollar cost to get one correct solution, which directly quantifies the trade-off between complexity and efficiency. The framework is model-agnostic, designed to work with any CLI tool; this paper demonstrates it with Claude Sonnet 4.5, using multiple LLM judges (Opus 4.5, Sonnet 4.5, Haiku 4.5) from the same vendor for evaluation consensus, where judges score results using direct tests, human-designed LLM-evaluated rubrics, and qualitative assessment. The result is a reproducible framework that quantifies trade-offs between agent complexity and actual outcomes, suggesting that architectural complexity does not always improve quality.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08765v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08750v1",
      "title": "DyMA-Fuzz: Dynamic Direct Memory Access Abstraction for Re-hosted Monolithic Firmware Fuzzing",
      "authors": [
        "Guy Farrelly",
        "Michael Chesser",
        "Seyit Camtepe",
        "Damith C. Ranasinghe"
      ],
      "published": "2026-02-09T14:52:57Z",
      "updated": "2026-02-09T14:52:57Z",
      "year": 2026,
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08750v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08750v1",
      "abstract": "The rise of smart devices in critical domains--including automotive, medical, industrial--demands robust firmware testing. Fuzzing firmware in re-hosted environments is a promising method for automated testing at scale, but remains difficult due to the tight coupling of code with a microcontroller's peripherals. Existing fuzzing frameworks primarily address input challenges in providing inputs for Memory-Mapped I/O or interrupts, but largely overlook Direct Memory Access (DMA), a key high-throughput interface used that bypasses the CPU. We introduce DyMA-Fuzz to extend recent advances in stream-based fuzz input injection to DMA-driven interfaces in re-hosted environments. It tackles key challenges--vendor-specific descriptors, heterogeneous DMA designs, and varying descriptor locations--using runtime analysis techniques to infer DMA memory access patterns and automatically inject fuzzing data into target buffers, without manual configuration or datasheets. Evaluated on 94 firmware samples and 8 DMA-guarded CVE benchmarks, DyMA-Fuzz reveals vulnerabilities and execution paths missed by state-of-the-art tools and achieves up to 122% higher code coverage. These results highlight DyMA-Fuzz as a practical and effective advancement in automated firmware testing and a scalable solution for fuzzing complex embedded systems.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08750v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08561v1",
      "title": "Automating Computational Reproducibility in Social Science: Comparing Prompt-Based and Agent-Based Approaches",
      "authors": [
        "Syed Mehtab Hussain Shah",
        "Frank Hopfgartner",
        "Arnim Bleier"
      ],
      "published": "2026-02-09T11:59:59Z",
      "updated": "2026-02-09T11:59:59Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08561v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08561v1",
      "abstract": "Reproducing computational research is often assumed to be as simple as rerunning the original code with provided data. In practice, missing packages, fragile file paths, version conflicts, or incomplete logic frequently cause analyses to fail, even when materials are shared. This study investigates whether large language models and AI agents can automate the diagnosis and repair of such failures, making computational results easier to reproduce and verify. We evaluate this using a controlled reproducibility testbed built from five fully reproducible R-based social science studies. Realistic failures were injected, ranging from simple issues to complex missing logic, and two automated repair workflows were tested in clean Docker environments. The first workflow is prompt-based, repeatedly querying language models with structured prompts of varying context, while the second uses agent-based systems that inspect files, modify code, and rerun analyses autonomously. Across prompt-based runs, reproduction success ranged from 31-79 percent, with performance strongly influenced by prompt context and error complexity. Complex cases benefited most from additional context. Agent-based workflows performed substantially better, with success rates of 69-96 percent across all complexity levels. These results suggest that automated workflows, especially agent-based systems, can significantly reduce manual effort and improve reproduction success across diverse error types. Unlike prior benchmarks, our testbed isolates post-publication repair under controlled failure modes, allowing direct comparison of prompt-based and agent-based approaches.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08561v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08517v1",
      "title": "TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor",
      "authors": [
        "Shaoang Zhang",
        "Yazhe Niu"
      ],
      "published": "2026-02-09T11:06:13Z",
      "updated": "2026-02-09T11:06:13Z",
      "year": 2026,
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08517v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08517v1",
      "abstract": "Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": false,
        "has_action_verb": false,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08517v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08422v1",
      "title": "LLMs + Security = Trouble",
      "authors": [
        "Benjamin Livshits"
      ],
      "published": "2026-02-09T09:27:28Z",
      "updated": "2026-02-09T09:27:28Z",
      "year": 2026,
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08422v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08422v1",
      "abstract": "We argue that when it comes to producing secure code with AI, the prevailing \"fighting fire with fire\" approach -- using probabilistic AI-based checkers or attackers to secure probabilistically generated code -- fails to address the long tail of security bugs. As a result, systems may remain exposed to zero-day vulnerabilities that can be discovered by better-resourced or more persistent adversaries. While neurosymbolic approaches that combine LLMs with formal methods are attractive in principle, we argue that they are difficult to reconcile with the \"vibe coding\" workflow common in LLM-assisted development: unless the end-to-end verification pipeline is fully automated, developers are repeatedly asked to validate specifications, resolve ambiguities, and adjudicate failures, making the human-in-the-loop a likely point of weakness, compromising secure-by-construction guarantees. In this paper we argue that stronger security guarantees can be obtained by enforcing security constraints during code generation (e.g., via constrained decoding), rather than relying solely on post-hoc detection and repair. This direction is particularly promising for diffusion-style code models, whose approach provides a natural elegant opportunity for modular, hierarchical security enforcement, allowing us to combine lower-latency generation techniques with generating secure-by-construction code.",
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": false,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08422v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08316v1",
      "title": "SWE Context Bench: A Benchmark for Context Learning in Coding",
      "authors": [
        "Jared Zhu",
        "Minhao Hu",
        "Junde Wu"
      ],
      "published": "2026-02-09T06:44:45Z",
      "updated": "2026-02-09T06:44:45Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08316v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08316v1",
      "abstract": "Large language models are increasingly used as programming agents for repository level software engineering tasks. While recent benchmarks evaluate correctness in realistic codebases, they largely treat tasks as independent and do not assess whether agents can reuse experience across related problems. As a result, the ability of agents to accumulate, retrieve, and apply prior experience, as well as the efficiency gains from such reuse, remains difficult to measure. We introduce SWE-ContextBench, a benchmark designed to explicitly evaluate experience reuse in programming agents. Built on SWE-Bench Lite, SWE-ContextBench augments 300 base tasks with 99 related tasks derived from real dependency and reference relationships among GitHub issues and pull requests, forming task sequences with shared context. The benchmark evaluates agents along three complementary dimensions: prediction accuracy, time efficiency, and cost efficiency. Using SWE-ContextBench, we study multiple experience reuse settings, including oracle guided and autonomous retrieval, as well as full execution trajectories and compact summaries. Our results show that correctly selected summarized experience improves resolution accuracy and substantially reduces runtime and token cost, particularly on harder tasks. In contrast, unfiltered or incorrectly selected experience provides limited or negative benefits. These findings highlight the importance of experience representation and retrieval quality, and position SWE-ContextBench as a principled benchmark for studying experience reuse in programming agents.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08316v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08263v1",
      "title": "Specification Vibing for Automated Program Repair",
      "authors": [
        "Taohong Zhu",
        "Lucas C. Cordeiro",
        "Mustafa A. Mustafa",
        "Youcheng Sun"
      ],
      "published": "2026-02-09T04:44:58Z",
      "updated": "2026-02-09T04:44:58Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08263v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08263v1",
      "abstract": "Large language model (LLM)-driven automated program repair (APR) has advanced rapidly, but most methods remain code-centric: they directly rewrite source code and thereby risk hallucinated, behaviorally inconsistent fixes. This limitation suggests the need for an alternative repair paradigm that relies on a representation more accessible to LLMs than raw code, enabling more accurate understanding, analysis, and alignment during repair. To address this gap, we propose VibeRepair, a specification-centric APR technique that treats repair as behavior-specification repair rather than ad-hoc code editing. VibeRepair first translates buggy code into a structured behavior specification that captures the program's intended runtime behavior, then infers and repairs specification misalignments, and finally synthesizes code strictly guided by the corrected behavior specification. An on-demand reasoning component enriches hard cases with program analysis and historical bug-fix evidence while controlling cost. Across Defects4J and real-world benchmarks and multiple LLMs, VibeRepair demonstrates consistently strong repair effectiveness with a significantly smaller patch space. On Defects4J v1.2, VibeRepair correctly repairs 174 bugs, exceeding the strongest state-of-the-art baseline by 28 bugs, which corresponds to a 19% improvement. On Defects4J v2.0, it repairs 178 bugs, outperforming prior approaches by 33 bugs, representing a 23% improvement. Evaluations on real-world benchmarks collected after the training period of selected LLMs further confirm its effectiveness and generalizability. By centering repair on explicit behavioral intent, VibeRepair reframes APR for the era of \"vibe\" coding: make the behavior sing, and the code will follow.",
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08263v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08242v1",
      "title": "Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications",
      "authors": [
        "Ali Hassaan Mughal",
        "Muhammad Bilal"
      ],
      "published": "2026-02-09T03:39:45Z",
      "updated": "2026-02-09T03:39:45Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.NI"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08242v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08242v1",
      "abstract": "Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08242v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08192v1",
      "title": "Adoption of Large Language Models in Scrum Management: Insights from Brazilian Practitioners",
      "authors": [
        "Mirko Perkusich",
        "Danyllo Albuquerque",
        "Allysson Allex Araújo",
        "Matheus Paixão",
        "Rohit Gheyi",
        "Marcos Kalinowski",
        "Angelo Perkusich"
      ],
      "published": "2026-02-09T01:21:23Z",
      "updated": "2026-02-09T01:21:23Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08192v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08192v1",
      "abstract": "Scrum is widely adopted in software project management due to its adaptability and collaborative nature. The recent emergence of Large Language Models (LLMs) has created new opportunities to support knowledge-intensive Scrum practices. However, existing research has largely focused on technical activities such as coding and testing, with limited evidence on the use of LLMs in management-related Scrum activities. In this study, we investigate the use of LLMs in Scrum management activities through a survey of 70 Brazilian professionals. Among them, 49 actively use Scrum, and 33 reported using LLM-based assistants in their Scrum practices. The results indicate a high level of proficiency and frequent use of LLMs, with 85% of respondents reporting intermediate or advanced proficiency and 52% using them daily. LLM use concentrates on exploring Scrum practices, with artifacts and events receiving targeted yet uneven support, whereas broader management tasks appear to be adopted more cautiously. The main benefits include increased productivity (78%) and reduced manual effort (75%). However, several critical risks remain, as respondents report 'almost correct' outputs (81%), confidentiality concerns (63%), and hallucinations during use (59%). This work provides one of the first empirical characterizations of LLM use in Scrum management, identifying current practices, quantifying benefits and risks, and outlining directions for responsible adoption and integration in Agile environments.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": true,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08192v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08181v1",
      "title": "ModARO: A Modular Approach to Architecture Reconstruction of Distributed Microservice Codebases",
      "authors": [
        "Oscar Manglaras",
        "Alex Farkas",
        "Thomas Woolford",
        "Christoph Treude",
        "Markus Wagner"
      ],
      "published": "2026-02-09T00:46:35Z",
      "updated": "2026-02-09T00:46:35Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08181v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08181v1",
      "abstract": "Microservice architectures promote small, independently developed services, but increase overall architectural complexity. It is crucial that developers understand the architecture and how changes to a service affect the overall system, but rapid and independent development of services increases the risk of architectural drift and discourages the creation and maintenance of documentation. Automatic architecture reconstruction can help avoid these issues, but it is difficult to reuse reconstruction code across multiple projects, as all use different combinations of technologies and project-specific conventions. Reconstruction of architecture-level details is further complicated by the tendency to split microservices into separate repositories, preventing a full view of the system from any one codebase. In this paper, we present and evaluate ModARO, an approach to microservice architecture reconstruction that allows writing modular reconstruction code ('extractors') for any technologies and reusing them across different projects, independent of the surrounding technology stack or whether or not the services are split into multiple codebases. We demonstrate the effectiveness of our approach by configuring ModARO to reconstruct 10 open source projects, and we validate the usefulness and usability of ModARO against a state-of-the-art baseline in a user study with 8 industry practitioners. Using this approach, developers can assemble or create extractors tailored to their technology stacks and distribute architecture reconstruction across repositories, enabling integration into repository CI/CD pipelines.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08181v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08166v1",
      "title": "Distributed Architecture Reconstruction of Polyglot and Multi-Repository Microservice Projects",
      "authors": [
        "Oscar Manglaras",
        "Alex Farkas",
        "Thomas Woolford",
        "Christoph Treude",
        "Markus Wagner"
      ],
      "published": "2026-02-08T23:59:19Z",
      "updated": "2026-02-08T23:59:19Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08166v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08166v1",
      "abstract": "Microservice architectures encourage the use of small, independently developed services; however, this can lead to increased architectural complexity. Accurate documentation is crucial, but is challenging to maintain due to the rapid, independent evolution of services. While static architecture reconstruction provides a way to maintain up-to-date documentation, existing approaches suffer from technology limitations, mono-repo constraints, or high implementation barriers. This paper presents a novel framework for static architecture reconstruction that supports technology-specific analysis modules, called \\emph{extractors}, and supports \\emph{distributed architecture reconstruction} in multi-repo environments. We describe the core design concepts and algorithms that govern how extractors are executed, how data is passed between them, and how their outputs are unified. Furthermore, the framework is interoperable with existing static analysis tools and algorithms, allowing them to be invoked from or embedded within extractors.",
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": false,
        "has_action_verb": false,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08166v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08146v1",
      "title": "Test vs Mutant: Adversarial LLM Agents for Robust Unit Test Generation",
      "authors": [
        "Pengyu Chang",
        "Yixiong Fang",
        "Silin Chen",
        "Yuling Shi",
        "Beijun Shen",
        "Xiaodong Gu"
      ],
      "published": "2026-02-08T22:34:30Z",
      "updated": "2026-02-08T22:34:30Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08146v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08146v1",
      "abstract": "Software testing is a critical, yet resource-intensive phase of the software development lifecycle. Over the years, various automated tools have been developed to aid in this process. Search-based approaches typically achieve high coverage but produce tests with low readability, whereas large language model (LLM)-based methods generate more human-readable tests but often suffer from low coverage and compilability. While the majority of research efforts have focused on improving test coverage and readability, little attention has been paid to enhancing the robustness of bug detection, particularly in exposing corner cases and vulnerable execution paths. To address this gap, we propose AdverTest, a novel adversarial framework for LLM-powered test case generation. AdverTest comprises two interacting agents: a test case generation agent (T) and a mutant generation agent (M). These agents engage in an adversarial loop, where M persistently creates new mutants \"hacking\" the blind spots of T's current test suite, while T iteratively refines its test cases to \"kill\" the challenging mutants produced by M. This interaction loop is guided by both coverage and mutation scores, enabling the system to co-evolve toward both high test coverage and bug detection capability. Experimental results in the Defects4J dataset show that our approach improves fault detection rates by 8.56% over the best existing LLM-based methods and by 63.30% over EvoSuite, while also improving line and branch coverage.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08146v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08133v1",
      "title": "Integrating Code Metrics into Automated Documentation Generation for Computational Notebooks",
      "authors": [
        "Mojtaba Mostafavi Ghahfarokhi",
        "Hamed Jahantigh",
        "Alireza Asadi",
        "Abbas Heydarnoori"
      ],
      "published": "2026-02-08T21:40:57Z",
      "updated": "2026-02-08T21:40:57Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08133v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08133v1",
      "abstract": "Effective code documentation is essential for collaboration, comprehension, and long-term software maintainability, yet developers often neglect it due to its repetitive nature. Automated documentation generation has evolved from heuristic and rule-based methods to neural network-based and large language model (LLM)-based approaches. However, existing methods often overlook structural and quantitative characteristics of code that influence readability and comprehension. Prior research suggests that code metrics capture information relevant to program understanding. Building on these insights, this paper investigates the role of source code metrics as auxiliary signals for automated documentation generation, focusing on computational notebooks, a popular medium among data scientists that integrates code, narrative, and results but suffers from inconsistent documentation. We propose a two-stage approach. First, the CodeSearchNet dataset construction process was refined to create a specialized dataset from over 17 million code and markdown cells. After structural and semantic filtering, approximately 36,734 high-quality (code, markdown) pairs were extracted. Second, two modeling paradigms, a lightweight CNN-RNN architecture and a few-shot GPT-3.5 architecture, were evaluated with and without metric information. Results show that incorporating code metrics improves the accuracy and contextual relevance of generated documentation, yielding gains of 6% in BLEU-1 and 3% in ROUGE-L F1 for CNN-RNN-based architecture, and 9% in BERTScore F1 for LLM-based architecture. These findings demonstrate that integrating code metrics provides valuable structural context, enhancing automated documentation generation across diverse model families.",
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08133v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08084v1",
      "title": "Outsourcing in Global Software Development: Effects of Temporal Location and Methodologies",
      "authors": [
        "Mark Looi",
        "Marc Szepan"
      ],
      "published": "2026-02-08T18:57:49Z",
      "updated": "2026-02-08T18:57:49Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08084v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08084v1",
      "abstract": "Developing software globally using outsourced resources has become a common practice, with project teams often distributed in different time zones. In this study, we focus on customers that contract software development to vendors in temporally nearshore or far offshore locations. We conducted a survey to determine the effect of temporal distance on overall success, costs, project management effort, schedule, quality, communication problems, and other outcomes of interest to managers. In the survey of 80 customers and interviews with 6 of them, we also investigated the effect of software development methodology on the same outcomes. The results show that nearshore development is advantageous for overall success, quality, reduced PM effort, maintaining schedule, higher quality, and engendering fewer communication problems. Development methodology appears to only influence higher costs. We assess our findings in the context of prior GSE research and provide practical advice for customers of outsourced global software development, chief of which is to favor nearshore for communication-intensive or Agile projects.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08084v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08072v1",
      "title": "IssueGuard: Real-Time Secret Leak Prevention Tool for GitHub Issue Reports",
      "authors": [
        "Md Nafiu Rahman",
        "Sadif Ahmed",
        "Zahin Wahab",
        "Gias Uddin",
        "Rifat Shahriyar"
      ],
      "published": "2026-02-08T18:05:27Z",
      "updated": "2026-02-08T18:05:27Z",
      "year": 2026,
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08072v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08072v1",
      "abstract": "GitHub and GitLab are widely used collaborative platforms whose issue-tracking systems contain large volumes of unstructured text, including logs, code snippets, and configuration examples. This creates a significant risk of accidental secret exposure, such as API keys and credentials, yet these platforms provide no mechanism to warn users before submission. We present \\textsc{IssueGuard}, a tool for real-time detection and prevention of secret leaks in issue reports. Implemented as a Chrome extension, \\textsc{IssueGuard} analyzes text as users type and combines regex-based candidate extraction with a fine-tuned CodeBERT model for contextual classification. This approach effectively separates real secrets from false positives and achieves an F1-score of 92.70\\% on a benchmark dataset, outperforming traditional regex-based scanners. \\textsc{IssueGuard} integrates directly into the web interface and continuously analyzes the issue editor, presenting clear visual warnings to help users avoid submitting sensitive data. The source code is publicly available at \\href{https://github.com/nafiurahman00/IssueGuard}{https://github.com/nafiurahman00/IssueGuard}, and a demonstration video is available at \\href{https://youtu.be/kvbWA8rr9cU}{https://youtu.be/kvbWA8rr9cU}.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08072v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08015v1",
      "title": "Bridging the Gap: Adapting Evidence to Decision Frameworks to support the link between Software Engineering academia and industry",
      "authors": [
        "Patricia G. F. Matsubara",
        "Tayana Conte"
      ],
      "published": "2026-02-08T15:30:19Z",
      "updated": "2026-02-08T15:30:19Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08015v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08015v1",
      "abstract": "Over twenty years ago, the Software Engineering (SE) research community have been involved with Evidence-Based Software Engineering (EBSE). EBSE aims to inform industrial practice with the best evidence from rigorous research, preferably from systematic literature reviews (SLRs). Since then, SE researchers have conducted many SLRs, perfected their SLR procedures, proposed alternative ways of presenting their results (such as Evidence Briefings), and profusely discussed how to conduct research that impacts practice. Nevertheless, there is still a feeling that SLRs' results are not reaching practitioners. Something is missing. In this vision paper, we introduce Evidence to Decision (EtD) frameworks from the health sciences, which propose gathering experts in panels to assess the existing best evidence about the impact of an intervention in all relevant outcomes and make structured recommendations based on them. The insight we can leverage from EtD frameworks is not their structure per se but all the relevant criteria for making recommendations to practitioners from SLRs. Furthermore, we provide a worked example based on an SE SLR. We also discuss the challenges the SE research and practice community may face when adopting EtD frameworks, highlighting the need for more comprehensive criteria in our recommendations to industry practitioners.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": false,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08015v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.08004v1",
      "title": "Agent Skills: A Data-Driven Analysis of Claude Skills for Extending Large Language Model Functionality",
      "authors": [
        "George Ling",
        "Shanshan Zhong",
        "Richard Huang"
      ],
      "published": "2026-02-08T15:14:12Z",
      "updated": "2026-02-08T15:14:12Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.SI"
      ],
      "abs_url": "http://arxiv.org/abs/2602.08004v1",
      "pdf_url": "https://arxiv.org/pdf/2602.08004v1",
      "abstract": "Agent skills extend large language model (LLM) agents with reusable, program-like modules that define triggering conditions, procedural logic, and tool interactions. As these skills proliferate in public marketplaces, it is unclear what types are available, how users adopt them, and what risks they pose. To answer these questions, we conduct a large-scale, data-driven analysis of 40,285 publicly listed skills from a major marketplace. Our results show that skill publication tends to occur in short bursts that track shifts in community attention. We also find that skill content is highly concentrated in software engineering workflows, while information retrieval and content creation account for a substantial share of adoption. Beyond content trends, we uncover a pronounced supply-demand imbalance across categories, and we show that most skills remain within typical prompt budgets despite a heavy-tailed length distribution. Finally, we observe strong ecosystem homogeneity, with widespread intent-level redundancy, and we identify non-trivial safety risks, including skills that enable state-changing or system-level actions. Overall, our findings provide a quantitative snapshot of agent skills as an emerging infrastructure layer for agents and inform future work on skill reuse, standardization, and safety-aware design.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.08004v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07900v1",
      "title": "Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents",
      "authors": [
        "Zhi Chen",
        "Zhensu Sun",
        "Yuling Shi",
        "Chao Peng",
        "Xiaodong Gu",
        "David Lo",
        "Lingxiao Jiang"
      ],
      "published": "2026-02-08T10:26:31Z",
      "updated": "2026-02-08T10:26:31Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07900v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07900v1",
      "abstract": "Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget. To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.",
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07900v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07893v1",
      "title": "Is Your Private Information Logged? An Empirical Study on Android App Logs",
      "authors": [
        "Zhiyuan Chen",
        "Soham Sanjay Deo",
        "Poorna Chander Reddy Puttaparthi",
        "Vanessa Nava-Camal",
        "Yiming Tang",
        "Xueling Zhang",
        "Weiyi Shang"
      ],
      "published": "2026-02-08T09:56:48Z",
      "updated": "2026-02-08T09:56:48Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07893v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07893v1",
      "abstract": "With the rapid growth of mobile apps, users' concerns about their privacy have become increasingly prominent. Android app logs serve as crucial computer resources, aiding developers in debugging and monitoring the status of Android apps, while also containing a wealth of software system information. Previous studies have acknowledged privacy leaks in software logs and Android apps as significant issues without providing a comprehensive view of the privacy leaks in Android app logs. In this study, we build a comprehensive dataset of Android app logs and conduct an empirical study to analyze the status and severity of privacy leaks in Android app logs. Our study comprises three aspects: (1) Understanding real-world developers' concerns regarding privacy issues related to software logs; (2) Studying privacy leaks in the Android app logs; (3) Investigating the characteristics of privacy-leaking Android app logs and analyzing the reasons behind them. Our study reveals five different categories of concerns from real-world developers regarding privacy issues related to software logs and the prevalence of privacy leaks in Android app logs, with the majority stemming from developers' unawareness of such leaks. Additionally, our study provides developers with suggestions to safeguard their privacy from being logged.",
      "title_signals": {
        "has_colon": false,
        "has_question": true,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07893v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07882v1",
      "title": "Rethinking Code Complexity Through the Lens of Large Language Models",
      "authors": [
        "Chen Xie",
        "Yuling Shi",
        "Xiaodong Gu",
        "Beijun Shen"
      ],
      "published": "2026-02-08T09:20:20Z",
      "updated": "2026-02-08T09:20:20Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07882v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07882v1",
      "abstract": "Code complexity metrics such as cyclomatic complexity have long been used to assess software quality and maintainability. With the rapid advancement of large language models (LLMs) on code understanding and generation tasks, an important yet underexplored question arises: do these traditional complexity metrics meaningfully characterize the difficulty LLMs experience when processing code? In this work, we empirically demonstrate that, after controlling for code length, classical metrics exhibit no consistent correlation with LLM performance, revealing a fundamental mismatch with model-perceived difficulty. To address this gap, we propose LM-CC, a novel code complexity metric designed from the perspective of LLMs. The core premise of LM-CC is that LLM-perceived difficulty is driven by the nonlinearity of program semantics. Accordingly, we decompose programs into semantic units based on entropy, organize these units into a compositional hierarchy, and quantify complexity as a principled aggregation of compositional level and branching-induced divergence, capturing cumulative model uncertainty during code processing. Our extensive experiments show that LM-CC not only correlates more strongly with LLM performance than traditional metrics but also that lowering it directly enhances task performance.",
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": false,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07882v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07871v1",
      "title": "HerAgent: Rethinking the Automated Environment Deployment via Hierarchical Test Pyramid",
      "authors": [
        "Xiang Li",
        "Siyu Lu",
        "Sarro Federica",
        "Claire Le Goues",
        "He Ye"
      ],
      "published": "2026-02-08T08:57:05Z",
      "updated": "2026-02-08T08:57:05Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07871v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07871v1",
      "abstract": "Automated software environment setup is a prerequisite for testing, debugging, and reproducing failures, yet remains challenging in practice due to complex dependencies, heterogeneous build systems, and incomplete documentation. Recent work leverages large language models to automate this process, but typically evaluates success using weak signals such as dependency installation or partial test execution, which do not ensure that a project can actually run. In this paper, we argue that environment setup success should be evaluated through executable evidence rather than a single binary signal. We introduce the Environment Maturity Hierarchy, which defines three success levels based on progressively stronger execution requirements, culminating in successful execution of a project's main entry point. Guided by this hierarchy, we propose HerAgent, an automated environment setup approach that incrementally constructs executable environments through execution-based validation and repair. We evaluate HerAgent on four public benchmarks, where it outperforms all related work, achieving up to 79.6\\% improvement due to its holistic understanding of project structure and dependencies. On complex C/C++ projects, HerAgent surpasses prior approaches by 66.7\\%. In addition, HerAgent uniquely resolves 11-30 environment instances across the benchmarks that no prior method can configure.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07871v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07821v1",
      "title": "Software Space Analytics: Towards Visualization and Statistics of Internal Software Execution",
      "authors": [
        "Shinobu Saito"
      ],
      "published": "2026-02-08T04:57:57Z",
      "updated": "2026-02-08T04:57:57Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07821v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07821v1",
      "abstract": "In software maintenance work, software architects and programmers need to identify modules that require modification or deletion. Whilst user requests and bug reports are utilised for this purpose, evaluating the execution status of modules within the software is also crucial. This paper, therefore, applies spatial statistics to assess internal software execution data. First, we define a software space dataset, viewing the software's internal structure as a space based on module call relationships. Then, using spatial statistics, we conduct the visualization of spatial clusters and the statistical testing using spatial measures. Finally, we consider the usefulness of spatial statistics in the software engineering domain and future challenges.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": true,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": false,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07821v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07783v1",
      "title": "Still Manual? Automated Linter Configuration via DSL-Based LLM Compilation of Coding Standards",
      "authors": [
        "Zejun Zhang",
        "Yixin Gan",
        "Zhenchang Xing",
        "Tian Zhang",
        "Yi Li",
        "Xiwei Xu",
        "Qinghua Lu",
        "Liming Zhu"
      ],
      "published": "2026-02-08T02:57:47Z",
      "updated": "2026-02-08T02:57:47Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07783v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07783v1",
      "abstract": "Coding standards are essential for maintaining consistent and high-quality code across teams and projects. Linters help developers enforce these standards by detecting code violations. However, manual linter configuration is complex and expertise-intensive, and the diversity and evolution of programming languages, coding standards, and linters lead to repetitive and maintenance-intensive configuration work. To reduce manual effort, we propose LintCFG, a domain-specific language (DSL)-driven, LLM-based compilation approach to automate linter configuration generation for coding standards, independent of programming languages, coding standards, and linters. Inspired by compiler design, we first design a DSL to express coding rules in a tool-agnostic, structured, readable, and precise manner. Then, we build linter configurations into DSL configuration instructions. For a given natural language coding standard, the compilation process parses it into DSL coding standards, matches them with the DSL configuration instructions to set configuration names, option names and values, verifies consistency between the standards and configurations, and finally generates linter-specific configurations. Experiments with Checkstyle for Java coding standard show that our approach achieves over 90% precision and recall in DSL representation, with accuracy, precision, recall, and F1-scores close to 70% (with some exceeding 70%) in fine-grained linter configuration generation. Notably, our approach outperforms baselines by over 100% in precision. A user study further shows that our approach improves developers' efficiency in configuring linters for coding standards. Finally, we demonstrate the generality of the approach by generating ESLint configurations for JavaScript coding standards, showcasing its broad applicability across other programming languages, coding standards, and linters.",
      "title_signals": {
        "has_colon": false,
        "has_question": true,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07783v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07698v1",
      "title": "On Sequence-to-Sequence Models for Automated Log Parsing",
      "authors": [
        "Adam Sorrenti",
        "Andriy Miranskyy"
      ],
      "published": "2026-02-07T20:47:45Z",
      "updated": "2026-02-07T20:47:45Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07698v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07698v1",
      "abstract": "Log parsing is a critical standard operating procedure in software systems, enabling monitoring, anomaly detection, and failure diagnosis. However, automated log parsing remains challenging due to heterogeneous log formats, distribution shifts between training and deployment data, and the brittleness of rule-based approaches. This study aims to systematically evaluate how sequence modelling architecture, representation choice, sequence length, and training data availability influence automated log parsing performance and computational cost. We conduct a controlled empirical study comparing four sequence modelling architectures: Transformer, Mamba state-space, monodirectional LSTM, and bidirectional LSTM models. In total, 396 models are trained across multiple dataset configurations and evaluated using relative Levenshtein edit distance with statistical significance testing. Transformer achieves the lowest mean relative edit distance (0.111), followed by Mamba (0.145), mono-LSTM (0.186), and bi-LSTM (0.265), where lower values are better. Mamba provides competitive accuracy with substantially lower computational cost. Character-level tokenization generally improves performance, sequence length has negligible practical impact on Transformer accuracy, and both Mamba and Transformer demonstrate stronger sample efficiency than recurrent models. Overall, Transformers reduce parsing error by 23.4%, while Mamba is a strong alternative under data or compute constraints. These results also clarify the roles of representation choice, sequence length, and sample efficiency, providing practical guidance for researchers and practitioners.",
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07698v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07672v1",
      "title": "Debugging code world models",
      "authors": [
        "Babak Rahmani"
      ],
      "published": "2026-02-07T19:32:15Z",
      "updated": "2026-02-07T19:32:15Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG",
        "cs.PL",
        "cs.SC"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07672v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07672v1",
      "abstract": "Code World Models (CWMs) are language models trained to simulate program execution by predicting explicit runtime state after every executed command. This execution-based world modeling enables internal verification within the model, offering an alternative to natural language chain-of-thought reasoning. However, the sources of errors and the nature of CWMs' limitations remain poorly understood. We study CWMs from two complementary perspectives: local semantic execution and long-horizon state tracking. On real-code benchmarks, we identify two dominant failure regimes. First, dense runtime state reveals produce token-intensive execution traces, leading to token-budget exhaustion on programs with long execution histories. Second, failures disproportionately concentrate in string-valued state, which we attribute to limitations of subword tokenization rather than program structure. To study long-horizon behavior, we use a controlled permutation-tracking benchmark that isolates state propagation under action execution. We show that long-horizon degradation is driven primarily by incorrect action generation: when actions are replaced with ground-truth commands, a Transformer-based CWM propagates state accurately over long horizons, despite known limitations of Transformers in long-horizon state tracking. These findings suggest directions for more efficient supervision and state representations in CWMs that are better aligned with program execution and data types.",
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": false,
        "has_action_verb": false,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07672v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07641v1",
      "title": "HAIF: A Human-AI Integration Framework for Hybrid Team Operations",
      "authors": [
        "Marc Bara"
      ],
      "published": "2026-02-07T17:49:53Z",
      "updated": "2026-02-07T17:49:53Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.HC"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07641v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07641v1",
      "abstract": "The rapid deployment of generative AI, copilots, and agentic systems in knowledge work has created an operational gap: no existing framework addresses how to organize daily work in teams where AI agents perform substantive, delegated tasks alongside humans. Agile, DevOps, MLOps, and AI governance frameworks each cover adjacent concerns but none models the hybrid team as a coherent delivery unit. This paper proposes the Human-AI Integration Framework (HAIF): a protocol-based, scalable operational system built around four core principles, a formal delegation decision model, tiered autonomy with quantifiable transition criteria, and feedback mechanisms designed to integrate into existing Agile and Kanban workflows without requiring additional roles for small teams. The framework is developed following a Design Science Research methodology. HAIF explicitly addresses the central adoption paradox: the more capable AI becomes, the harder it is to justify the oversight the framework demands-and yet the greater the consequences of not providing it. The paper includes domain-specific validation checklists, adaptation guidance for non-software environments, and an examination of the framework's structural limitations-including the increasingly common pattern of continuous human-AI co-production that challenges the discrete delegation model. The framework is tool-agnostic and designed for iterative adoption. Empirical validation is identified as future work.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": false,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07641v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07609v1",
      "title": "Evaluating Large Language Models for Detecting Architectural Decision Violations",
      "authors": [
        "Ruoyu Su",
        "Alexander Bakhtin",
        "Noman Ahmad",
        "Matteo Esposito",
        "Valentina Lenarduzzi",
        "Davide Taibi"
      ],
      "published": "2026-02-07T16:36:14Z",
      "updated": "2026-02-07T16:36:14Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07609v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07609v1",
      "abstract": "Architectural Decision Records (ADRs) play a central role in maintaining software architecture quality, yet many decision violations go unnoticed because projects lack both systematic documentation and automated detection mechanisms. Recent advances in Large Language Models (LLMs) open up new possibilities for automating architectural reasoning at scale. We investigated how effectively LLMs can identify decision violations in open-source systems by examining their agreement, accuracy, and inherent limitations. Our study analyzed 980 ADRs across 109 GitHub repositories using a multi-model pipeline in which one LLM primary screens potential decision violations, and three additional LLMs independently validate the reasoning. We assessed agreement, accuracy, precision, and recall, and complemented the quantitative findings with expert evaluation. The models achieved substantial agreement and strong accuracy for explicit, code-inferable decisions. Accuracy falls short for implicit or deployment-oriented decisions that depend on deployment configuration or organizational knowledge. Therefore, LLMs can meaningfully support validation of architectural decision compliance; however, they are not yet replacing human expertise for decisions not focused on code.",
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07609v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07589v1",
      "title": "A Course on the Introduction to Quantum Software Engineering: Experience Report",
      "authors": [
        "Andriy Miranskyy"
      ],
      "published": "2026-02-07T15:35:04Z",
      "updated": "2026-02-07T15:35:04Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.CY",
        "cs.ET",
        "quant-ph"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07589v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07589v1",
      "abstract": "Quantum computing is increasingly practiced through programming, yet most educational offerings emphasize algorithmic or framework-level use rather than software engineering concerns such as testing, abstraction, tooling, and lifecycle management. This paper reports on the design and first offering of a cross-listed undergraduate--graduate course that frames quantum computing through a software engineering lens, focusing on early-stage competence relevant to software engineering practice. The course integrates foundational quantum concepts with software engineering perspectives, emphasizing executable artifacts, empirical reasoning, and trade-offs arising from probabilistic behaviour, noise, and evolving toolchains. Evidence is drawn from instructor observations, student feedback, surveys, and analysis of student work. Despite minimal prior exposure to quantum computing, students were able to engage productively with quantum software engineering topics once a foundational understanding of quantum information and quantum algorithms, expressed through executable artifacts, was established. This experience report contributes a modular course design, a scalable assessment model for mixed academic levels, and transferable lessons for software engineering educators developing quantum computing curricula.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": false,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07589v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07569v1",
      "title": "Clarifying Core Dimensions in Digital Maturity Models: An Integrative Approach",
      "authors": [
        "Eduardo C. Peixoto",
        "Hector Oliveira",
        "Geber L. Ramalho",
        "Cesar França"
      ],
      "published": "2026-02-07T14:30:05Z",
      "updated": "2026-02-07T14:30:05Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.SI"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07569v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07569v1",
      "abstract": "Digital Transformation (DT) initiatives frequently face high failure rates, and while Digital Maturity Models (DMMs) offer potential solutions, they have notable shortcomings. Specifically, there is significant disparity in the dimensions considered relevant, a lack of clarity in their definitions, and uncertainty regarding their components. This study aims to provide a clearer understanding of DMMs by proposing integrative definitions of the most frequently used dimensions. Using a Systematic Mapping approach, including automatic search and snowballing techniques, we analyzed 76 DMMs to answer two Research Questions: (RQ1) What are the most frequent dimensions in DMMs? and (RQ2) How are these dimensions described, including their components? We reconcile varying interpretations of the ten most frequent dimensions -- Organization, Strategy, Technology, Culture, Process, Operations, People, Management, Customer, and Data -- and propose integrative definitions for each. Compared to previous analyses, this study provides a broader and more recent perspective on Digital Maturity Models.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07569v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07561v1",
      "title": "ComPass: Contrastive Learning for Automated Patch Correctness Assessment in Program Repair",
      "authors": [
        "Quanjun Zhang",
        "Ye Shang",
        "Haichuan Hu",
        "Chunrong Fang",
        "Zhenyu Chen",
        "Liang Xiao"
      ],
      "published": "2026-02-07T14:17:21Z",
      "updated": "2026-02-07T14:17:21Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07561v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07561v1",
      "abstract": "Automated program repair (APR) attempts to reduce manual debugging efforts and plays a vital role in software maintenance. Despite remarkable progress, APR is still limited in generating overfitting patches, i.e., patches passing available test suites but incorrect. This issue, known as patch overfitting, has become a key concern in the APR community, with numerous approaches proposed to address it. Very recent work proposes a pre-trained language model (PLM)-based automated patch correctness assessment (APCA) approach, indicating the potential of such PLMs in reasoning about patch correctness. Despite being promising, it is still far from perfect due to various limitations, such as the training paradigm and training dataset. In this paper, we present ComPass, a PLM-based APCA approach that leverages contrastive learning and data augmentation to address the technical limitations of prior work. Our work is inspired by the opportunity to integrate contrastive learning with recent PLMs in the field of patch correctness assessment, where large-scale labeled patches are difficult to obtain. ComPass utilizes code transformation rules to generate semantic-preserving code snippets for both unlabeled pre-training corpus and labeled fine-tuning patches. ComPass then pre-trains PLMs with contrastive learning, which captures code features with the same semantics but different structures. ComPass finally integrates representation embeddings of patch code snippets and fine-tunes PLMs with a binary classifier jointly to assess patch code correctness. Experimental results on 2274 real-world patches from Defects4J demonstrate that ComPass achieves an accuracy of 88.35%, significantly outperforming state-of-the-art baseline APPT.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07561v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07457v1",
      "title": "Pull Requests as a Training Signal for Repo-Level Code Editing",
      "authors": [
        "Qinglin Zhu",
        "Tianyu Chen",
        "Shuai Lu",
        "Lei Ji",
        "Runcong Zhao",
        "Murong Ma",
        "Xiangxiang Dai",
        "Yulan He",
        "Lin Gui",
        "Peng cheng",
        "Yeyun Gong"
      ],
      "published": "2026-02-07T09:22:25Z",
      "updated": "2026-02-07T09:22:25Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07457v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07457v1",
      "abstract": "Repository-level code editing requires models to understand complex dependencies and execute precise multi-file modifications across a large codebase. While recent gains on SWE-bench rely heavily on complex agent scaffolding, it remains unclear how much of this capability can be internalised via high-quality training signals. To address this, we propose Clean Pull Request (Clean-PR), a mid-training paradigm that leverages real-world GitHub pull requests as a training signal for repository-level editing. We introduce a scalable pipeline that converts noisy pull request diffs into Search/Replace edit blocks through reconstruction and validation, resulting in the largest publicly available corpus of 2 million pull requests spanning 12 programming languages. Using this training signal, we perform a mid-training stage followed by an agentless-aligned supervised fine-tuning process with error-driven data augmentation. On SWE-bench, our model significantly outperforms the instruction-tuned baseline, achieving absolute improvements of 13.6% on SWE-bench Lite and 12.3% on SWE-bench Verified. These results demonstrate that repository-level code understanding and editing capabilities can be effectively internalised into model weights under a simplified, agentless protocol, without relying on heavy inference-time scaffolding.",
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07457v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07412v1",
      "title": "Forecasting Developer Environments with GenAI: A Research Perspective",
      "authors": [
        "Raula Gaikovina Kula",
        "Christoph Treude",
        "Xing Hu",
        "Sebastian Baltes",
        "Earl T. Barr",
        "Kelly Blincoe",
        "Fabio Calefato",
        "Junjie Chen",
        "Marc Cheong",
        "Youmei Fan",
        "Daniel M. German",
        "Marco Gerosa",
        "Jin L. C. Guo",
        "Shinpei Hayashi",
        "Robert Hirschfeld",
        "Reid Holmes",
        "Yintong Huo",
        "Takashi Kobayashi",
        "Michele Lanza",
        "Zhongxin Liu",
        "Olivier Nourry",
        "Nicole Novielli",
        "Denys Poshyvanyk",
        "Shinobu Saito",
        "Kazumasa Shimari",
        "Igor Steinmacher",
        "Mairieli Wessel",
        "Markus Wagner",
        "Annie Vella",
        "Laurie Williams",
        "Xin Xia"
      ],
      "published": "2026-02-07T07:16:01Z",
      "updated": "2026-02-07T07:16:01Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07412v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07412v1",
      "abstract": "Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222, a four-day intensive research meeting. Four themes emerged as areas of interest for researchers and practitioners.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07412v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07303v1",
      "title": "KRONE: Hierarchical and Modular Log Anomaly Detection",
      "authors": [
        "Lei Ma",
        "Jinyang Liu",
        "Tieying Zhang",
        "Peter M. VanNostrand",
        "Dennis M. Hofmann",
        "Lei Cao",
        "Elke A. Rundensteiner",
        "Jianjun Chen"
      ],
      "published": "2026-02-07T01:30:19Z",
      "updated": "2026-02-07T01:30:19Z",
      "year": 2026,
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07303v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07303v1",
      "abstract": "Log anomaly detection is crucial for uncovering system failures and security risks. Although logs originate from nested component executions with clear boundaries, this structure is lost when they are stored as flat sequences. As a result, state-of-the-art methods risk missing true dependencies within executions while learning spurious ones across unrelated events. We propose KRONE, the first hierarchical anomaly detection framework that automatically derives execution hierarchies from flat logs for modular multi-level anomaly detection. At its core, the KRONE Log Abstraction Model captures application-specific semantic hierarchies from log data. This hierarchy is then leveraged to recursively decompose log sequences into multiple levels of coherent execution chunks, referred to as KRONE Seqs, transforming sequence-level anomaly detection into a set of modular KRONE Seq-level detection tasks. For each test KRONE Seq, KRONE employs a hybrid modular detection mechanism that dynamically routes between an efficient level-independent Local-Context detector, which rapidly filters normal sequences, and a Nested-Aware detector that incorporates cross-level semantic dependencies and supports LLM-based anomaly detection and explanation. KRONE further optimizes hierarchical detection through cached result reuse and early-exit strategies. Experiments on three public benchmarks and one industrial dataset from ByteDance Cloud demonstrate that KRONE achieves consistent improvements in detection accuracy, F1-score, data efficiency, resource efficiency, and interpretability. KRONE improves the F1-score by more than 10 percentage points over prior methods while reducing LLM usage to only a small fraction of the test data.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07303v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07287v1",
      "title": "Patch-to-PoC: A Systematic Study of Agentic LLM Systems for Linux Kernel N-Day Reproduction",
      "authors": [
        "Juefei Pu",
        "Xingyu Li",
        "Haonan Li",
        "Zhengchuan Liang",
        "Jonathan Cox",
        "Yifan Wu",
        "Kareem Shehada",
        "Arrdya Srivastav",
        "Zhiyun Qian"
      ],
      "published": "2026-02-07T00:34:08Z",
      "updated": "2026-02-07T00:34:08Z",
      "year": 2026,
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07287v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07287v1",
      "abstract": "Autonomous large language model (LLM) based systems have recently shown promising results across a range of cybersecurity tasks. However, there is no systematic study on their effectiveness in autonomously reproducing Linux kernel vulnerabilities with concrete proofs-of-concept (PoCs). Owing to the size, complexity, and low-level nature of the Linux kernel, such tasks are widely regarded as particularly challenging for current LLM-based approaches. In this paper, we present the first large-scale study of LLM-based Linux kernel vulnerability reproduction. For this purpose, we develop K-Repro, an LLM-based agentic system equipped with controlled code-browsing, virtual machine management, interaction, and debugging capabilities. Using kernel security patches as input, K-Repro automates end-to-end bug reproduction of N-day vulnerabilities in the Linux kernel. On a dataset of 100 real-world exploitable Linux kernel vulnerabilities collected from KernelCTF, our results show that K-Repro can generate PoCs that reproduce over 50\\% of the cases with practical time and monetary cost. Beyond aggregate success rates, we perform an extensive study of effectiveness, efficiency, stability, and impact factors to explain when agentic reproduction succeeds, where it fails, and which components drive performance. These findings provide actionable guidance for building more reliable autonomous security agents and for assessing real-world N-day risk from both offensive and defensive perspectives.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07287v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07264v1",
      "title": "aerial-autonomy-stack -- a Faster-than-real-time, Autopilot-agnostic, ROS2 Framework to Simulate and Deploy Perception-based Drones",
      "authors": [
        "Jacopo Panerati",
        "Sina Sajjadi",
        "Sina Soleymanpour",
        "Varunkumar Mehta",
        "Iraj Mantegh"
      ],
      "published": "2026-02-06T23:29:33Z",
      "updated": "2026-02-06T23:29:33Z",
      "year": 2026,
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07264v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07264v1",
      "abstract": "Unmanned aerial vehicles are rapidly transforming multiple applications, from agricultural and infrastructure monitoring to logistics and defense. Introducing greater autonomy to these systems can simultaneously make them more effective as well as reliable. Thus, the ability to rapidly engineer and deploy autonomous aerial systems has become of strategic importance. In the 2010s, a combination of high-performance compute, data, and open-source software led to the current deep learning and AI boom, unlocking decades of prior theoretical work. Robotics is on the cusp of a similar transformation. However, physical AI faces unique hurdles, often combined under the umbrella term \"simulation-to-reality gap\". These span from modeling shortcomings to the complexity of vertically integrating the highly heterogeneous hardware and software systems typically found in field robots. To address the latter, we introduce aerial-autonomy-stack, an open-source, end-to-end framework designed to streamline the pipeline from (GPU-accelerated) perception to (flight controller-based) action. Our stack allows the development of aerial autonomy using ROS2 and provides a common interface for two of the most popular autopilots: PX4 and ArduPilot. We show that it supports over 20x faster-than-real-time, end-to-end simulation of a complete development and deployment stack -- including edge compute and networking -- significantly compressing the build-test-release cycle of perception-based autonomy.",
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07264v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07195v1",
      "title": "Automated Modernization of Machine Learning Engineering Notebooks for Reproducibility",
      "authors": [
        "Bihui Jin",
        "Kaiyuan Wang",
        "Pengyu Nie"
      ],
      "published": "2026-02-06T21:03:36Z",
      "updated": "2026-02-06T21:03:36Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.LG"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07195v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07195v1",
      "abstract": "Interactive computational notebooks (e.g., Jupyter notebooks) are widely used in machine learning engineering (MLE) to program and share end-to-end pipelines, from data preparation to model training and evaluation. However, environment erosion-the rapid evolution of hardware and software ecosystems for machine learning-has rendered many published MLE notebooks non-reproducible in contemporary environments, hindering code reuse and scientific progress. To quantify this gap, we study 12,720 notebooks mined from 79 popular Kaggle competitions: only 35.4% remain reproducible today. Crucially, we find that environment backporting, i.e., downgrading dependencies to match the submission time, does not improve reproducibility but rather introduces additional failure modes. To address environment erosion, we design and implement MLEModernizer, an LLM-driven agentic framework that treats the contemporary environment as a fixed constraint and modernizes notebook code to restore reproducibility. MLEModernizer iteratively executes notebooks, collects execution feedback, and applies targeted fixes in three types: error-repair, runtime-reduction, and score-calibration. Evaluated on 7,402 notebooks that are non-reproducible under the baseline environment, MLEModernizer makes 5,492 (74.2%) reproducible. MLEModernizer enables practitioners to validate, reuse, and maintain MLE artifacts as the hardware and software ecosystems continue to evolve.",
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07195v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07182v1",
      "title": "Measuring Complexity at the Requirements Stage: Spectral Metrics as Development Effort Predictors",
      "authors": [
        "Maximilian Vierlboeck",
        "Antonio Pugliese",
        "Roshanak Nilchian",
        "Paul Grogan",
        "Rashika Sugganahalli Natesh Babu"
      ],
      "published": "2026-02-06T20:38:00Z",
      "updated": "2026-02-06T20:38:00Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07182v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07182v1",
      "abstract": "Complexity in engineered systems presents one of the most persistent challenges in modern development since it is driving cost overruns, schedule delays, and outright project failures. Yet while architectural complexity has been studied, the structural complexity embedded within requirements specifications remains poorly understood and inadequately quantified. This gap is consequential: requirements fundamentally drive system design, and complexity introduced at this stage propagates through architecture, implementation, and integration. To address this gap, we build on Natural Language Processing methods that extract structural networks from textual requirements. Using these extracted structures, we conducted a controlled experiment employing molecular integration tasks as structurally isomorphic proxies for requirements integration - leveraging the topological equivalence between molecular graphs and requirement networks while eliminating confounding factors such as domain expertise and semantic ambiguity. Our results demonstrate that spectral measures predict integration effort with correlations exceeding 0.95, while structural metrics achieve correlations above 0.89. Notably, density-based metrics show no significant predictive validity. These findings indicate that eigenvalue-derived measures capture cognitive and effort dimensions that simpler connectivity metrics cannot. As a result, this research bridges a critical methodological gap between architectural complexity analysis and requirements engineering practice, providing a validated foundation for applying these metrics to requirements engineering, where similar structural complexity patterns may predict integration effort.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07182v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07150v1",
      "title": "On Randomness in Agentic Evals",
      "authors": [
        "Bjarni Haukur Bjarnason",
        "André Silva",
        "Martin Monperrus"
      ],
      "published": "2026-02-06T19:49:13Z",
      "updated": "2026-02-06T19:49:13Z",
      "year": 2026,
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07150v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07150v1",
      "abstract": "Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.",
      "title_signals": {
        "has_colon": false,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07150v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.07147v1",
      "title": "Architectural Anti-Patterns in Student-Developed Microservice Architectures: An Exploratory Study",
      "authors": [
        "Marco De Luca",
        "Michele Perlotto",
        "Anna Rita Fasolino",
        "Porfirio Tramontana"
      ],
      "published": "2026-02-06T19:44:22Z",
      "updated": "2026-02-06T19:44:22Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.07147v1",
      "pdf_url": "https://arxiv.org/pdf/2602.07147v1",
      "abstract": "Teaching microservice architectures is challenging due to distributed complexity and the gap between academia and industry. Understanding the quality issues students introduce in MSAs is essential to improve education. This study analyzes student-developed microservices using an established anti-pattern taxonomy and derives lessons learned with actionable teaching recommendations. We conducted a longitudinal, project-based course (2023-2025) involving 216 Master's students (67 teams) who designed and deployed a realistic, containerized MSA for a gamified testing platform. The final systems revealed 23 out of 58 known MSA anti-patterns, spanning five categories. Security issues were most frequent, highlighting weaknesses in authentication, authorization, and data protection. Team Organization and Service Interaction problems followed, reflecting limited DevOps experience and difficulties in inter-service coordination. Fewer issues appeared in Intra-service Design and Inter-service Decomposition, suggesting students generally defined service boundaries well. Overall, students prioritized feature delivery over robustness and operational discipline. To address this, we recommend enforcing minimal standards (API contracts, gateways), providing labs on resilient communication, integrating security-by-design practices, and offering CI-CD templates. The paper contributes a realistic, full-scale educational experience and a replicable model for teaching industry-aligned microservice architecture.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.07147v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.06875v1",
      "title": "TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code",
      "authors": [
        "Jiangping Huang",
        "Wenguang Ye",
        "Weisong Sun",
        "Jian Zhang",
        "Mingyue Zhang",
        "Yang Liu"
      ],
      "published": "2026-02-06T16:59:48Z",
      "updated": "2026-02-06T16:59:48Z",
      "year": 2026,
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "abs_url": "http://arxiv.org/abs/2602.06875v1",
      "pdf_url": "https://arxiv.org/pdf/2602.06875v1",
      "abstract": "Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": false,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.06875v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.06831v1",
      "title": "Statistical-Based Metric Threshold Setting Method for Software Fault Prediction in Firmware Projects: An Industrial Experience",
      "authors": [
        "Marco De Luca",
        "Domenico Amalfitano",
        "Anna Rita Fasolino",
        "Porfirio Tramontana"
      ],
      "published": "2026-02-06T16:19:36Z",
      "updated": "2026-02-06T16:19:36Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.06831v1",
      "pdf_url": "https://arxiv.org/pdf/2602.06831v1",
      "abstract": "Ensuring software quality in embedded firmware is critical, especially in safety-critical domains where compliance with functional safety standards (ISO 26262) requires strong guarantees of software reliability. While machine learning-based fault prediction models have demonstrated high accuracy, their lack of interpretability limits their adoption in industrial settings. Developers need actionable insights that can be directly employed in software quality assurance processes and guide defect mitigation strategies. In this paper, we present a structured process for defining context-specific software metric thresholds suitable for integration into fault detection workflows in industrial settings. Our approach supports cross-project fault prediction by deriving thresholds from one set of projects and applying them to independently developed firmware, thereby enabling reuse across similar software systems without retraining or domain-specific tuning. We analyze three real-world C-embedded firmware projects provided by an industrial partner, using Coverity and Understand static analysis tools to extract software metrics. Through statistical analysis and hypothesis testing, we identify discriminative metrics and derived empirical threshold values capable of distinguishing faulty from non-faulty functions. The derived thresholds are validated through an experimental evaluation, demonstrating their effectiveness in identifying fault-prone functions with high precision. The results confirm that the derived thresholds can serve as an interpretable solution for fault prediction, aligning with industry standards and SQA practices. This approach provides a practical alternative to black-box AI models, allowing developers to systematically assess software quality, take preventive actions, and integrate metric-based fault prediction into industrial development workflows to mitigate software faults.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.06831v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.06751v1",
      "title": "Beyond Function-Level Analysis: Context-Aware Reasoning for Inter-Procedural Vulnerability Detection",
      "authors": [
        "Yikun Li",
        "Ting Zhang",
        "Jieke Shi",
        "Chengran Yang",
        "Junda He",
        "Xin Zhou",
        "Jinfeng Jiang",
        "Huihui Huang",
        "Wen Bin Leow",
        "Yide Yin",
        "Eng Lieh Ouh",
        "Lwin Khin Shar",
        "David Lo"
      ],
      "published": "2026-02-06T14:49:49Z",
      "updated": "2026-02-06T14:49:49Z",
      "year": 2026,
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.06751v1",
      "pdf_url": "https://arxiv.org/pdf/2602.06751v1",
      "abstract": "Recent progress in ML and LLMs has improved vulnerability detection, and recent datasets have reduced label noise and unrelated code changes. However, most existing approaches still operate at the function level, where models are asked to predict whether a single function is vulnerable without inter-procedural context. In practice, vulnerability presence and root cause often depend on contextual information. Naively appending such context is not a reliable solution: real-world context is long, redundant, and noisy, and we find that unstructured context frequently degrades the performance of strong fine-tuned code models. We present CPRVul, a context-aware vulnerability detection framework that couples Context Profiling and Selection with Structured Reasoning. CPRVul constructs a code property graph, and extracts candidate context. It then uses an LLM to generate security-focused profiles and assign relevance scores, selecting only high-impact contextual elements that fit within the model's context window. In the second phase, CPRVul integrates the target function, the selected context, and auxiliary vulnerability metadata to generate reasoning traces, which are used to fine-tune LLMs for reasoning-based vulnerability detection. We evaluate CPRVul on three high-quality vulnerability datasets: PrimeVul, TitanVul, and CleanVul. Across all datasets, CPRVul consistently outperforms function-only baselines, achieving accuracies ranging from 64.94% to 73.76%, compared to 56.65% to 63.68% for UniXcoder. Specifically, on the challenging PrimeVul benchmark, CPRVul achieves 67.78% accuracy, outperforming prior state-of-the-art approaches, improving accuracy from 55.17% to 67.78% (22.9% improvement). Our ablations further show that neither raw context nor processed context alone benefits strong code models; gains emerge only when processed context is paired with structured reasoning.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": false
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": true,
        "has_gap_phrase": true,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.06751v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.06709v1",
      "title": "Using Large Language Models to Support Automation of Failure Management in CI/CD Pipelines: A Case Study in SAP HANA",
      "authors": [
        "Duong Bui",
        "Stefan Grintz",
        "Alexander Berndt",
        "Thomas Bach"
      ],
      "published": "2026-02-06T13:55:48Z",
      "updated": "2026-02-06T13:55:48Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.06709v1",
      "pdf_url": "https://arxiv.org/pdf/2602.06709v1",
      "abstract": "CI/CD pipeline failure management is time-consuming when performed manually. Automating this process is non-trivial because the information required for effective failure management is unstructured and cannot be automatically processed by traditional programs. With their ability to process unstructured data, large language models (LLMs) have shown promising results for automated failure management by previous work. Following these studies, we evaluated whether an LLM-based system could automate failure management in a CI/CD pipeline in the context of a large industrial software project, namely SAP HANA. We evaluated the ability of the LLM-based system to identify the error location and to propose exact solutions that contain no unnecessary actions. To support the LLM in generating exact solutions, we provided it with different types of domain knowledge, including pipeline information, failure management instructions, and data from historical failures. We conducted an ablation study to determine which type of domain knowledge contributed most to solution accuracy. The results show that data from historical failures contributed the most to the system's accuracy, enabling it to produce exact solutions in 92.1% of cases in our dataset. The system correctly identified the error location with 97.4% accuracy when provided with domain knowledge, compared to 84.2% accuracy without it. In conclusion, our findings indicate that LLMs, when provided with data from historical failures, represent a promising approach for automating CI/CD pipeline failure management.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": false,
        "has_results": true
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.06709v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    },
    {
      "arxiv_id": "2602.06671v1",
      "title": "Code vs Serialized AST Inputs for LLM-Based Code Summarization: An Empirical Study",
      "authors": [
        "Shijia Dong",
        "Haoruo Zhao",
        "Paul Harvey"
      ],
      "published": "2026-02-06T12:55:01Z",
      "updated": "2026-02-06T12:55:01Z",
      "year": 2026,
      "categories": [
        "cs.SE"
      ],
      "abs_url": "http://arxiv.org/abs/2602.06671v1",
      "pdf_url": "https://arxiv.org/pdf/2602.06671v1",
      "abstract": "Summarizing source code into natural language descriptions (code summarization) helps developers better understand program functionality and reduce the burden of software maintenance. Abstract Syntax Trees (ASTs), as opposed to source code, have been shown to improve summarization quality in traditional encoder-decoder-based code summarization models. However, most large language model (LLM)-based code summarization methods rely on raw code or only incorporate partial AST signals, meaning that the potential of complete AST representation has not been fully explored for LLMs. This paper presents AST(NIT), an AST augmentation and serialization method that preserves lexical details and encodes structural information into LLM-compatible sequences. Experiments with the LLaMA-3.1-8B model on the CodeXGLUE Python dataset show that the proposed serialized ASTs reduce the length of LLM inputs, require shorter training times, and achieve summarization quality comparable to existing approaches.",
      "title_signals": {
        "has_colon": true,
        "has_question": false,
        "has_towards": false,
        "has_less_is_more": false,
        "has_acronym": true
      },
      "abstract_signals": {
        "has_numbers": true,
        "has_action_verb": false,
        "has_gap_phrase": true,
        "has_results": false
      },
      "pdf": {
        "source": "arxiv",
        "url": "https://arxiv.org/pdf/2602.06671v1",
        "downloaded": false,
        "error": "skipped (--no-download)",
        "extract_error": null,
        "path": null,
        "preview_text_path": null
      },
      "signals": {}
    }
  ]
}