{
  "generated_at": "2026-02-16T14:57:53.421596+00:00",
  "source_publications_url": "https://xing-hu.github.io/publications/",
  "paper_count": 70,
  "papers": [
    {
      "id": "aae8646f497a",
      "label": "C4",
      "title": "DeepCommenter: A Deep Code Comment Generation Tool with Hybrid Lexical and Syntactical Information",
      "year": 2020,
      "venue": "The 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2020)",
      "authors": "[C4] Boao Li, Meng Yan, Xin Xia, Xing Hu, Ge Li, David Lo",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W3099640733",
        "doi": "https://doi.org/10.1145/3368089.3417926",
        "display_name": "DeepCommenter: a deep code comment generation tool with hybrid lexical and syntactical information",
        "publication_year": 2020,
        "is_oa": false,
        "abstract": "As the scale of software projects increases, the code comments are more and more important for program comprehension. Unfortunately, many code comments are missing, mismatched or outdated due to tight development schedule or other reasons. Automatic code comment generation is of great help for developers to comprehend source code and reduce their workload. Thus, we propose a code comment generation tool (DeepCommenter) to generate descriptive comments for Java methods. DeepCommenter formulates the comment generation task as a machine translation problem and exploits a deep neural network that combines the lexical and structural information of Java methods. We implement DeepCommenter in the form of an Integrated Development Environment (i.e., Intellij IDEA) plug-in. Such plug-in is built upon a Client/Server architecture. The client formats the code selected by the user, sends request to the server and inserts the comment generated by the server above the selected code. The server listens for client's request, analyzes the requested code using the pre-trained model and sends back the generated comment to the client. The pre-trained model learns both the lexical and syntactical information from source code tokens and Abstract Syntax Trees (AST) respectively and combines these two types of information together to generate comments. To evaluate DeepCommenter, we conduct experiments on a large corpus built from a large number of open source Java projects on GitHub. The experimental results on different metrics show that DeepCommenter outperforms the state-of-the-art approaches by a substantial margin.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "0e9bd2d485b0",
      "label": "C5",
      "title": "https://lj2lijia.github.io/index.html",
      "year": 2021,
      "venue": "36th IEEE/ACM International Conference on Automated Software Engineering (ASE 2021)",
      "authors": "[C5] <a href=",
      "is_award": false,
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "is_oa": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/EditSum_ASE2021.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2021_ASE_https_lj2lijia.github.io_index.html_0e9bd2d485b0.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2021_ASE_https_lj2lijia.github.io_index.html_0e9bd2d485b0.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "79c7f3ab1621",
      "label": "C6",
      "title": "Automating User Notice Generation for Smart Contract Functions",
      "year": 2021,
      "venue": "36th IEEE/ACM International Conference on Automated Software Engineering (ASE 2021)",
      "authors": "[C6] Xing Hu, Zhipeng Gao, Xin Xia, David Lo, Xiaohu Yang",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4206459383",
        "doi": "https://doi.org/10.1109/ase51524.2021.9678552",
        "display_name": "Automating User Notice Generation for Smart Contract Functions",
        "publication_year": 2021,
        "is_oa": false,
        "abstract": "Smart contracts have obtained much attention and are crucial for automatic financial and business transactions. For end-users who have never seen the source code, they can read the user notice shown in end-user client to understand what a transaction does of a smart contract function. However, due to time constraints or lack of motivation, user notice is often missing during the development of smart contracts. For end-users who lack the information of the user notices, there is no easy way for them to check the code semantics of the smart contracts. Thus, in this paper, we propose a new approach SMARTDOC to generate user notice for smart contract functions automatically. Our tool can help end-users better understand the smart contract and aware of the financial risks, improving the users' confidence on the reliability of the smart contracts. SMARTDOC exploits the Transformer to learn the representation of source code and generates natural language descriptions from the learned representation. We also integrate the Pointer mechanism to copy words from the input source code instead of generating words during the prediction process. We extract 7,878 〈function, notice〉 pairs from 54,739 smart contracts written in Solidity. Due to the limited amount of collected smart contract functions (i.e., 7,878 functions), we exploit a transfer learning technique to utilize the learned knowledge to improve the performance of SMARTDOC. The learned knowledge obtained by the pre-training on a corpus of Java code, that has similar characteristics as Solidity code. The experimental results show that our approach can effectively generate user notice given the source code and significantly outperform the state-of-the-art approaches. To investigate human perspectives on our generated user notice, we also conduct a human evaluation and ask participants to score user notice generated by different approaches. Results show that SMARTDOC outperforms baselines from three aspects, naturalness, informativeness, and similarity.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/ase21.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2021_ASE_Automating User Notice Generation for Smart Contract Functions_79c7f3ab1621.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2021_ASE_Automating User Notice Generation for Smart Contract Functions_79c7f3ab1621.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "9e7717578e9c",
      "label": "J4",
      "title": "Assessing and Improving an Evaluation Dataset for Detecting Semantic Code Clones via Deep Learning",
      "year": 2021,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J4] Hao Yu, Xing Hu, Ge Li, Tao Xie, Ying Li, Qianxiang Wang",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4283525684",
        "doi": "https://doi.org/10.1145/3502852",
        "display_name": "Assessing and Improving an Evaluation Dataset for Detecting Semantic Code Clones via Deep Learning",
        "publication_year": 2022,
        "is_oa": false,
        "abstract": "In recent years, applying deep learning to detect semantic code clones has received substantial attention from the research community. Accordingly, various evaluation benchmark datasets, with the most popular one as BigCloneBench, are constructed and selected as benchmarks to assess and compare different deep learning models for detecting semantic clones. However, there is no study to investigate whether an evaluation benchmark dataset such as BigCloneBench is properly used to evaluate models for detecting semantic code clones. In this article, we present an experimental study to show that BigCloneBench typically includes semantic clone pairs that use the same identifier names, which however are not used in non-semantic-clone pairs. Subsequently, we propose an undesirable-by-design Linear-Model that considers only which identifiers appear in a code fragment; this model can achieve high effectiveness for detecting semantic clones when evaluated on BigCloneBench, even comparable to state-of-the-art deep learning models recently proposed for detecting semantic clones. To alleviate these issues, we abstract a subset of the identifier names (including type, variable, and method names) in BigCloneBench to result in AbsBigCloneBench and use AbsBigCloneBench to better assess the effectiveness of deep learning models on the task of detecting semantic clones.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/tosem1.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2021_TOSEM_Assessing and Improving an Evaluation Dataset for Detecting Semantic Code Clones_9e7717578e9c.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2021_TOSEM_Assessing and Improving an Evaluation Dataset for Detecting Semantic Code Clones_9e7717578e9c.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "446429ee2791",
      "label": "J5",
      "title": "Correlating Automated and Human Evaluation of Code Documentation Generation Quality",
      "year": 2021,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J5] Xing Hu, Qiuyuan Chen, Haoye Wang, Xin Xia, David Lo, Thomas Zimmermann",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4281400706",
        "doi": "https://doi.org/10.1145/3502853",
        "display_name": "Correlating Automated and Human Evaluation of Code Documentation Generation Quality",
        "publication_year": 2022,
        "is_oa": true,
        "abstract": "Automatic code documentation generation has been a crucial task in the field of software engineering. It not only relieves developers from writing code documentation but also helps them to understand programs better. Specifically, deep-learning-based techniques that leverage large-scale source code corpora have been widely used in code documentation generation. These works tend to use automatic metrics (such as BLEU, METEOR, ROUGE, CIDEr, and SPICE) to evaluate different models. These metrics compare generated documentation to reference texts by measuring the overlapping words. Unfortunately, there is no evidence demonstrating the correlation between these metrics and human judgment. We conduct experiments on two popular code documentation generation tasks, code comment generation and commit message generation, to investigate the presence or absence of correlations between these metrics and human judgments. For each task, we replicate three state-of-the-art approaches and the generated documentation is evaluated automatically in terms of BLEU, METEOR, ROUGE-L, CIDEr, and SPICE. We also ask 24 participants to rate the generated documentation considering three aspects (i.e., language, content, and effectiveness). Each participant is given Java methods or commit diffs along with the target documentation to be rated. The results show that the ranking of generated documentation from automatic metrics is different from that evaluated by human annotators. Thus, these automatic metrics are not reliable enough to replace human evaluation for code documentation generation tasks. In addition, METEOR shows the strongest correlation (with moderate Pearson correlation r about 0.7) to human evaluation metrics. However, it is still much lower than the correlation observed between different annotators (with a high Pearson correlation r about 0.8) and correlations that are reported in the literature for other tasks (e.g., Neural Machine Translation [ 39 ]). Our study points to the need to develop specialized automated evaluation metrics that can correlate more closely to human evaluation metrics for code generation tasks.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/tosem2.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2021_TOSEM_Correlating Automated and Human Evaluation of Code Documentation Generation Qual_446429ee2791.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2021_TOSEM_Correlating Automated and Human Evaluation of Code Documentation Generation Qual_446429ee2791.txt"
      },
      "signals": {
        "has_abstract_heading": false,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": true,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": false
      }
    },
    {
      "id": "bed85dfaaba4",
      "label": "C7",
      "title": "Practitioners’ Expectations on Automated Code Comment Generation",
      "year": 2022,
      "venue": "44th ACM/IEEE International Conference on Software Engineering （ICSE 2022)",
      "authors": "[C7] Xing Hu, Xin Xia, David Lo, Zhiyuan Wan, Qiuyuan Chen, Thomas Zimmermann",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4284704125",
        "doi": "https://doi.org/10.1145/3510003.3510152",
        "display_name": "Practitioners' expectations on automated code comment generation",
        "publication_year": 2022,
        "is_oa": false,
        "abstract": "Good comments are invaluable assets to software projects, as they help developers understand and maintain projects. However, due to some poor commenting practices, comments are often missing or inconsistent with the source code. Software engineering practitioners often spend a significant amount of time and effort reading and understanding programs without or with poor comments. To counter this, researchers have proposed various techniques to automatically generate code comments in recent years, which can not only save developers time writing comments but also help them better understand existing software projects. However, it is unclear whether these techniques can alleviate comment issues and whether practitioners appreciate this line of research. To fill this gap, we performed an empirical study by interviewing and surveying practitioners about their expectations of research in code comment generation. We then compared what practitioners need and the current state-of-the-art research by performing a literature review of papers on code comment generation techniques published in the premier publication venues from 2010 to 2020. From this comparison, we highlighted the directions where researchers need to put effort to develop comment generation techniques that matter to practitioners.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/ICSE22.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2022_ICSE_Practitioners’ Expectations on Automated Code Comment Generation_bed85dfaaba4.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2022_ICSE_Practitioners’ Expectations on Automated Code Comment Generation_bed85dfaaba4.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": true,
        "has_threats_to_validity": false,
        "has_evaluation_section": false,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "27893b65283c",
      "label": "C9",
      "title": "Automated Unearthing of Dangerous Issue Reports",
      "year": 2022,
      "venue": "The 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022)",
      "authors": "[C9] Shengyi Pan, Jiayuan Zhou, Filipe R. Cogo, Xin Xia, Lingfeng Bao, Xing Hu, Shanping Li, Ahmed E. Hassan",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4308648311",
        "doi": "https://doi.org/10.1145/3540250.3549156",
        "display_name": "Automated unearthing of dangerous issue reports",
        "publication_year": 2022,
        "is_oa": false,
        "abstract": "The coordinated vulnerability disclosure (CVD) process is commonly adopted for open source software (OSS) vulnerability management, which suggests to privately report the discovered vulnerabilities and keep relevant information secret until the official disclosure. However, in practice, due to various reasons (e.g., lacking security domain expertise or the sense of security management), many vulnerabilities are first reported via public issue reports (IRs) before its official disclosure. Such IRs are dangerous IRs, since attackers can take advantages of the leaked vulnerability information to launch zero-day attacks. It is crucial to identify such dangerous IRs at an early stage, such that OSS users can start the vulnerability remediation process earlier and OSS maintainers can timely manage the dangerous IRs. In this paper, we propose and evaluate a deep learning based approach, namely MemVul, to automatically identify dangerous IRs at the time they are reported. MemVul augments the neural networks with a memory component, which stores the external vulnerability knowledge from Common Weakness Enumeration (CWE). We rely on publicly accessible CVE-referred IRs (CIRs) to operationalize the concept of dangerous IR. We mine 3,937 CIRs distributed across 1,390 OSS projects hosted on GitHub. Evaluated under a practical scenario of high data imbalance, MemVul achieves the best trade-off between precision and recall among all baselines. In particular, the F1-score of MemVul (i.e., 0.49) improves the best performing baseline by 44%. For IRs that are predicted as CIRs but not reported to CVE, we conduct a user study to investigate their usefulness to OSS stakeholders. We observe that 82% (41 out of 50) of these IRs are security-related and 28 of them are suggested by security experts to be publicly disclosed, indicating MemVul is capable of identifying undisclosed dangerous IRs.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/fse22.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2022_FSE_Automated Unearthing of Dangerous Issue Reports_27893b65283c.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2022_FSE_Automated Unearthing of Dangerous Issue Reports_27893b65283c.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "4a286494b9c8",
      "label": "C10",
      "title": "https://lj2lijia.github.io/index.html",
      "year": 2023,
      "venue": "45th ACM/IEEE International Conference on Software Engineering （ICSE 2023)",
      "authors": "[C10] <a href=",
      "is_award": false,
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "is_oa": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/SkCoder_ICSE2023.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2023_ICSE_https_lj2lijia.github.io_index.html_4a286494b9c8.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2023_ICSE_https_lj2lijia.github.io_index.html_4a286494b9c8.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "11a3c8fef7ca",
      "label": "C11",
      "title": "CoLeFunDa: Explainable Silent Vulnerability Fix Identification",
      "year": 2023,
      "venue": "45th ACM/IEEE International Conference on Software Engineering （ICSE 2023)",
      "authors": "[C11] Jiayuan Zhou, Michael Pacheco, Jinfu Chen, Xing Hu*, Xin Xia, David Lo, Ahmed E. Hassan",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4384302745",
        "doi": "https://doi.org/10.1109/icse48619.2023.00214",
        "display_name": "CoLeFunDa: Explainable Silent Vulnerability Fix Identification",
        "publication_year": 2023,
        "is_oa": false,
        "abstract": "It is common practice for OSS users to leverage and monitor security advisories to discover newly disclosed OSS vulnerabilities and their corresponding patches for vulnerability remediation. It is common for vulnerability fixes to be publicly available one week earlier than their disclosure. This gap in time provides an opportunity for attackers to exploit the vulnerability. Hence, OSS users need to sense the fix as early as possible so that the vulnerability can be remediated before it is exploited. However, it is common for OSS to adopt a vulnerability disclosure policy which causes the majority of vulnerabilities to be fixed silently, meaning the commit with the fix does not indicate any vulnerability information. In this case even if a fix is identified, it is hard for OSS users to understand the vulnerability and evaluate its potential impact. To improve early sensing of vulnerabilities, the identification of silent fixes and their corresponding explanations (e.g., the corresponding common weakness enumeration (CWE) and exploitability rating) are equally important. However, it is challenging to identify silent fixes and provide explanations due to the limited and diverse data. To tackle this challenge, we propose CoLeFunDa: a framework consisting of a Contrastive Learner and FunDa, which is a novel approach for Function change Data augmentation. FunDa first increases the fix data (i.e., code changes) at the function level with unsupervised and supervised strategies. Then the contrastive learner leverages contrastive learning to effectively train a function change encoder, FCBERT, from diverse fix data. Finally, we leverage FCBERT to further fine-tune three downstream tasks, i.e., silent fix identification, CWE category classification, and exploitability rating classification, respectively. Our result shows that CoLeFunDa outperforms all the state-of-art baselines in all downstream tasks. We also conduct a survey to verify the effectiveness of CoLeFunDa in practical usage. The result shows that CoLeFunDa can categorize 62.5% (25 out of 40) CVEs with correct CWE categories within the top 2 recommendations.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/icse23colefunda.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2023_ICSE_CoLeFunDa_ Explainable Silent Vulnerability Fix Identification_11a3c8fef7ca.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2023_ICSE_CoLeFunDa_ Explainable Silent Vulnerability Fix Identification_11a3c8fef7ca.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "36a3ca1294be",
      "label": "C12",
      "title": "AutoDebloater: Automated Android App Debloating",
      "year": 2023,
      "venue": "38th IEEE/ACM International Conference on Automated Software Engineering (ASE 2023)",
      "authors": "[C12] Jiakun Liu, Xing Hu, Ferdian Thung, Shahar Maoz, Eran Toch, Debin Gao, David Lo",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4388502415",
        "doi": "https://doi.org/10.1109/ase56229.2023.00017",
        "display_name": "AutoDebloater: Automated Android App Debloating",
        "publication_year": 2023,
        "is_oa": false,
        "abstract": "Android applications are getting bigger with an increasing number of features. However, not all the features are needed by a specific user. The unnecessary features can increase the attack surface and cost additional resources (e.g., storage and memory). Therefore, it is important to remove unnecessary features from Android applications. However, it is difficult for the end users to fully explore the apps to identify the unnecessary features, and there is no off-the-shelf tool available to assist users to debloat the apps by themselves. In this work, we propose AutoDebloater to debloat Android applications automatically for end users. AutoDebloater is a web application that can be accessed by end-users through a web browser. In particular, AutoDebloater can automatically explore an app and identify the transitions between activities. Then, AutoDebloater will present the Activity Transition Graph to users and ask them to select the activities they do not want to keep. Finally, AutoDebloater will remove the activities that are selected by users from the app. We conducted a user study on five Android apps downloaded from three categories (i.e., Finance, Tools, and Navigation) in Google Play and F-Droid. The results show that users are satisfied with AutoDebloater in terms of the stability of the debloated apps and the ability of AutoDebloater to identify features that are never noticed before. The tool is available at http://autodebloater.club. The code is available at https://github.com/jiakun-liu/autodebloater/ and the demonstration video can be found at https://youtu.be/Gmz0-p2n9D4.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "6859112aee0c",
      "label": "C13",
      "title": "Identify and Update Test Cases when Production Code Changes: A Transformer-based Approach",
      "year": 2023,
      "venue": "38th IEEE/ACM International Conference on Automated Software Engineering (ASE 2023)",
      "authors": "[C13] Xing Hu, Zhuang Liu, Xin Xia, Zhongxin Liu, Tongtong Xu, Xiaohu Yang",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4388483507",
        "doi": "https://doi.org/10.1109/ase56229.2023.00165",
        "display_name": "Identify and Update Test Cases When Production Code Changes: A Transformer-Based Approach",
        "publication_year": 2023,
        "is_oa": false,
        "abstract": "Software testing is one of the most essential parts of the software lifecycle and requires a substantial amount of time and effort. During the software evolution, test cases should co-evolve with the production code. However, the co-evolution of test cases often fails due to tight project schedules and other reasons. Obsolete test cases improve the cost of software maintenance and may fail to reveal faults and even lead to future bugs. Therefore, it is essential to detect and update these obsolete test cases in time. In this paper, we propose a novel approach Ceprot (Co-Evolution of Production-Test Code) to identify outdated test cases and update them automatically according to changes in the production code. Ceprot consists of two stages, i.e., obsolete test identification and updating. Specifically, given a production code change and a corresponding test case, Ceprot first identifies whether the test case should be updated. If the test is identified as obsolete, Ceprot will update it to a new version of test case. To evaluate the effectiveness of the two stages, we construct two datasets. Our dataset focuses on method-level production code changes and updates on their obsolete test cases. The experimental results show that Ceprot can effectively identify obsolete test cases with precision and recall of 98.3% and 90.0%, respectively. In addition, test cases generated by Ceprot are identical to the ground truth for 12.3% of samples that are identified as obsolete by Ceprot. We also conduct dynamic evaluation and human evaluation to measure the effectiveness of the updated test cases by Ceprot. 48.0% of updated test cases can be compiled and the average coverage of updated cases is 34.2% which achieves 89% coverage improvement over the obsolete tests. We believe that this study can motivate the co-evolution of production and test code.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/ASE2023CEPROT.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2023_ASE_Identify and Update Test Cases when Production Code Changes_ A Transformer-based_6859112aee0c.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2023_ASE_Identify and Update Test Cases when Production Code Changes_ A Transformer-based_6859112aee0c.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "a3ec2c67e8b3",
      "label": "C14",
      "title": "Are They All Good? Studying Practitioners' Expectations on the Readability of Log Messages",
      "year": 2023,
      "venue": "38th IEEE/ACM International Conference on Automated Software Engineering (ASE 2023)",
      "authors": "[C14] Zhenhao Li, An Ran Chen, Xing Hu*, Xin Xia, Tse-Hsun (Peter) Chen, Weiyi Shang",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4388483112",
        "doi": "https://doi.org/10.1109/ase56229.2023.00136",
        "display_name": "Are They All Good? Studying Practitioners' Expectations on the Readability of Log Messages",
        "publication_year": 2023,
        "is_oa": false,
        "abstract": "Developers write logging statements to generate logs that provide run-time information for various tasks. The readability of log messages in the logging statements (i.e., the descriptive text) is rather crucial to the value of the generated logs. Immature log messages may slow down or even obstruct the process of log analysis. Despite the importance of log messages, there is still a lack of standards on what constitutes good readability of log messages and how to write them. In this paper, we conduct a series of interviews with 17 industrial practitioners to investigate their expectations on the readability of log messages. Through the interviews, we derive three aspects related to the readability of log messages, including Structure, Information, and Wording, along with several specific practices to improve each aspect. We validate our findings through a series of online questionnaire surveys and receive positive feedback from the participants. We then manually investigate the readability of log messages in large-scale open source systems and find that a large portion (38.1%) of the log messages have inadequate readability. Motivated by such observation, we further explore the potential of automatically classifying the readability of log messages using deep learning and machine learning models. We find that both deep learning and machine learning models can effectively classify the readability of log messages with a balanced accuracy above 80.0% on average. Our study provides comprehensive guidelines for composing log messages to further improve practitioners' logging practices.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/ASE23log.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2023_ASE_Are They All Good_ Studying Practitioners' Expectations on the Readability of Lo_a3ec2c67e8b3.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2023_ASE_Are They All Good_ Studying Practitioners' Expectations on the Readability of Lo_a3ec2c67e8b3.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": true,
        "has_threats_to_validity": true,
        "has_evaluation_section": false,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "917359dc5eb9",
      "label": "C15",
      "title": "C3: Code Clone-based Identification of Duplicated Components",
      "year": 2023,
      "venue": "The 31th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE Industry Track 2023)",
      "authors": "[C15] Yanming Yang, Ying Zou, Xing Hu*, David Lo, Chao Ni, John Grundy, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.0214529914529915,
        "id": "https://openalex.org/W4220666464",
        "doi": "https://doi.org/10.5281/zenodo.6336225",
        "display_name": "C3: Code Clone-based Identification of Duplicated Component",
        "publication_year": 2022,
        "is_oa": true,
        "abstract": "The source code of C3 consists of two sections. The first section is used to pre-process the raw dataset. The second section is used to identify cloned components in software. The model runs on python3. In the first part: We first get all building files to obtain the composition information of components in software systems. Note that: different systems may adopt different forms as their building files. For example, in our study, our dataset uses ''CmakeList'' to build their project and Linux adopts ''MakeFile'' as their building files. Therefore, if you want to detect cloned components in Linux, you need to change the related code for suitable the grammar that Linux's building files use. After analyzing the building files, we know the source code in different components and collect the components by zipping the corresponding source code. <pre><code>We use SourcererCC, which is an existing clone detection tool (https://github.com/Mondego/SourcererCC) to detect file-level clones in the system. This tool can detect clones simultaneously for multiple projects。 After detecting the cloned files, we will get two useful files generated by SourcererCC. The first file is: files-stats-0.stats In this file, SourcererCC will provide an ID for all source files in the system. the form is like: 1,1,\"XXX/XXX/XXX/XXX/tools/XXX/stub.c\",\"d41d8cd98f00b204e9800998ecf8427e\",0,1,1,0 1,18,\"XXX/XXX/XXX/XXX/tools/XXX/LLT_UserConfig.c\",\"6d057b39787dafd44717fca6e47578e5\",6576,219,184,26 3,22,\"XXX/XXX/XXX/XXX//mpu/XXX/sa_omc_ftpc.c\",\"87e843612424455f213b42898b1980cb\",63965,1682,1462,1182 The first number represents the component ID and the second number denotes the file ID in this component. For example, the second line represents the 18th source file in Component 1 is ''LLT_UserConfig.c'' and this file and the ''stub.c'' are both the source code in the same component. The third line denotes the 22th source file in Component 3. Note that: the file ID is sequential. The second useful file generated by SourcererCC notes the clone result. The form is: 1,2,4,28 1,4,8,59 ... The first line means the second files in Component 1 is similar to the 28th source file in Component 4. Based on file-level clone results, we calculate the similarity (i.e., clone ratio) between components. We use 11 metrics to describe a component, including: cmpt_id: component ID cmpt_name: component name cmpt_file_num: the number of source files in a component cmpt_code_line: the line of code in each source file in this component clone_cmpt_with_id: (list) the ID of a component that is similar to this component cmpt_clone_file_tuple: records the component ID and source file ID of other components that have similar source files to the source files in this component. [(component ID, file ID), ...] cross_cmpt_clone_file_in_id: the file ID that are similar to the source code of this component cross_cmpt_clone_ratio: the average cross clone ratio cross_cmpt_clone_ratio_with_each_cloned_cmpt: the clone ratio (similarity) with other components (Here other components refer to the components that have similar code to this component.) within_clone_file_in_id: within clone files in this components (i.e., the two source files in a file-level clone pair are both in this component.)</code></pre> In the second part: We first achieve the edges from the cloning relationships. You can run ''python generate_model_input.py para'' (e.g., python generate_model_input.py 1) to genrate the edges. the parameter set 1 which means the k value is set to 0.1. So para can be set from 0 to 10 (int). We build a community network to describe the cloning relationships between components and adopt a community detection algorithm to obtain component-level clone groups. You can run ''python clone_cluster.py para'' (e.g., python clone_cluster.py 5) to get the component-level clone groups under different parameter values. <pre><code> The result form is like: [652, 781, 782, 780, 784, 783, 805, 688, 689, 691, 955, 700, 705, 839, 715, 716, 718, 848, 853] [773, 687, 721, 754, 659, 732, 638, 671]. ... A list represents a clone group, and the IDs in a list are the component IDs.</code></pre>",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/fse2023Industry.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2023_FSE_C3_ Code Clone-based Identification of Duplicated Components_917359dc5eb9.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2023_FSE_C3_ Code Clone-based Identification of Duplicated Components_917359dc5eb9.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "7fd4ea77f603",
      "label": "C16",
      "title": "ACWRecommender: A Tool for Validating Actionable Warnings with Weak Supervision",
      "year": 2023,
      "venue": "38th IEEE/ACM International Conference on Automated Software Engineering (ASE 2023)",
      "authors": "[C16] Zhipeng Xue, Zhipeng Gao, Xing Hu, Shanping Li",
      "is_award": true,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4388483130",
        "doi": "https://doi.org/10.1109/ase56229.2023.00169",
        "display_name": "ACWRecommender: A Tool for Validating Actionable Warnings with Weak Supervision",
        "publication_year": 2023,
        "is_oa": false,
        "abstract": "Static analysis tools have gained popularity among developers for finding potential bugs, but their widespread adoption is hindered by the accomnpanying high false alarm rates (up to 90%). To address this challenge, previous studies proposed the concept of actionable warnings, and apply machine-learning methods to distinguish actionable warnings from false alarms. Despite these efforts, our preliminary study suggests that the current methods used to collect actionable warnings are rather shaky and unreliable, resulting in a large proportion of invalid actionable warnings. In this work, we mined 68,274 reversions from Top-500 Github C repositories to create a substantia actionable warning dataset and assigned weak labels to each warning's likelihood of being a real bug. To automatically identify actionable warnings and recommend those with a high probability of being real bugs (AWHB), we propose a two-stage framework called ACWRecommender. In the first stage, our tool use a pre-trained model, i.e., UniXcoder, to identify actionable warnings from a huge number of SA tool's reported warnings. In the second stage, we rerank valid actionable warnings to the top by using weakly supervised learning. Experimental results showed that our tool outperformed several baselines for actionable warning detection (in terms of F1-score) and performed better for AWHB recommendation (in terms of nDCG and MRR). Additionaly, we also performed an in-the-wild evaluation, we manually validated 24 warnings out of 2,197 reported warnings on 10 randomly selected projects, 22 of which were confirmed by developers as real bugs, demonstrating the practical usage of our tool.",
        "error": null
      },
      "pdf": {
        "url": "http://arxiv.org/pdf/2309.09721v1.pdf",
        "source": "arxiv",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2023_ASE_ACWRecommender_ A Tool for Validating Actionable Warnings with Weak Supervision_7fd4ea77f603.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2023_ASE_ACWRecommender_ A Tool for Validating Actionable Warnings with Weak Supervision_7fd4ea77f603.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "2d4e4e42cb2f",
      "label": "J10",
      "title": "The Lost World: Characterizing and Detecting Undiscovered Test Smells",
      "year": 2023,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J10] Yanming Yang, Xing Hu*, Xin Xia, Xiaohu Yang",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4388827066",
        "doi": "https://doi.org/10.1145/3631973",
        "display_name": "The Lost World: Characterizing and Detecting Undiscovered Test Smells",
        "publication_year": 2023,
        "is_oa": false,
        "abstract": "Test smell refers to poor programming and design practices in testing and widely spreads throughout software projects. Considering test smells have negative impacts on the comprehension and maintenance of test code and even make code-under-test more defect-prone, it thus has great importance in mining, detecting, and refactoring them. Since Deursen et al. introduced the definition of “test smell”, several studies worked on discovering new test smells from test specifications and software practitioners’ experience. Indeed, many bad testing practices are “observed” by software developers during creating test scripts rather than through academic research and are widely discussed in the software engineering community (e.g., Stack Overflow) [ 70 , 94 ]. However, no prior studies explored new bad testing practices from software practitioners’ discussions, formally defined them as new test smell types, and analyzed their characteristics, which plays a bad role for developers in knowing these bad practices and avoiding using them during test code development. Therefore, we pick up those challenges and act by working on systematic methods to explore new test smell types from one of the most mainstream developers’ Q&amp;A platforms, i.e., Stack Overflow. We further investigate the harmfulness of new test smells and analyze possible solutions for eliminating them. We find that some test smells make it hard for developers to fix failed test cases and trace their failing reasons. To exacerbate matters, we have identified two types of test smells that pose a risk to the accuracy of test cases. Next, we develop a detector to detect test smells from software. The detector is composed of six detection methods for different smell types. These detection methods are both wrapped with a set of syntactic rules based on the code patterns extracted from different test smells and developers’ code styles. We manually construct a test smell dataset from seven popular Java projects and evaluate the effectiveness of our detector on it. The experimental results show that our detector achieves high performance in precision, recall, and F1 score. Then, we utilize our detector to detect smells from 919 real-world Java projects to explore whether the six test smells are prevalent in practice. We observe that these test smells are widely spread in 722 out of 919 Java projects, which demonstrates that they are prevalent in real-world projects. Finally, to validate the usefulness of test smells in practice, we submit 56 issue reports to 53 real-world projects with different smells. Our issue reports achieve 76.4% acceptance by conducting sentiment analysis on developers’ replies. These evaluations confirm the effectiveness of our detector and the prevalence and practicality of new test smell types on real-world projects.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/tosem24testsmell.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2023_TOSEM_The Lost World_ Characterizing and Detecting Undiscovered Test Smells_2d4e4e42cb2f.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2023_TOSEM_The Lost World_ Characterizing and Detecting Undiscovered Test Smells_2d4e4e42cb2f.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": false,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "2c16ca45524d",
      "label": "J11",
      "title": "Federated Learning for Software Engineering: A Case Study of Code Clone Detection and Defect Prediction",
      "year": 2023,
      "venue": "IEEE Transactions on Software Engineering (TSE)",
      "authors": "[J11] Yanming Yang, Xing Hu*, Zhipeng Gao, Jinfu Chen, Chao Ni, Xin Xia, David Lo",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4390533936",
        "doi": "https://doi.org/10.1109/tse.2023.3347898",
        "display_name": "Federated Learning for Software Engineering: A Case Study of Code Clone Detection and Defect Prediction",
        "publication_year": 2024,
        "is_oa": false,
        "abstract": "In various research domains, artificial intelligence (AI) has gained significant prominence, leading to the development of numerous learning-based models in research laboratories, which are evaluated using benchmark datasets. While the models proposed in previous studies may demonstrate satisfactory performance on benchmark datasets, translating academic findings into practical applications for industry practitioners presents challenges. This can entail either the direct adoption of trained academic models into industrial applications, leading to a performance decrease, or retraining models with industrial data, a task often hindered by insufficient data instances or skewed data distributions. Real-world industrial data is typically significantly more intricate than benchmark datasets, frequently exhibiting data-skewing issues, such as label distribution skews and quantity skews. Furthermore, accessing industrial data, particularly source code, can prove challenging for Software Engineering (SE) researchers due to privacy policies. This limitation hinders SE researchers' ability to gain insights into industry developers' concerns and subsequently enhance their proposed models. To bridge the divide between academic models and industrial applications, we introduce a federated learning (FL)-based framework called <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Almity</small> . Our aim is to simplify the process of implementing research findings into practical use for both SE researchers and industry developers. <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Almity</small> enhances model performance on sensitive skewed data distributions while ensuring data privacy and security. It introduces an innovative aggregation strategy that takes into account three key attributes: data scale, data balance, and minority class learnability. This strategy is employed to refine model parameters, thereby enhancing model performance on sensitive skewed datasets. In our evaluation, we employ two well-established SE tasks, i.e., code clone detection and defect prediction, as evaluation tasks. We compare the performance of <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Almity</small> on both machine learning (ML) and deep learning (DL) models against two mainstream training methods, specifically the Centralized Training Method (CTM) and Vanilla Federated Learning (VFL), to validate the effectiveness and generalizability of <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Almity</small> . Our experimental results demonstrate that our framework is not only feasible but also practical in real-world scenarios. <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Almity</small> consistently enhances the performance of learning-based models, outperforming baseline training methods across all types of data distributions.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/tse24federated.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2023_TSE_Federated Learning for Software Engineering_ A Case Study of Code Clone Detectio_2c16ca45524d.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2023_TSE_Federated Learning for Software Engineering_ A Case Study of Code Clone Detectio_2c16ca45524d.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "40f083ff6be4",
      "label": "J7",
      "title": "https://lj2lijia.github.io/index.html",
      "year": 2023,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J7] <a href=",
      "is_award": false,
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "is_oa": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/CodeEditor_TOSEM.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2023_TOSEM_https_lj2lijia.github.io_index.html_40f083ff6be4.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2023_TOSEM_https_lj2lijia.github.io_index.html_40f083ff6be4.txt"
      },
      "signals": {
        "has_abstract_heading": false,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": false
      }
    },
    {
      "id": "e6da55a39956",
      "label": "J8",
      "title": "https://jw-zhang-zju.github.io/",
      "year": 2023,
      "venue": "IEEE Transactions on Software Engineering (TSE)",
      "authors": "[J8] <a href=",
      "is_award": false,
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "is_oa": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/TSE-EPVD.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2023_TSE_https_jw-zhang-zju.github.io__e6da55a39956.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2023_TSE_https_jw-zhang-zju.github.io__e6da55a39956.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": true,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "c32c7fabf00f",
      "label": "J9",
      "title": "https://lj2lijia.github.io/index.html",
      "year": 2023,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J9] <a href=",
      "is_award": false,
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "is_oa": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "d2e230ec6bed",
      "label": "C17",
      "title": "https://qi-zhan.github.io/",
      "year": 2024,
      "venue": "46th International Conference on Software Engineering (ICSE 2024)",
      "authors": "[C17] <a href=",
      "is_award": false,
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "is_oa": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/icse24zq.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2024_ICSE_https_qi-zhan.github.io__d2e230ec6bed.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2024_ICSE_https_qi-zhan.github.io__d2e230ec6bed.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": true,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "1429fd571f50",
      "label": "C18",
      "title": "https://chen-zirui.github.io/",
      "year": 2024,
      "venue": "46th International Conference on Software Engineering (ICSE 2024)",
      "authors": "[C18] <a href=",
      "is_award": false,
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "is_oa": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "42612050d697",
      "label": "C19",
      "title": "MUT: Human-in-the-Loop Unit Test Migration",
      "year": 2024,
      "venue": "46th International Conference on Software Engineering (ICSE 2024)",
      "authors": "[C19] Yi Gao, Xing Hu*, Tongtong Xu, Xin Xia, David Lo, Xiaohu Yang",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4394768861",
        "doi": "https://doi.org/10.1145/3597503.3639124",
        "display_name": "MUT: Human-in-the-Loop Unit Test Migration",
        "publication_year": 2024,
        "is_oa": false,
        "abstract": "Test migration, which enables the reuse of test cases crafted with knowledge and creativity by testers across various platforms and programming languages, has exhibited effectiveness in mobile app testing. However, unit test migration at the source code level has not garnered adequate attention and exploration. In this paper, we propose a novel cross-language and cross-platform test migration methodology, named MUT, which consists of four modules: code mapping, test case filtering, test case translation, and test case adaptation. MUT initially calculates code mappings to establish associations between source and target projects, and identifies suitable unit tests for migration from the source project. Then, MUT's code translation component generates a syntax tree by parsing the code to be migrated and progressively converts each node in the tree, ultima tely generating the target tests, which are compiled and executed in the target project. Moreover, we develop a web tool to assist developers in test migration. The effectiveness of our approach has been validated on five prevalent functional domain projects within the open-source community. We migrate a total of 550 unit tests and submitted pull requests to augment test code in the target projects on GitHub. By the time of this paper submission, 253 of these tests have already been merged into the projects (including 197 unit tests in the Luliyucoordinate-LeetCode project and 56 unit tests in the Rangerlee-HtmlParser project). Through running these tests, we identify 5 bugs, and 2 functional defects, and submitted corresponding issues to the project. The evaluation substantiates that MUT's test migration is both viable and beneficial across programming languages and different projects.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/icse24mut.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2024_ICSE_MUT_ Human-in-the-Loop Unit Test Migration_42612050d697.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2024_ICSE_MUT_ Human-in-the-Loop Unit Test Migration_42612050d697.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "a3ce1edc6ca2",
      "label": "C20",
      "title": "https://chen-junkai.github.io/",
      "year": 2024,
      "venue": "46th International Conference on Software Engineering (ICSE 2024)",
      "authors": "[C20] <a href=",
      "is_award": true,
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "is_oa": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/ICSE2024_Code_Suggestion.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2024_ICSE_https_chen-junkai.github.io__a3ce1edc6ca2.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2024_ICSE_https_chen-junkai.github.io__a3ce1edc6ca2.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "b89276e97c95",
      "label": "C21",
      "title": "Streamlining Java Programming: Uncovering Well-Formed Idioms with IdioMine",
      "year": 2024,
      "venue": "46th International Conference on Software Engineering (ICSE 2024)",
      "authors": "[C21] Yanming Yang, Xing Hu*, Xin Xia, David Lo, Xiaohu Yang",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4394744395",
        "doi": "https://doi.org/10.1145/3597503.3639135",
        "display_name": "Streamlining Java Programming: Uncovering Well-Formed Idioms with IdioMine",
        "publication_year": 2024,
        "is_oa": false,
        "abstract": "Code idioms are commonly used patterns, techniques, or practices that aid in solving particular problems or specific tasks across multiple software projects. They can improve code quality, performance, and maintainability, and also promote program standardization and reuse across projects. However, identifying code idioms is significantly challenging, as existing studies have still suffered from three main limitations. First, it is difficult to recognize idioms that span non-contiguous code lines. Second, identifying idioms with intricate data flow and code structures can be challenging. Moreover, they only extract dataset-specific idioms, so common idioms or well-established code/design patterns that are rarely found in datasets cannot be identified.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/icse24idioms.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2024_ICSE_Streamlining Java Programming_ Uncovering Well-Formed Idioms with IdioMine_b89276e97c95.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2024_ICSE_Streamlining Java Programming_ Uncovering Well-Formed Idioms with IdioMine_b89276e97c95.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "93c3e7938f2a",
      "label": "C22",
      "title": "Towards More Practical Automation of Vulnerability Assessment",
      "year": 2024,
      "venue": "46th International Conference on Software Engineering (ICSE 2024)",
      "authors": "[C22] Shengyi Pan, Lingfeng Bao, Jiayuan Zhou, Xing Hu, Xin Xia, Shanping Li",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4394746930",
        "doi": "https://doi.org/10.1145/3597503.3639110",
        "display_name": "Towards More Practical Automation of Vulnerability Assessment",
        "publication_year": 2024,
        "is_oa": false,
        "abstract": "It is increasingly suggested to identify emerging software vulnerabilities (SVs) through relevant development activities (e.g., issue reports) to allow early warnings to open source software (OSS) users. However, the support for the following assessment of the detected SVs has not yet been explored. SV assessment characterizes the detected SVs to prioritize limited remediation resources on the critical ones. To fill this gap, we aim to enable early vulnerability assessment based on SV-related issue reports (SIR). Besides, we observe the following concerns of the existing assessment techniques: 1) the assessment output lacks rationale and practical value; 2) the associations between Common Vulnerability Scoring System (CVSS) metrics have been ignored; 3) insufficient evaluation scenarios and metrics. We address these concerns to enhance the practicality of our proposed early vulnerability assessment approach (namely proEVA). Specifically, based on the observation of strong associations between CVSS metrics, we propose a prompt-based model to exploit such relations for CVSS metrics prediction. Moreover, we design a curriculum-learning (CL) schedule to guide the model better learn such hidden associations during training. Aside from the standard classification metrics adopted in existing works, we propose two severity-aware metrics to provide a more comprehensive evaluation regarding the prioritization of the high-severe SVs. Experimental results show that proEVA significantly outperforms the baselines in both types of metrics. We further discuss the transferability of the prediction model regarding the upgrade of the assessment system, an important yet overlooked evaluation scenario in existing works. The results verify that proEVA is more efficient and flexible in migrating to different assessment systems.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "aab29c4cae52",
      "label": "C24",
      "title": "PPT4J: Patch Presence Test for Java Binaries",
      "year": 2024,
      "venue": "46th International Conference on Software Engineering (ICSE 2024)",
      "authors": "[C24] Zhiyuan Pan, Xing Hu*, Xin Xia, Xian Zhan, David Lo, Xiaohu Yang",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4394769495",
        "doi": "https://doi.org/10.1145/3597503.3639231",
        "display_name": "PPT4J: Patch Presence Test for Java Binaries",
        "publication_year": 2024,
        "is_oa": false,
        "abstract": "The number of vulnerabilities reported in open source software has increased substantially in recent years. Security patches provide the necessary measures to protect software from attacks and vulnerabilities. In practice, it is difficult to identify whether patches have been integrated into software, especially if we only have binary files. Therefore, the ability to test whether a patch is applied to the target binary, a.k.a. patch presence test, is crucial for practitioners. However, it is challenging to obtain accurate semantic information from patches, which could lead to incorrect results.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/icse24ppt4j.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2024_ICSE_PPT4J_ Patch Presence Test for Java Binaries_aab29c4cae52.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2024_ICSE_PPT4J_ Patch Presence Test for Java Binaries_aab29c4cae52.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": true,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "1d4e6af85457",
      "label": "C26",
      "title": "Learning in the Wild: Towards Leveraging Unlabeled Data for Effectively Tuning Pre-trained Code Models",
      "year": 2024,
      "venue": "46th International Conference on Software Engineering (ICSE 2024)",
      "authors": "[C26] Shuzheng Gao, Wenxin Mao, Cuiyun Gao, Li Li, Xing Hu, Xin Xia, Michael Lyu",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4394769152",
        "doi": "https://doi.org/10.1145/3597503.3639216",
        "display_name": "Learning in the Wild: Towards Leveraging Unlabeled Data for Effectively Tuning Pre-trained Code Models",
        "publication_year": 2024,
        "is_oa": false,
        "abstract": "Pre-trained code models have recently achieved substantial improvements in many code intelligence tasks. These models are first pre-trained on large-scale unlabeled datasets in a task-agnostic manner using self-supervised learning, and then fine-tuned on labeled datasets in downstream tasks. However, the labeled datasets are usually limited in size (i.e., human intensive efforts), which may hinder the performance of pre-trained code models in specific tasks. To mitigate this, one possible solution is to leverage the large-scale unlabeled data in the tuning stage by pseudo-labeling, i.e., generating pseudo labels for unlabeled data and further training the pre-trained code models with the pseudo-labeled data. However, directly employing the pseudo-labeled data can bring a large amount of noise, i.e., incorrect labels, leading to suboptimal performance. How to effectively leverage the noisy pseudo-labeled data is a challenging yet under-explored problem.",
        "error": null
      },
      "pdf": {
        "url": "http://arxiv.org/pdf/2401.01060v1.pdf",
        "source": "arxiv",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2024_ICSE_Learning in the Wild_ Towards Leveraging Unlabeled Data for Effectively Tuning P_1d4e6af85457.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2024_ICSE_Learning in the Wild_ Towards Leveraging Unlabeled Data for Effectively Tuning P_1d4e6af85457.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "7fd173a618ce",
      "label": "C27",
      "title": "MiniMon: Minimizing Android Applications with Intelligent Monitoring-Based Debloating",
      "year": 2024,
      "venue": "46th International Conference on Software Engineering (ICSE 2024)",
      "authors": "[C27] Jiakun Liu, Zicheng Zhang, Xing Hu, Ferdian Thung, Shahar Maoz, Debin Gao, Eran Toch, Zhipeng Zhao, David Lo",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4394769449",
        "doi": "https://doi.org/10.1145/3597503.3639113",
        "display_name": "MiniMon: Minimizing Android Applications with Intelligent Monitoring-Based Debloating",
        "publication_year": 2024,
        "is_oa": true,
        "abstract": "The size of Android applications is getting larger to fulfill the requirements of various users. However, not all the features of the applications are needed and desired by a specific user. The unnecessary and non-desired features can increase the attack surface and consume system resources such as storage and memory. To address this issue, we propose a framework, MiniMon, to debloat unnecessary features from an Android app based on the logs of specific users' interactions with the app.",
        "error": null
      },
      "pdf": {
        "url": "https://dl.acm.org/doi/pdf/10.1145/3597503.3639113",
        "source": "openalex",
        "downloaded": false,
        "error": "HTTPError: HTTP Error 403: Forbidden",
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "83a185d0977c",
      "label": "C28",
      "title": "Automating Zero-Shot Patch Porting for Hard Forks",
      "year": 2024,
      "venue": "the 33rd International Symposium on Software Testing and Analysis (ISSTA 2024)",
      "authors": "[C28] Shengyi Pan, You Wang, Zhongxin Liu, Xing Hu, Xin Xia, Shanping Li",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4402457491",
        "doi": "https://doi.org/10.1145/3650212.3652134",
        "display_name": "Automating Zero-Shot Patch Porting for Hard Forks",
        "publication_year": 2024,
        "is_oa": false,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "http://arxiv.org/pdf/2404.17964v1.pdf",
        "source": "arxiv",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2024_ISSTA_Automating Zero-Shot Patch Porting for Hard Forks_83a185d0977c.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2024_ISSTA_Automating Zero-Shot Patch Porting for Hard Forks_83a185d0977c.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "d4fec024fbdc",
      "label": "C29",
      "title": "Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning and In-Context Learning",
      "year": 2024,
      "venue": "International Conference on the Foundations of Software Engineering (FSE 2024)",
      "authors": "[C29] Yubo Mai, Zhipeng Gao, Xing Hu, Lingfeng Bao, Yu Liu, Jianling Sun",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4396786234",
        "doi": "https://doi.org/10.1145/3660811",
        "display_name": "Are Human Rules Necessary? Generating Reusable APIs with CoT Reasoning and In-Context Learning",
        "publication_year": 2024,
        "is_oa": true,
        "abstract": "Nowadays, more and more developers resort to Stack Overflow for solutions (e.g., code snippets) when they encounter technical problems. Although domain experts provide huge amounts of valuable solutions in Stack Overflow, these code snippets are often difficult to reuse directly. Developers have to digest the information within relevant posts and make necessary modifications, and the whole solution-seeking process can be time-consuming and tedious. To facilitate the reuse of Stack Overflow code snippets, Terragni et al. first explored transforming a code snippet in Stack Overflow into a well-formed method API ( A pplication P rogram I nterface) by using a rule-based approach, named APIzator. The reported performance of their approach is promising, however, after our in-depth analysis of their experiment results, we find that (1) 92.5% of APIs generated by APIzator are pointless and thus are difficult to use in practice. This is because the method name generated by APIzator (extracting verb + object ) can rarely represent the method’s functionality, which can hardly be claimed as meaningful/reusable APIs. (2) The authors manually summarized a number of rules to identify parameter variables and return statements for Java methods. These hand-crafted rules are extremely complex and sophisticated, and the manual rule design process is labor-intensive and error-prone. Moreover, since these rules are designed for Java, they can hardly be extended to other programming languages. Inspired by the great potential of Large Language Models (LLMs) for solving complex coding tasks, in this paper, we propose a novel approach, named Code 2API, to automatically perform APIzation for Stack Overflow code snippets. Code 2API does not require additional model training or any manual crafting rules and can be easily deployed on personal computers without relying on other external tools. Specifically, Code 2API guides the LLMs through well-designed prompts to generate well-formed APIs for given code snippets. To elicit knowledge and logical reasoning from LLMs, we used c hain- o f- t hought (CoT) reasoning and few-shot in-context learning, which can help the LLMs fully understand the APIzation task and solve it step by step in a manner similar to a developer. Our evaluations show that Code 2API achieves a remarkable accuracy in identifying method parameters (65%) and return statements (66%) equivalent to human-generated ones, surpassing the current state-of-the-art approach, APIzator, by 15.0% and 16.5% respectively. Moreover, compared with APIzator, our user study demonstrates that Code 2API exhibits superior performance in generating meaningful method names, even surpassing the human-level performance, and developers are more willing to use APIs generated by our approach, highlighting the applicability of our tool in practice. Finally, we successfully extend our framework to the Python dataset, achieving a comparable performance with Java, which verifies the generalizability of our tool.",
        "error": null
      },
      "pdf": {
        "url": "https://arxiv.org/pdf/2405.03509",
        "source": "openalex",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2024_FSE_Are Human Rules Necessary_ Generating Reusable APIs with CoT Reasoning and In-Co_d4fec024fbdc.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2024_FSE_Are Human Rules Necessary_ Generating Reusable APIs with CoT Reasoning and In-Co_d4fec024fbdc.txt"
      },
      "signals": {
        "has_abstract_heading": false,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": false
      }
    },
    {
      "id": "5a021ace3cea",
      "label": "C30",
      "title": "Easy over Hard: A Simple Baseline for Test Failures Causes Prediction",
      "year": 2024,
      "venue": "International Conference on the Foundations of Software Engineering (FSE 2024, Industry Track)",
      "authors": "[C30] Zhipeng Gao, Zhipeng Xue, Xing Hu*, Weiyi Shang, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4396786304",
        "doi": "https://doi.org/10.1145/3663529.3663850",
        "display_name": "Easy over Hard: A Simple Baseline for Test Failures Causes Prediction",
        "publication_year": 2024,
        "is_oa": true,
        "abstract": "The test failure causes analysis is critical since it determines the\\nsubsequent way of handling different types of bugs, which is the prerequisite\\nto get the bugs properly analyzed and fixed. After a test case fails, software\\ntesters have to inspect the test execution logs line by line to identify its\\nroot cause. However, manual root cause determination is often tedious and\\ntime-consuming, which can cost 30-40% of the time needed to fix a problem.\\nTherefore, there is a need for automatically predicting the test failure causes\\nto lighten the burden of software testers. In this paper, we present a simple\\nbut hard-to-beat approach, named NCChecker to automatically identify the\\nfailure causes for failed test logs. Our approach can help developers\\nefficiently identify the test failure causes, and flag the most probable log\\nlines of indicating the root causes for investigation. Our approach has three\\nmain stages: log abstraction, lookup table construction, and failure causes\\nprediction. We first perform log abstraction to parse the unstructured log\\nmessages into structured log events. NCChecker then automatically maintains and\\nupdates a lookup table via employing our heuristic rules, which record the\\nmatching score between different log events and test failure causes. When it\\ncomes to the failure cause prediction stage, for a newly generated failed test\\nlog, NCChecker can easily infer its failed reason by checking out the\\nassociated log events' scores from the lookup table. We have developed a\\nprototype and evaluated our tool on a real-world industrial dataset with more\\nthan 10K test logs. The extensive experiments show the promising performance of\\nour model over a set of benchmarks. Moreover, our approach is highly efficient\\nand memory-saving, and can successfully handle the data imbalance problem.\\n",
        "error": null
      },
      "pdf": {
        "url": "https://arxiv.org/pdf/2405.02922",
        "source": "openalex",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2024_FSE_Easy over Hard_ A Simple Baseline for Test Failures Causes Prediction_5a021ace3cea.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2024_FSE_Easy over Hard_ A Simple Baseline for Test Failures Causes Prediction_5a021ace3cea.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": true,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "e6d2404add16",
      "label": "C31",
      "title": "Practitioners’ Expectations on Automated Test Generations",
      "year": 2024,
      "venue": "the 33rd International Symposium on Software Testing and Analysis (ISSTA 2024)",
      "authors": "[C31] Xiao Yu, Lei Liu, Xing Hu*, Jacky Keung, Xin Xia, David Lo",
      "is_award": false,
      "openalex": {
        "score": 1.0209909909909909,
        "id": "https://openalex.org/W4402457626",
        "doi": "https://doi.org/10.1145/3650212.3680386",
        "display_name": "Practitioners’ Expectations on Automated Test Generation",
        "publication_year": 2024,
        "is_oa": false,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "b605898987df",
      "label": "C32",
      "title": "SelfPiCo: Self-Guided Partial Code Execution with LLMs",
      "year": 2024,
      "venue": "the 33rd International Symposium on Software Testing and Analysis (ISSTA 2024)",
      "authors": "[C32] Zhipeng Xue, Zhipeng Gao, Shaohua Wang, Xing Hu, Xin Xia, Shanping Li",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4402442680",
        "doi": "https://doi.org/10.1145/3650212.3680368",
        "display_name": "SelfPiCo: Self-Guided Partial Code Execution with LLMs",
        "publication_year": 2024,
        "is_oa": false,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/issta24selfpi.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2024_ISSTA_SelfPiCo_ Self-Guided Partial Code Execution with LLMs_b605898987df.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2024_ISSTA_SelfPiCo_ Self-Guided Partial Code Execution with LLMs_b605898987df.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "c0afd1709dfe",
      "label": "C33",
      "title": "Silent Taint-Style Vulnerability Fixes Identification",
      "year": 2024,
      "venue": "the 33rd International Symposium on Software Testing and Analysis (ISSTA 2024)",
      "authors": "[C33] Zhongzhen Wen, Jiayuan Zhou, Minxue Pan, Shaohua Wang, Xing Hu, Tongtong Xu, Tian Zhang, Xuandong Li",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4402457456",
        "doi": "https://doi.org/10.1145/3650212.3652139",
        "display_name": "Silent Taint-Style Vulnerability Fixes Identification",
        "publication_year": 2024,
        "is_oa": false,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "b4cf493fa8ae",
      "label": "C35",
      "title": "https://qi-zhan.github.io/",
      "year": 2024,
      "venue": "39th IEEE/ACM International Conference on Automated Software Engineering (ASE 2024)",
      "authors": "[C35] <a href=",
      "is_award": false,
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "is_oa": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/ase24zq.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2024_ASE_https_qi-zhan.github.io__b4cf493fa8ae.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2024_ASE_https_qi-zhan.github.io__b4cf493fa8ae.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": true,
        "has_rq": false,
        "has_threats_to_validity": true,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "49a8c6b5c70b",
      "label": "C36",
      "title": "What Makes a High-Quality Training Dataset for Large Language Models: A Practitioners’ Perspective",
      "year": 2024,
      "venue": "39th IEEE/ACM International Conference on Automated Software Engineering (ASE 2024)",
      "authors": "[C36] Xiao Yu, Zexian Zhang, Feifei Niu, Xing Hu, Xin Xia, John Grundy",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4403536522",
        "doi": "https://doi.org/10.1145/3691620.3695061",
        "display_name": "What Makes a High-Quality Training Dataset for Large Language Models: A Practitioners' Perspective",
        "publication_year": 2024,
        "is_oa": false,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "f47f3e079346",
      "label": "J12",
      "title": "Automating TODO-missed Methods Detection and Patching",
      "year": 2024,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J12] Zhipeng Gao, Yanqi Su, Xing Hu*, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4404112638",
        "doi": "https://doi.org/10.1145/3700793",
        "display_name": "Automating TODO-missed Methods Detection and Patching",
        "publication_year": 2024,
        "is_oa": true,
        "abstract": "TODO comments are widely used by developers to remind themselves or others about incomplete tasks. In other words, TODO comments are usually associated with temporary or suboptimal solutions. In practice, all the equivalent suboptimal implementations should be updated (e.g., adding TODOs) simultaneously. However, due to various reasons (e.g., time constraints or carelessness), developers may forget or even are unaware of adding TODO comments to all necessary places, which results in the TODO-missed methods . These “hidden” suboptimal implementations in TODO-missed methods may hurt the software quality and maintainability in the long-term. Therefore, in this article, we propose the novel task of TODO-missed methods detection and patching and develop a novel model, namely T O D O-comment Patcher ( TDPatcher ), to automatically patch TODO comments to the TODO-missed methods in software projects. Our model has two main stages: offline learning and online inference. During the offline learning stage, TDPatcher employs the GraphCodeBERT and contrastive learning for encoding the TODO comment (natural language) and its suboptimal implementation (code fragment) into vector representations. For the online inference stage, we can identify the TODO-missed methods and further determine their patching position by leveraging the offline trained model. We built our dataset by collecting TODO-introduced methods from the top-10,000 Python GitHub repositories and evaluated TDPatcher on them. Extensive experimental results show the promising performance of our model over a set of benchmarks. We further conduct an in-the-wild evaluation that successfully detects 26 TODO-missed methods from 50 GitHub repositories.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/tosem24todo.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2024_TOSEM_Automating TODO-missed Methods Detection and Patching_f47f3e079346.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2024_TOSEM_Automating TODO-missed Methods Detection and Patching_f47f3e079346.txt"
      },
      "signals": {
        "has_abstract_heading": false,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": false
      }
    },
    {
      "id": "71ad45ac6ee3",
      "label": "J14",
      "title": "Automating Comment Generation for Smart Contract from Bytecode",
      "year": 2024,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J14] Jianhang Xiang, Zhipeng Gao, Lingfeng Bao, Xing Hu, Jiayuan Chen, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4403185844",
        "doi": "https://doi.org/10.1145/3699597",
        "display_name": "Automating Comment Generation for Smart Contract from Bytecode",
        "publication_year": 2024,
        "is_oa": true,
        "abstract": "Recently, smart contracts have played a vital role in automatic financial and business transactions. To help end users without programming background to better understand the logic of smart contracts, previous studies have proposed models for automatically translating smart contract source code into their corresponding code summaries. However, in practice, only 13% of smart contracts deployed on the Ethereum blockchain are associated with source code. The practical usage of these existing tools is significantly restricted. Considering that bytecode is always necessary when deploying smart contracts, in this article, we first introduce the task of automatically generating smart contract code summaries from bytecode. We propose a novel approach, named Smart Contract Bytecode Translator ( SmartBT ) for automatically translating smart contract bytecode into fine-grained natural language description directly. Two key challenges are posed for this task: structural code logic hidden in bytecode and the huge semantic gap between bytecode and natural language descriptions. To address the first challenge, we transform bytecode into Control-Flow Graph (CFG) to learn code structural and logic details. Regarding the second challenge, we introduce an information retrieval component to fetch similar comments for filling the semantic gap. Then, the structural input and semantic input are used to build an attentional sequence-to-sequence neural network model. The copy mechanism is employed to copy rare words directly from similar comments, and the coverage mechanism is employed to eliminate repetitive outputs. The automatic evaluation results show that SmartBT outperforms a set of baselines by a large margin, and the human evaluation results show the effectiveness and potential of SmartBT in producing meaningful and accurate comments for smart contract code from bytecode directly.",
        "error": null
      },
      "pdf": {
        "url": "https://dl.acm.org/doi/pdf/10.1145/3699597",
        "source": "openalex",
        "downloaded": false,
        "error": "HTTPError: HTTP Error 403: Forbidden",
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "f3d89b4f5256",
      "label": "J15",
      "title": "Vulnerability Detection via Multiple-Graph-Based Code Representation",
      "year": 2024,
      "venue": "IEEE Transactions on Software Engineering (TSE)",
      "authors": "[J15] Fangcheng Qiu, Zhongxin Liu, Xing Hu, Xin Xia, Gang Chen, Xinyu Wang",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4400579932",
        "doi": "https://doi.org/10.1109/tse.2024.3427815",
        "display_name": "Vulnerability Detection via Multiple-Graph-Based Code Representation",
        "publication_year": 2024,
        "is_oa": false,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "e036b5c3b3d2",
      "label": "J16",
      "title": "Fight Fire with Fire: How Much Can We Trust ChatGPT on Source Code-Related Tasks?",
      "year": 2024,
      "venue": "IEEE Transactions on Software Engineering (TSE)",
      "authors": "[J16] Xiao Yu, Lei Liu, Xing Hu*, Jacky Wai Keung, Jin Liu, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4404057519",
        "doi": "https://doi.org/10.1109/tse.2024.3492204",
        "display_name": "Fight Fire With Fire: How Much Can We Trust ChatGPT on Source Code-Related Tasks?",
        "publication_year": 2024,
        "is_oa": false,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/tse24fire.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2024_TSE_Fight Fire with Fire_ How Much Can We Trust ChatGPT on Source Code-Related Tasks_e036b5c3b3d2.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2024_TSE_Fight Fire with Fire_ How Much Can We Trust ChatGPT on Source Code-Related Tasks_e036b5c3b3d2.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": true,
        "has_rq": true,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "3ab4ece1d009",
      "label": "J17",
      "title": "Just-in-time todo-missed commits detection",
      "year": 2024,
      "venue": "IEEE Transactions on Software Engineering (TSE)",
      "authors": "[J17] Haoye Wang, Zhipeng Gao, Xing Hu, David Lo, John Grundy, Xinyu Wang",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4398765100",
        "doi": "https://doi.org/10.1109/tse.2024.3405005",
        "display_name": "Just-In-Time TODO-Missed Commits Detection",
        "publication_year": 2024,
        "is_oa": false,
        "abstract": "TODO comments play an important role in helping developers to manage their tasks and communicate with other team members. TODO comments are often introduced by developers as a type of technical debt, such as a reminder to add/remove features or a request to optimize the code implementations. These can all be considered as notifications for developers to revisit regarding the current suboptimal solutions. TODO comments often bring short-term benefits – higher productivity or shorter development cost – and indicate attention needs to be paid for the long-term software quality. Unfortunately, due to their lack of knowledge or experience and/or the time constraints, developers sometimes may forget or even not be aware of suboptimal implementations. The loss of the TODO comments for these suboptimal solutions may hurt the software quality and reliability in the long-term. Therefore it is beneficial to remind the developers of the suboptimal solutions whenever they change the code. In this work, we refer this problem to the task of detecting <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TODO-missed commits</i> , and we propose a novel approach named TDR <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">eminder</small> ( <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">T</b> O <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">D</b> O comment <bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Reminder</b> ) to address the task. With the help of TDR <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">eminder</small> , developers can identify possible missing TODO commits just-in-time when submitting a commit. Our approach has two phases: offline training and online inference. We first embed code change and commit message into contextual vector representations using two neural encoders respectively. The association between these representations is learned by our model automatically.In the online inference phase, TDR <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">eminder</small> leverages the trained model to compute the likelihood of a commit being a <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TODO-missed commit</i> . We evaluate TDR <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">eminder</small> on datasets crawled from 10k popular Python and Java repositories in GitHub respectively. Our experimental results show that TDR <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">eminder</small> outperforms a set of benchmarks by a large margin in <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TODO-missed commits</i> detection. Moreover, to better help developers use TDR <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">eminder</small> in practice, we have incorporated Large Language Models (LLMs) with our approach to provide explainable recommendations. The user study shows that our tool can effectively inform developers not only \"when\" to add TODOs, but also \"where\" and \"what\" TODOs should be added, verifying the value of our tool in practical application.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "81cd38b2a801",
      "label": "C37",
      "title": "https://chen-junkai.github.io/",
      "year": 2025,
      "venue": "47th International Conference on Software Engineering (ICSE 2025)",
      "authors": "[C37] <a href=",
      "is_award": false,
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "is_oa": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/ICSE2025CodeReasoning.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_ICSE_https_chen-junkai.github.io__81cd38b2a801.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_ICSE_https_chen-junkai.github.io__81cd38b2a801.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "715dc1f4a3db",
      "label": "C38",
      "title": "Similar but Patched Code Considered Harmful – The Impact of Similar but Patched Code on Recurring Vulnerability Detection and How to Remove Them",
      "year": 2025,
      "venue": "47th International Conference on Software Engineering (ICSE 2025)",
      "authors": "[C38] Zixuan Tan, Jiayuan Zhou, Xing Hu*, Shengyi Pan, Kui Liu, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4411552123",
        "doi": "https://doi.org/10.1109/icse55347.2025.00110",
        "display_name": "Similar but Patched Code Considered Harmful: The Impact of Similar but Patched Code on Recurring Vulnerability Detection and How to Remove Them",
        "publication_year": 2025,
        "is_oa": false,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/icse25Vul.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_ICSE_Similar but Patched Code Considered Harmful – The Impact of Similar but Patched_715dc1f4a3db.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_ICSE_Similar but Patched Code Considered Harmful – The Impact of Similar but Patched_715dc1f4a3db.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": true,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "c536f1b4aecd",
      "label": "C39",
      "title": "Towards Better Answers: Automated Stack Overflow Post Updating",
      "year": 2025,
      "venue": "47th International Conference on Software Engineering (ICSE 2025)",
      "authors": "[C39] Yubo Mai, Zhipeng Gao, Haoye Wang, Tingting Bi, Xing Hu, Xin Xia, Jianling Sun",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4402509981",
        "doi": "https://doi.org/10.48550/arxiv.2408.09095",
        "display_name": "Towards Better Answers: Automated Stack Overflow Post Updating",
        "publication_year": 2024,
        "is_oa": true,
        "abstract": "Utilizing code snippets on Stack Overflow (SO) is a common practice among developers for problem-solving. Although SO code snippets serve as valuable resources, it is important to acknowledge their imperfections, reusing problematic code snippets can lead to the introduction of suboptimal or buggy code into software projects. SO comments often point out weaknesses of a post and provide valuable insights to improve the quality of answers, while SO comments are usually missed and/or ignored, leaving these problematic code snippets untouched. In this work, we first investigate the task of automatic SO posts updating based on their associated comments. We introduce a novel framework, named Soup (Stack Overflow Updator for Post) for this task. Soup addresses two key tasks: Valid Comment-Edit Prediction (VCP) and Automatic Post Updating (APU). Extensive experimental results show the promising performance of our model over a set of benchmarks. Moreover, we also performed an in-the-wild evaluation on Stack Overflow, we submitted 50 edits generated by our approach to Stack Overflow posts and 21 of them have been verified and accepted by SO maintainers, further proving the practical value of Soup.",
        "error": null
      },
      "pdf": {
        "url": "https://arxiv.org/pdf/2408.09095",
        "source": "openalex",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_ICSE_Towards Better Answers_ Automated Stack Overflow Post Updating_c536f1b4aecd.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_ICSE_Towards Better Answers_ Automated Stack Overflow Post Updating_c536f1b4aecd.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": true,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "03df58defd42",
      "label": "C41",
      "title": "Automated Unit Test Refactoring",
      "year": 2025,
      "venue": "International Conference on the Foundations of Software Engineering (FSE 2025)",
      "authors": "[C41] Yi Gao, Xing Hu*, Xiaohu Yang, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4411449959",
        "doi": "https://doi.org/10.1145/3715750",
        "display_name": "Automated Unit Test Refactoring",
        "publication_year": 2025,
        "is_oa": true,
        "abstract": "Test smells arise from poor design practices and insufficient domain knowledge, which can lower the quality of test code and make it harder to maintain and update. Manually refactoring of test smells is time-consuming and error-prone, highlighting the necessity for automated approaches. Current rule-based refactoring methods often struggle in scenarios not covered by predefined rules and lack the flexibility needed to handle diverse cases effectively. In this paper, we propose a novel approach called UTRefactor, a context-enhanced, LLM-based framework for automatic test refactoring in Java projects. UTRefactor extracts relevant context from test code and leverages an external knowledge base that includes test smell definitions, descriptions, and DSL-based refactoring rules. By simulating the manual refactoring process through a chain-of-thought approach, UTRefactor guides the LLM to eliminate test smells in a step-by-step process, ensuring both accuracy and consistency throughout the refactoring. Additionally, we implement a checkpoint mechanism to facilitate comprehensive refactoring, particularly when multiple smells are present. We evaluate UTRefactor on 879 tests from six open-source Java projects, reducing the number of test smells from 2,375 to 265, achieving an 89% reduction. UTRefactor outperforms direct LLM-based refactoring methods by 61.82% in smell elimination and significantly surpasses the performance of a rule-based test smell refactoring tool. Our results demonstrate the effectiveness of UTRefactor in enhancing test code quality while minimizing manual involvement.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/FSE25TestRefactoring.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_FSE_Automated Unit Test Refactoring_03df58defd42.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_FSE_Automated Unit Test Refactoring_03df58defd42.txt"
      },
      "signals": {
        "has_abstract_heading": false,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": false
      }
    },
    {
      "id": "0d759c6cf0b6",
      "label": "C42",
      "title": "https://jw-zhang-zju.github.io/",
      "year": 2025,
      "venue": "International Conference on the Foundations of Software Engineering (FSE 2025)",
      "authors": "[C42] <a href=",
      "is_award": true,
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "is_oa": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/FSE25CleanTest.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_FSE_https_jw-zhang-zju.github.io__0d759c6cf0b6.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_FSE_https_jw-zhang-zju.github.io__0d759c6cf0b6.txt"
      },
      "signals": {
        "has_abstract_heading": false,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": false
      }
    },
    {
      "id": "009528dca325",
      "label": "C43",
      "title": "Code Change Intention, Development Artifact and History Vulnerability: Putting Them Together for Vulnerability Fix Detection by LLM",
      "year": 2025,
      "venue": "International Conference on the Foundations of Software Engineering (FSE 2025)",
      "authors": "[C43] Xu Yang, Wenhan Zhu, Michael Pacheco, Jiayuan Zhou, Shaowei Wang, Xing Hu, Kui Liu",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4411449706",
        "doi": "https://doi.org/10.1145/3715738",
        "display_name": "Code Change Intention, Development Artifact, and History Vulnerability: Putting Them Together for Vulnerability Fix Detection by LLM",
        "publication_year": 2025,
        "is_oa": true,
        "abstract": "Detecting vulnerability fix commits in open-source software is crucial for maintaining software security. To help OSS identify vulnerability fix commits, several automated approaches are developed. However, existing approaches like VulFixMiner and CoLeFunDa, focus solely on code changes, neglecting essential context from development artifacts. Tools like Vulcurator, which integrates issue reports, fail to leverage semantic associations between different development artifacts (e.g., pull requests and history vulnerability fixes). Moreover, they miss vulnerability fixes in tangled commits and lack explanations, limiting practical use. Hence to address those limitations, we propose LLM4VFD, a novel framework that leverages Large Language Models (LLMs) enhanced with Chain-of-Thought reasoning and In-Context Learning to improve the accuracy of vulnerability fix detection. LLM4VFD comprises three components: (1) Code Change Intention, which analyzes commit summaries, purposes, and implications using Chain-of-Thought reasoning; (2) Development Artifact, which incorporates context from related issue reports and pull requests; (3) Historical Vulnerability, which retrieves similar past vulnerability fixes to enrich context. More importantly, on top of the prediction, LLM4VFD also provides a detailed analysis and explanation to help security experts understand the rationale behind the decision. We evaluated LLM4VFD against state-of-the-art techniques, including Pre-trained Language Model-based approaches and vanilla LLMs, using a newly collected dataset, BigVulFixes. Experimental results demonstrate that LLM4VFD significantly outperforms the best-performed existing approach by 68.1%--145.4%. Furthermore, We conducted a user study with security experts, showing that the analysis generated by LLM4VFD improves the efficiency of vulnerability fix identification.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "ebc94c9a07e3",
      "label": "C44",
      "title": "HornBro: Homotopy-like Method for Automated Quantum Program Repair",
      "year": 2025,
      "venue": "International Conference on the Foundations of Software Engineering (FSE 2025)",
      "authors": "[C44] Siwei Tan, Liqiang Lu, Debin Xiang, Tianyao Chu, Congliang Lang, Jintao Chen, Xing Hu, Jianwei Yin",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4411449889",
        "doi": "https://doi.org/10.1145/3715751",
        "display_name": "HornBro: Homotopy-Like Method for Automated Quantum Program Repair",
        "publication_year": 2025,
        "is_oa": true,
        "abstract": "Quantum programs provide exponential speedups compared to classical programs in certain areas, but they also inevitably encounter logical faults. Automatically repairing quantum programs is much more challenging than repairing classical programs due to the non-replicability of data, the vast search space of program inputs, and the new programming paradigm. Existing works based on semantic-based or learning-based program repair techniques are fundamentally limited in repairing efficiency and effectiveness. In this work, we propose HornBro, an efficient framework for automated quantum program repair. The key insight of HornBro lies in the homotopy-like method, which iteratively switches between the classical and quantum parts. This approach allows the repair tasks to be efficiently offloaded to the most suitable platforms, enabling a progressive convergence toward the correct program. We start by designing an implication assertion pragma to enable rigorous specifications of quantum program behavior, which helps to generate a quantum test suite automatically. This suite leverages the orthonormal bases of quantum programs to accommodate different encoding schemes. Given a fixed number of test cases, it allows the maximum input coverage of potential counter-example candidates. Then, we develop a Clifford approximation method with an SMT-based search to transform the patch localization program into a symbolic reasoning problem. Finally, we offload the computationally intensive repair of gate parameters to quantum hardware by leveraging the differentiability of quantum gates. Experiments suggest that HornBro increases the repair success rate by more than 62.5% compared to the existing repair techniques, supporting more types of quantum bugs. It also achieves 35.7× speedup in the repair and 99.9% gate reduction of the patch.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "1585186dc0b7",
      "label": "C48",
      "title": "Understanding Practitioners’ Expectations on Clear Code Review Comments",
      "year": 2025,
      "venue": "the 34th International Symposium on Software Testing and Analysis (ISSTA 2025)",
      "authors": "[C48] Junkai Chen, Zhenhao Li, Qiheng Mao, Xing Hu*, Kui Liu, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4411523132",
        "doi": "https://doi.org/10.1145/3728931",
        "display_name": "Understanding Practitioners’ Expectations on Clear Code Review Comments",
        "publication_year": 2025,
        "is_oa": true,
        "abstract": "The code review comment (CRC) is pivotal in the process of modern code review. It provides reviewers with the opportunity to identify potential bugs, offer constructive feedback, and suggest improvements. Clear and concise code review comments (CRCs) facilitate the communication between developers and are crucial to the correct understanding of the identified issues and proposed solutions. Despite the importance of CRCs’ clarity, there is still a lack of guidelines on what constitutes a good clarity and how to evaluate it. In this paper, we conduct a comprehensive study on understanding and evaluating the clarity of CRCs. We first derive a set of attributes related to the clarity of CRCs, namely RIE attributes (i.e., Relevance , Informativeness , and Expression ), as well as their corresponding evaluation criteria based on our literature review and survey with practitioners. We then investigate the clarity of CRCs in open-source projects written in nine programming languages and find that a large portion (i.e., 28.8%) of the CRCs lack the clarity in at least one of the attributes. Finally, we explore the potential of automatically evaluating the clarity of CRCs by proposing ClearCRC. Experimental results show that ClearCRC with pre-trained language models is promising for effective evaluation of the clarity of CRCs, achieving a balanced accuracy up to 73.04% and a F-1 score up to 94.61%.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/issta25codereview.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_ISSTA_Understanding Practitioners’ Expectations on Clear Code Review Comments_1585186dc0b7.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_ISSTA_Understanding Practitioners’ Expectations on Clear Code Review Comments_1585186dc0b7.txt"
      },
      "signals": {
        "has_abstract_heading": false,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": true,
        "has_threats_to_validity": true,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": false
      }
    },
    {
      "id": "3cf242a6e957",
      "label": "C49",
      "title": "Safe4U: Identifying Unsound Safe Encapsulations of Unsafe Calls in Rust using LLMs",
      "year": 2025,
      "venue": "the 34th International Symposium on Software Testing and Analysis (ISSTA 2025)",
      "authors": "[C49] Huan Li, Bei Wang*, Xing Hu*, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4411523107",
        "doi": "https://doi.org/10.1145/3728890",
        "display_name": "Safe4U: Identifying Unsound Safe Encapsulations of Unsafe Calls in Rust using LLMs",
        "publication_year": 2025,
        "is_oa": true,
        "abstract": "Rust is an emerging programming language that ensures safety through strict compile-time checks. A Rust function marked as unsafe indicates it has additional safety requirements (e.g., initialized, not null), known as contracts in the community. These unsafe functions can only be called within explicit unsafe blocks and the contracts must be guaranteed by the caller. To reuse and reduce unsafe code, the community recommends using safe encapsulation of unsafe calls (EUC) in practice. However, an EUC is unsound if any contract is not guaranteed and could lead to undefined behaviors in safe Rust, thus breaking Rust's safety promise. It is challenging to identify unsound EUCs with conventional techniques due to the limitation in cross-lingual comprehension of code and natural language. Large language models (LLMs) have demonstrated impressive capabilities, but their performance is unsatisfactory owing to the complexity of contracts and the lack of domain knowledge. To this end, we propose a novel framework, Safe4U, which incorporates LLMs, static analysis tools, and domain knowledge to identify unsound EUCs. Safe4U first utilizes static analysis tools to retrieve relevant context. Then, it decomposes the primitive contract description into several fine-grained classified contracts. Ultimately, Safe4U introduces domain knowledge and invokes the reasoning capability of LLMs to verify every fine-grained contract. The evaluation results show that Safe4U brings a general performance improvement and the fine-grained results are constructive for locating specific unsound sources. In real-world scenarios, Safe4U can identify 9 out of 11 unsound EUCs from CVE. Furthermore, Safe4U detected 22 new unsound EUCs in the most downloaded crates, 16 of which have been confirmed.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/issta25safe4U.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_ISSTA_Safe4U_ Identifying Unsound Safe Encapsulations of Unsafe Calls in Rust using LL_3cf242a6e957.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_ISSTA_Safe4U_ Identifying Unsound Safe Encapsulations of Unsafe Calls in Rust using LL_3cf242a6e957.txt"
      },
      "signals": {
        "has_abstract_heading": false,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": false
      }
    },
    {
      "id": "96628cfd0157",
      "label": "C51",
      "title": "RealisticCodeBench: Towards More Realistic Evaluation of Large Language Models for Code Generation",
      "year": 2025,
      "venue": "40th IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)",
      "authors": "[C51] Xiao Yu, Haoxuan Chen, Lei Liu, Xing Hu, Jacky Keung, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W7125952837",
        "doi": "https://doi.org/10.1109/ase63991.2025.00248",
        "display_name": "RealisticCodeBench: Towards More Realistic Evaluation of Large Language Models for Code Generation",
        "publication_year": 2025,
        "is_oa": false,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/ase254.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_ASE_RealisticCodeBench_ Towards More Realistic Evaluation of Large Language Models f_96628cfd0157.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_ASE_RealisticCodeBench_ Towards More Realistic Evaluation of Large Language Models f_96628cfd0157.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": true,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "e6278874079d",
      "label": "C52",
      "title": "Code-DiTing: Automatic Evaluation of Code Generation without References or Test Cases",
      "year": 2025,
      "venue": "40th IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)",
      "authors": "[C52] Guang Yang, Yu Zhou, Xiang Chen, Wei Zheng, Xing Hu, Xin Zhou, David Lo, Taolue Chen",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W7125971982",
        "doi": "https://doi.org/10.1109/ase63991.2025.00021",
        "display_name": "Code-DiTing: Automatic Evaluation of Code Generation without References or Test Cases",
        "publication_year": 2025,
        "is_oa": false,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/ase252.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_ASE_Code-DiTing_ Automatic Evaluation of Code Generation without References or Test_e6278874079d.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_ASE_Code-DiTing_ Automatic Evaluation of Code Generation without References or Test_e6278874079d.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "b5ae6b51eb39",
      "label": "C54",
      "title": "When AllClose Fails: Round-Off Error Estimation for Deep Learning Programs",
      "year": 2025,
      "venue": "40th IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)",
      "authors": "[C54] Qi Zhan, Xing Hu*, Yuanyi Lin, Tongtong Xu, Xin Xia, Shanping Li",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W7125990376",
        "doi": "https://doi.org/10.1109/ase63991.2025.00016",
        "display_name": "When AllClose Fails: Round-Off Error Estimation for Deep Learning Programs",
        "publication_year": 2025,
        "is_oa": false,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/ase251.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_ASE_When AllClose Fails_ Round-Off Error Estimation for Deep Learning Programs_b5ae6b51eb39.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_ASE_When AllClose Fails_ Round-Off Error Estimation for Deep Learning Programs_b5ae6b51eb39.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": true,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "6b1eb35b736f",
      "label": "C55",
      "title": "HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing",
      "year": 2025,
      "venue": "40th IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)",
      "authors": "[C55] Yukai Zhao, Menghan Wu, Xing Hu*, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W7125947608",
        "doi": "https://doi.org/10.1109/ase63991.2025.00225",
        "display_name": "HFuzzer: Testing Large Language Models for Package Hallucinations via Phrase-based Fuzzing",
        "publication_year": 2025,
        "is_oa": false,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/ase253.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_ASE_HFuzzer_ Testing Large Language Models for Package Hallucinations via Phrase-bas_6b1eb35b736f.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_ASE_HFuzzer_ Testing Large Language Models for Package Hallucinations via Phrase-bas_6b1eb35b736f.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "5faaec134eae",
      "label": "J18",
      "title": "An Empirical Study on Vulnerability Disclosure Management of Open Source Software Systems",
      "year": 2025,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J18] Shuhan Liu, Jiayuan Zhou, Xing Hu*, Filipe R. Cogo, Xin Xia, Xiaohu Yang",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4408797770",
        "doi": "https://doi.org/10.1145/3716822",
        "display_name": "An empirical study on vulnerability disclosure management of open source software systems",
        "publication_year": 2025,
        "is_oa": true,
        "abstract": "Vulnerability disclosure is critical for ensuring the security and reliability of open source software (OSS). However, in practice, many vulnerabilities are reported and discussed on public platforms before being formally disclosed, posing significant risks to vulnerability management. Inadequate vulnerability disclosure can expose users to security threats and severely impact the stability and reliability of software systems. For example, prior work shows that over 21% of CVEs are publicly discussed before a patch is released. Despite its importance, we still lack clarity on the vulnerability disclosure practices adopted by open source communities and the preferences of practitioners regarding vulnerability management. To fill this gap, we analyzed the vulnerability disclosure practices of 8,073 OSS projects spanning from 2017 to 2023. We then conducted an empirical study by surveying practitioners about their preferences and recommendations in vulnerability disclosure management. Finally, we compared the survey results with the actual vulnerability practice observed within the OSS projects. Our results show that while over 80% of practitioners support Coordinated Vulnerability Disclosure (CVD), only 55% of vulnerabilities conform to CVD in practice. Although only 20% of practitioners advocate discussions before disclosure, 42% of vulnerabilities are discussed in issue reports before their disclosure. This study reveals the vulnerability management practices in OSS, provides valuable guidance to OSS owners, and highlights potential directions to improve the security of OSS platforms.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/tosem25shuhan.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_TOSEM_An Empirical Study on Vulnerability Disclosure Management of Open Source Softwar_5faaec134eae.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_TOSEM_An Empirical Study on Vulnerability Disclosure Management of Open Source Softwar_5faaec134eae.txt"
      },
      "signals": {
        "has_abstract_heading": false,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": true,
        "has_threats_to_validity": false,
        "has_evaluation_section": false,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": false
      }
    },
    {
      "id": "e74ad45ea1e1",
      "label": "J19",
      "title": "Towards On-The-Fly Code Performance Profiling",
      "year": 2025,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J19] Xing Hu, Weixin Lin, Zhuang Liu, Xin Xia, Michael Ling, Yuan Wang, David Lo",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4408851891",
        "doi": "https://doi.org/10.1145/3725212",
        "display_name": "Towards On-the-Fly Code Performance Profiling",
        "publication_year": 2025,
        "is_oa": true,
        "abstract": "Improving the performance of software applications is one of the most important tasks in software evolution and maintenance. In the Intel Microarchitecture, CPUs employ pipelining to utilize resources as effectively as possible. Some types of software patterns or algorithms can have implications on the underlying CPU pipelines and result in inefficiencies. Therefore, analyzing how well the CPU’s pipeline(s) are being utilized while running an application is important in software performance analysis. Existing techniques, such as Intel VTune Profiler, usually detect software performance issues from CPU pipeline metrics after the software enters production and during the running time. These techniques require developers to manually analyze monitoring data and perform additional test runs to obtain relevant information about performance problems. It costs a lot of time and human effort for developers to build, deploy, test, execute, and monitor the software. To alleviate these problems, we propose a novel approach named PGProf to predict the CPU pipeline before execution and provide the profiling feedback during the development process. PGProf exploits the graph neural networks to learn semantic and structural representations for C functions and then predict the fraction of pipeline slots in each category for them during the development process. Given a code snippet, we fuse different types of code structures, e.g., Abstract Syntax Tree (AST), Dataflow Graph (DFG), and Control Flow Graph (CFG) into one program graph. During offline learning, we first leverage the gated graph neural network to capture representations of C functions. PGProf then automatically estimates the final pipeline values according to the learned semantic and structural features. For online prediction, we predict pipeline metrics with four category values by leveraging the offline trained model. We build our dataset from C projects in GitHub and use Intel VTune profiler to get profiling information by running them. Extensive experimental results show the promising performance of our model. We achieved absolute result of 49.90% and 79.44% in terms of \\(Acc@5\\%\\) and \\(Acc@10\\%\\) with improvements of 8.0%–42.7% and 7.8%–20.1% over a set of baselines.",
        "error": null
      },
      "pdf": {
        "url": "https://xing-hu.github.io/assets/papers/tosem25profiling.pdf",
        "source": "author_page",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_TOSEM_Towards On-The-Fly Code Performance Profiling_e74ad45ea1e1.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_TOSEM_Towards On-The-Fly Code Performance Profiling_e74ad45ea1e1.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "99f51c0a1819",
      "label": "J20",
      "title": "https://chen-junkai.github.io/",
      "year": 2025,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J20] <a href=",
      "is_award": false,
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "is_oa": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "f8d337554edf",
      "label": "J21",
      "title": "https://jw-zhang-zju.github.io/",
      "year": 2025,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J21] <a href=",
      "is_award": false,
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "is_oa": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "91a474e2b4d1",
      "label": "J22",
      "title": "More Than Meets the Eye: On Evaluating SBOM Tools In Java",
      "year": 2025,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J22] Menghan Wu, Yukai Zhao, Xing Hu*, Xian Zhan, Shanping Li, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4414246221",
        "doi": "https://doi.org/10.1145/3766073",
        "display_name": "More Than Meets the Eye: On Evaluating SBOM Tools In Java",
        "publication_year": 2025,
        "is_oa": false,
        "abstract": "Open-source software is widely used in current software development. Unfortunately, this integration introduces a spectrum of challenges and potential threats. Such challenges emerge due to the diversity of import scenarios, which in turn may introduce malicious or vulnerable code in the client software, thereby causing significant security risks. To improve the transparency of software supply chains, Software Bill of Materials (SBOM) tools are proposed to identify the components within software systems. However, there limit investigation of functionality (i.e., tool operational process and data fields) and their practical performance of SBOM tools across various import scenarios. In this paper, we perform a comprehensive empirical study to investigate the impact of different import scenarios on SBOM tools. Specifically, we focus on three distinct component import scenarios: Build Tool Import, Dynamic Loading, and Source Code Import across a new benchmark consisting of 152 projects. We find that (1) The detection capabilities of SBOM tools exhibit considerable variance, especially in identifying dependency relationships; (2) The effectiveness of SBOM tools within the import scenarios of Dynamic Loading and Source Code Import falls short of expectations. Based on our findings, we summarize the lessons learned from different perspectives, including practitioners, tool vendors, and researchers. Our study provides valuable insights into the intricate landscape of software component usage, contributing to enhancing SBOM tools in modern software development.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "6cdc019e6473",
      "label": "J23",
      "title": "Less Is More: Unlocking Semi-Supervised Deep Learning for Vulnerability Detection",
      "year": 2025,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J23] Xiao Yu, Guancheng Lin, Xing Hu, Jacky Wai Keung, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4403854610",
        "doi": "https://doi.org/10.1145/3699602",
        "display_name": "Less Is More: Unlocking Semi-Supervised Deep Learning for Vulnerability Detection",
        "publication_year": 2024,
        "is_oa": true,
        "abstract": "Deep learning has demonstrated its effectiveness in software vulnerability detection, but acquiring a large number of labeled code snippets for training deep learning models is challenging due to labor-intensive annotation. With limited labeled data, complex deep learning models often suffer from overfitting and poor performance. To address this limitation, semi-supervised deep learning offers a promising approach by annotating unlabeled code snippets with pseudo-labels and utilizing limited labeled data together as training sets to train vulnerability detection models. However, applying semi-supervised deep learning for accurate vulnerability detection comes with several challenges. One challenge lies in how to select correctly pseudo-labeled code snippets as training data, while another involves mitigating the impact of potentially incorrectly pseudo-labeled training code snippets during model training. To address these challenges, we propose the semi-supervised vulnerability detection (SSVD) approach. SSVD leverages the information gain of model parameters as the certainty of the correctness of pseudo-labels and prioritizes high-certainty pseudo-labeled code snippets as training data. Additionally, it incorporates the proposed noise-robust triplet loss to maximize the separation between vulnerable and non-vulnerable code snippets to better propagate labels from labeled code snippets to nearby unlabeled snippets and utilizes the proposed noise-robust cross-entropy loss for gradient clipping to mitigate the error accumulation caused by incorrect pseudo-labels. We evaluate SSVD with nine semi-supervised approaches on four widely-used public vulnerability datasets. The results demonstrate that SSVD outperforms the baselines with an average of 29.82% improvement in terms of F1-score and 56.72% in terms of MCC. In addition, SSVD trained on a certain proportion of labeled data can outperform or closely match the performance of fully supervised LineVul and ReVeal vulnerability detection models trained on 100% labeled data in most scenarios. This indicates that SSVD can effectively learn from limited labeled data to enhance vulnerability detection performance, thereby reducing the effort required for labeling a large number of code snippets.",
        "error": null
      },
      "pdf": {
        "url": "https://dl.acm.org/doi/pdf/10.1145/3699602",
        "source": "openalex",
        "downloaded": false,
        "error": "HTTPError: HTTP Error 403: Forbidden",
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "22652b499fc6",
      "label": "J24",
      "title": "The current challenges of software engineering in the era of large language models",
      "year": 2025,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J24] Cuiyun Gao, Xing Hu*, Shan Gao, Xin Xia, Zhi Jin*",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4406324010",
        "doi": "https://doi.org/10.1145/3712005",
        "display_name": "The Current Challenges of Software Engineering in the Era of Large Language Models",
        "publication_year": 2025,
        "is_oa": false,
        "abstract": "With the advent of large language models (LLMs) in the AI area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape’s challenges and opportunities. The article aims at revisiting the software development lifecycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The article first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as SE and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement and design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field.",
        "error": null
      },
      "pdf": {
        "url": "http://arxiv.org/pdf/2412.14554v2.pdf",
        "source": "arxiv",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_TOSEM_The current challenges of software engineering in the era of large language mode_22652b499fc6.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_TOSEM_The current challenges of software engineering in the era of large language mode_22652b499fc6.txt"
      },
      "signals": {
        "has_abstract_heading": false,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": false
      }
    },
    {
      "id": "e98d75094757",
      "label": "J25",
      "title": "Artificial Intelligence for Software Engineering: The Journey so far and the Road ahead",
      "year": 2025,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J25] Iftekhar Ahmed, Aldeida Aleti, Haipeng Cai, Alexander Chatzigeorgiou, Pinjia He, Xing Hu, Mauro Pezzè, Denys Poshyvanyk, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4409584646",
        "doi": "https://doi.org/10.1145/3719006",
        "display_name": "Artificial Intelligence for Software Engineering: The Journey So Far and the Road Ahead",
        "publication_year": 2025,
        "is_oa": true,
        "abstract": "Artificial intelligence and recent advances in deep learning architectures, including transformer networks and large language models, change the way people think and act to solve problems. Software engineering, as an increasingly complex process to design, develop, test, deploy, and maintain large-scale software systems for solving real-world challenges, is profoundly affected by many revolutionary artificial intelligence tools in general and machine learning in particular. In this roadmap for artificial intelligence in software engineering, we highlight the recent deep impact of artificial intelligence on software engineering by discussing successful stories of applications of artificial intelligence to classic and new software development challenges. We identify the new challenges that the software engineering community has to address in the coming years to successfully apply artificial intelligence in software engineering, and we share our research roadmap toward the effective use of artificial intelligence in the software engineering profession, while still protecting fundamental human values. We spotlight three main areas that challenge the research in software engineering: the use of generative artificial intelligence and large language models for engineering large software systems, the need of large and unbiased datasets and benchmarks for training and evaluating deep learning and large language models for software engineering, and the need of a new code of digital ethics to apply artificial intelligence in software engineering.",
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    },
    {
      "id": "9bdac7c0270f",
      "label": "J26",
      "title": "CATCODER: Repository-Level Code Generation with Relevant Code and Type Context",
      "year": 2025,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J26] Zhiyuan Pan, Xing Hu*, Xin Xia, Xiaohu Yang",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4399448014",
        "doi": "https://doi.org/10.48550/arxiv.2406.03283",
        "display_name": "CATCODER: Repository-Level Code Generation with Relevant Code and Type Context",
        "publication_year": 2024,
        "is_oa": true,
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, repository-level code generation presents unique challenges, particularly due to the need to utilize information spread across multiple files within a repository. Specifically, successful generation depends on a solid grasp of both general, context-agnostic knowledge and specific, context-dependent knowledge. While LLMs are widely used for the context-agnostic aspect, existing retrieval-based approaches sometimes fall short as they are limited in obtaining a broader and deeper repository context. In this paper, we present CatCoder, a novel code generation framework designed for statically typed programming languages. CatCoder enhances repository-level code generation by integrating relevant code and type context. Specifically, it leverages static analyzers to extract type dependencies and merges this information with retrieved code to create comprehensive prompts for LLMs. To evaluate the effectiveness of CatCoder, we adapt and construct benchmarks that include 199 Java tasks and 90 Rust tasks. The results show that CatCoder outperforms the RepoCoder baseline by up to 14.44% and 17.35%, in terms of compile@k and pass@k scores. In addition, the generalizability of CatCoder is assessed using various LLMs, including both code-specialized models and general-purpose models. Our findings indicate consistent performance improvements across all models, which underlines the practicality of CatCoder. Furthermore, we evaluate the time consumption of CatCoder in a large open source repository, and the results demonstrate the scalability of CatCoder.",
        "error": null
      },
      "pdf": {
        "url": "https://arxiv.org/pdf/2406.03283",
        "source": "openalex",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_TOSEM_CATCODER_ Repository-Level Code Generation with Relevant Code and Type Context_9bdac7c0270f.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_TOSEM_CATCODER_ Repository-Level Code Generation with Relevant Code and Type Context_9bdac7c0270f.txt"
      },
      "signals": {
        "has_abstract_heading": false,
        "has_introduction_heading": true,
        "has_contributions_phrase": true,
        "has_rq": false,
        "has_threats_to_validity": true,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": false
      }
    },
    {
      "id": "1c76289261f6",
      "label": "J27",
      "title": "Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks",
      "year": 2025,
      "venue": "In ACM Transactions on Software Engineering and Methodology (TOSEM)",
      "authors": "[J27] Xing Hu, Feifei Niu, Junkai Chen, Xin Zhou, Junwei Zhang, Junda He, Xin Xia, David Lo",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W7117477189",
        "doi": "https://doi.org/10.1145/3786771",
        "display_name": "Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks",
        "publication_year": 2025,
        "is_oa": false,
        "abstract": "Large language models (LLMs) are gaining increasing popularity in software engineering (SE) due to their unprecedented performance across various applications. These models are increasingly being utilized for a range of SE tasks, including requirements engineering and design, code analysis and generation, software maintenance, and quality assurance. As LLMs become more integral to SE, evaluating their effectiveness is crucial for understanding their potential in this field. In recent years, substantial efforts have been made to assess LLM performance in various SE tasks, resulting in the creation of several benchmarks tailored to this purpose. This paper offers a thorough review of 291 benchmarks, addressing three main aspects: what benchmarks are available , how benchmarks are constructed , and the future outlook for these benchmarks . We begin by examining SE tasks such as requirements engineering and design, coding assistant, software testing, AIOps, software maintenance, and quality management. We then analyze the benchmarks and their development processes, highlighting the limitations of existing benchmarks. Additionally, we discuss the successes and failures of LLMs in different software tasks and explore future opportunities and challenges for SE-related benchmarks. We aim to provide a comprehensive overview of benchmark research in SE and offer insights to support the creation of more effective evaluation tools.",
        "error": null
      },
      "pdf": {
        "url": "http://arxiv.org/pdf/2505.08903v4.pdf",
        "source": "arxiv",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2025_TOSEM_Assessing and Advancing Benchmarks for Evaluating Large Language Models in Softw_1c76289261f6.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2025_TOSEM_Assessing and Advancing Benchmarks for Evaluating Large Language Models in Softw_1c76289261f6.txt"
      },
      "signals": {
        "has_abstract_heading": false,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": true,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": false
      }
    },
    {
      "id": "a09f884a86ec",
      "label": "C56",
      "title": "Automating Just-In-Time Python Type Annotation Updating",
      "year": 2026,
      "venue": "48th International Conference on Software Engineering (ICSE 2026)",
      "authors": "[C56] Zhipeng Xue, Zhipeng Gao, Xing Hu, Jingyuan Chen, Xin Xia, Shanping Li",
      "is_award": false,
      "openalex": {
        "score": 0.3793103448275862,
        "id": "https://openalex.org/W1641498739",
        "doi": "https://doi.org/10.1109/tmi.2014.2377694",
        "display_name": "The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)",
        "publication_year": 2014,
        "is_oa": true,
        "abstract": "In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients-manually annotated by up to four raters-and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%-85%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.",
        "error": null
      },
      "pdf": {
        "url": "https://ieeexplore.ieee.org/ielx7/42/7283692/06975210.pdf",
        "source": "openalex",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2026_ICSE_Automating Just-In-Time Python Type Annotation Updating_a09f884a86ec.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2026_ICSE_Automating Just-In-Time Python Type Annotation Updating_a09f884a86ec.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": false,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "fb2a78351109",
      "label": "C57",
      "title": "Diffploit: Facilitating Cross-Version Exploit Migration for Open Source Library Vulnerabilities",
      "year": 2026,
      "venue": "48th International Conference on Software Engineering (ICSE 2026)",
      "authors": "[C57] Zirui Chen, Zhipeng Xue, Jiayuan Zhou, Xing Hu*, Xin Xia, Xiaohu Yang",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4416360780",
        "doi": "https://doi.org/10.48550/arxiv.2511.12950",
        "display_name": "Diffploit: Facilitating Cross-Version Exploit Migration for Open Source Library Vulnerabilities",
        "publication_year": 2025,
        "is_oa": true,
        "abstract": "Exploits are commonly used to demonstrate the presence of library vulnerabilities and validate their impact across different versions. However, their direct application to alternative versions often fails due to breaking changes introduced during evolution. These failures stem from both changes in triggering conditions (e.g., API refactorings) and broken dynamic environments (e.g., build or runtime errors), which are challenging to interpret and adapt manually. Existing techniques primarily focus on code-level trace alignment through fuzzing, which is both time-consuming and insufficient for handling environment-level failures. Moreover, they often fall short when dealing with complicated triggering condition changes across versions. To overcome this, we propose Diffploit, an iterative, diff-driven exploit migration method structured around two key modules: the Context Module and the Migration Module. The Context Module dynamically constructs contexts derived from analyzing behavioral discrepancies between the target and reference versions, which capture the failure symptom and its related diff hunks. Leveraging these contexts, the Migration Module guides an LLM-based adaptation through an iterative feedback loop, balancing exploration of diff candidates and gradual refinement to resolve reproduction failures effectively. We evaluate Diffploit on a large-scale dataset containing 102 Java CVEs and 689 version-migration tasks across 79 libraries. Diffploit successfully migrates 84.2% exploits, outperforming the change-aware test repair tool TARGET by 52.0% and the rule-based tool in IDEA by 61.6%. Beyond technical effectiveness, Diffploit identifies 5 CVE reports with incorrect affected version ranges, three of which have been confirmed. It also discovers 111 unreported vulnerable versions in the GitHub Advisory Database.",
        "error": null
      },
      "pdf": {
        "url": "https://arxiv.org/pdf/2511.12950",
        "source": "openalex",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2026_ICSE_Diffploit_ Facilitating Cross-Version Exploit Migration for Open Source Library_fb2a78351109.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2026_ICSE_Diffploit_ Facilitating Cross-Version Exploit Migration for Open Source Library_fb2a78351109.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "3c4d10d0af56",
      "label": "C58",
      "title": "CREME: Robustness Enhancement of Code LLMs via Layer-Aware Model Editing",
      "year": 2026,
      "venue": "48th International Conference on Software Engineering (ICSE 2026)",
      "authors": "[C58] Shuhan Liu, Xing Hu*, Kerui Huang, Xiaohu Yang, David Lo, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4414418240",
        "doi": "https://doi.org/10.48550/arxiv.2507.16407",
        "display_name": "CREME: Robustness Enhancement of Code LLMs via Layer-Aware Model Editing",
        "publication_year": 2025,
        "is_oa": true,
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in code generation, where the natural language prompt plays a crucial role in conveying user intent to the model. However, prior studies have shown that LLMs are highly sensitive to prompt perturbations. Minor modifications in wording, syntax, or formatting can significantly reduce the functional correctness of generated code. As perturbations frequently occur in real-world scenarios, improving the robustness of LLMs to prompt perturbations is essential for ensuring reliable performance in practical code generation. In this paper, we introduce CREME (Code Robustness Enhancement via Model Editing), a novel approach that enhances LLM robustness through targeted parameter updates. CREME first identifies robustness-sensitive layers by comparing hidden states between an original prompt and its perturbed variant. Then, it performs lightweight parameter editing at the identified layer to reduce performance degradation. We evaluate CREME on two widely used code generation benchmarks (HumanEval and MBPP) along with their perturbed counterparts. Experimental results show that CREME improves Pass@1 accuracy by 63% on perturbed prompts while maintaining stable performance on clean inputs, with accuracy deviations within 1%. Further analysis reveals that robustness-sensitive layers are primarily concentrated in the middle and deeper layers of the network, and their locations vary across different model architectures. These insights provide a valuable foundation for developing future robustness-oriented editing strategies.",
        "error": null
      },
      "pdf": {
        "url": "https://arxiv.org/pdf/2507.16407",
        "source": "openalex",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2026_ICSE_CREME_ Robustness Enhancement of Code LLMs via Layer-Aware Model Editing_3c4d10d0af56.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2026_ICSE_CREME_ Robustness Enhancement of Code LLMs via Layer-Aware Model Editing_3c4d10d0af56.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "30872b09976a",
      "label": "C59",
      "title": "Actionable Warning Is Not Enough: Recommending Valid Actionable Warnings with Weak Supervision",
      "year": 2026,
      "venue": "48th International Conference on Software Engineering (ICSE 2026)",
      "authors": "[C59] Zhipeng Xue, Zhipeng Gao, Tongtong Xu, Xing Hu, Xin Xia, Shanping Li",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W4416458649",
        "doi": "https://doi.org/10.1145/3744916.3773133",
        "display_name": "Actionable Warning Is Not Enough: Recommending Valid Actionable Warnings with Weak Supervision",
        "publication_year": 2025,
        "is_oa": true,
        "abstract": "The use of static analysis tools has gained increasing popularity among developers in the last few years. However, the widespread adoption of static analysis tools is hindered by their high false alarm rates. Previous studies have introduced the concept of actionable warnings and built a machine-learning method to distinguish actionable warnings from false alarms. However, according to our empirical observation, the current assumption used for actionable warning(s) collection is rather shaky and inaccurate, leading to a large number of invalid actionable warnings. To address this problem, in this study, we build the first large actionable warning dataset by mining 68,274 reversions from Top-500 GitHub C repositories, we then take one step further by assigning each actionable warning a weak label regarding its likelihood of being a real bug. Following that, we propose a two-stage framework called ACWRecommender to automatically recommend the actionable warnings with high probability to be real bugs (AWHB). Our approach warms up the pre-trained model UniXcoder by identifying actionable warnings task (coarse-grained detection stage) and rerank AWHB to the top by weakly supervised learning (fine-grained reranking stage). Experimental results show that our proposed model outperforms several baselines by a large margin in terms of nDCG and MRR for AWHB recommendation. Moreover, we ran our tool on 6 randomly selected projects and manually checked the top-ranked warnings from 2,197 reported warnings, we reported top-10 recommended warnings to developers, 27 of them were already confirmed by developers as real bugs. Developers can quickly find real bugs among the massive amount of reported warnings, which verifies the practical usage of our tool.",
        "error": null
      },
      "pdf": {
        "url": "https://arxiv.org/pdf/2511.12229",
        "source": "openalex",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2026_ICSE_Actionable Warning Is Not Enough_ Recommending Valid Actionable Warnings with We_30872b09976a.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2026_ICSE_Actionable Warning Is Not Enough_ Recommending Valid Actionable Warnings with We_30872b09976a.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": true,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "bb809066c8a9",
      "label": "C60",
      "title": "Depradar: Agentic Coordination for Context-Aware Defect Impact Analysis in Deep Learning Libraries",
      "year": 2026,
      "venue": "48th International Conference on Software Engineering (ICSE 2026)",
      "authors": "[C60] Yi Gao, Xing Hu*, Tongtong Xu, Jiali Zhao, Xiaohu Yang, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": 1.03,
        "id": "https://openalex.org/W7124283157",
        "doi": "https://doi.org/10.48550/arxiv.2601.09440",
        "display_name": "DepRadar: Agentic Coordination for Context Aware Defect Impact Analysis in Deep Learning Libraries",
        "publication_year": 2026,
        "is_oa": true,
        "abstract": "Deep learning libraries like Transformers and Megatron are now widely adopted in modern AI programs. However, when these libraries introduce defects, ranging from silent computation errors to subtle performance regressions, it is often challenging for downstream users to assess whether their own programs are affected. Such impact analysis requires not only understanding the defect semantics but also checking whether the client code satisfies complex triggering conditions involving configuration flags, runtime environments, and indirect API usage. We present DepRadar, an agent coordination framework for fine grained defect and impact analysis in DL library updates. DepRadar coordinates four specialized agents across three steps: 1. the PR Miner and Code Diff Analyzer extract structured defect semantics from commits or pull requests, 2. the Orchestrator Agent synthesizes these signals into a unified defect pattern with trigger conditions, and 3. the Impact Analyzer checks downstream programs to determine whether the defect can be triggered. To improve accuracy and explainability, DepRadar integrates static analysis with DL-specific domain rules for defect reasoning and client side tracing. We evaluate DepRadar on 157 PRs and 70 commits across two representative DL libraries. It achieves 90% precision in defect identification and generates high quality structured fields (average field score 1.6). On 122 client programs, DepRadar identifies affected cases with 90% recall and 80% precision, substantially outperforming other baselines.",
        "error": null
      },
      "pdf": {
        "url": "https://arxiv.org/pdf/2601.09440",
        "source": "openalex_discovered",
        "downloaded": true,
        "path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\pdfs_huxing\\2026_ICSE_Depradar_ Agentic Coordination for Context-Aware Defect Impact Analysis in Deep_bb809066c8a9.pdf",
        "preview_text_path": "C:\\Users\\daoge\\OneDrive\\Projects\\templog\\writingskills\\caches\\extracted_huxing\\2026_ICSE_Depradar_ Agentic Coordination for Context-Aware Defect Impact Analysis in Deep_bb809066c8a9.txt"
      },
      "signals": {
        "has_abstract_heading": true,
        "has_introduction_heading": true,
        "has_contributions_phrase": false,
        "has_rq": false,
        "has_threats_to_validity": false,
        "has_evaluation_section": true,
        "mentions_tool_or_implementation": false,
        "abstract_has_numbers": true
      }
    },
    {
      "id": "8836fc2e39bf",
      "label": "C61",
      "title": "IntentTester: Intent-Driven Multi-Agent Framework for Cross-Library Test Migration",
      "year": 2026,
      "venue": "International Conference on the Foundations of Software Engineering (FSE 2026)",
      "authors": "[C61] Yi Gao, Ziyuan Zhang, Xing Hu, Xiaohu Yang, Xin Xia",
      "is_award": false,
      "openalex": {
        "score": -1.0,
        "id": null,
        "doi": null,
        "display_name": null,
        "publication_year": null,
        "is_oa": null,
        "abstract": null,
        "error": null
      },
      "pdf": {
        "url": null,
        "source": null,
        "downloaded": false,
        "path": null,
        "preview_text_path": null
      },
      "signals": null
    }
  ]
}