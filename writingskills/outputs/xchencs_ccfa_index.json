{
  "generated_at": "2026-02-12T02:54:31.221814+00:00",
  "source_url": "https://xchencs.github.io/publications.html",
  "total_ccfa_papers": 27,
  "papers": [
    {
      "title": "Deep Learning Framework Testing via Model Mutation: How Far Are We?",
      "year": 2026,
      "venue": "TSE",
      "venue_full": "IEEE Transactions on Software Engineering",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1109/tse.2026.3651022",
      "cited_by_count": 0,
      "abstract": null,
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W7118193374",
      "openalex_score": 1.03
    },
    {
      "title": "ACTaint: Agent-Based Taint Analysis for Access Control Vulnerabilities in Smart Contracts.",
      "year": 2025,
      "venue": "ASE",
      "venue_full": "In Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1109/ase63991.2025.00210",
      "cited_by_count": 0,
      "abstract": null,
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W7125899799",
      "openalex_score": 1.03
    },
    {
      "title": "Code-DiTing: Automatic Evaluation of Code Generation without References or Test Cases.",
      "year": 2025,
      "venue": "ASE",
      "venue_full": "In Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1109/ase63991.2025.00021",
      "cited_by_count": 0,
      "abstract": null,
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W7125971982",
      "openalex_score": 1.03
    },
    {
      "title": "Evaluating and Improving Framework-based Parallel Code Completion with Large Language Models.",
      "year": 2025,
      "venue": "ASE",
      "venue_full": "In Proceedings of the 40th IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1109/ase63991.2025.00204",
      "cited_by_count": 0,
      "abstract": null,
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W7125955781",
      "openalex_score": 1.03
    },
    {
      "title": "Improving Deep Learning Framework Testing with Model-Level Metamorphic Testing.",
      "year": 2025,
      "venue": "ISSTA",
      "venue_full": "In Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2025)",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1145/3728972",
      "cited_by_count": 1,
      "abstract": "Deep learning (DL) frameworks are essential to DL-based software systems, and framework bugs may lead to substantial disasters, thus requiring effective testing. Researchers adopt DL models or single interfaces as test inputs and analyze their execution results to detect bugs. However, floating-point errors, inherent randomness, and the complexity of test inputs make it challenging to analyze execution results effectively, leading to existing methods suffering from a lack of suitable test oracles. Some researchers utilize metamorphic testing to tackle this challenge. They design Metamorphic Relations (MRs) based on input data and parameter settings of a single framework interface to generate equivalent test inputs, ensuring consistent execution results between original and generated test inputs. Despite their promising effectiveness, they still face certain limitations. (1) Existing MRs overlook structural complexity, limiting test input diversity. (2) Existing MRs focus on limited interfaces, which limits generalization and necessitates additional adaptations. (3) Their detected bugs are related to the result consistency of single interfaces and far from those exposed in multi-interface combinations and runtime metrics (e.g., resource usage). To address these limitations, we propose ModelMeta, a model-level metamorphic testing method for DL frameworks with four MRs focused on the structure characteristics of DL models. ModelMeta augments seed models with diverse interface combinations to generate test inputs with consistent outputs, guided by the QR-DQN strategy. It then detects bugs through fine-grained analysis of training loss/gradients, memory/GPU usage, and execution time. We evaluate the effectiveness of ModelMeta on three popular DL frameworks (i.e., MindSpore, PyTorch, and ONNX) with 17 DL models from ten real-world tasks ranging from image classification to object detection. Results demonstrate that ModelMeta outperforms state-of-the-art baselines from the perspective of test coverage and diversity of generated test inputs. Regarding bug detection, ModelMeta has identified 31 new bugs, of which 27 have been confirmed, and 11 have been fixed. Among them, seven bugs existing methods cannot detect, i.e., five wrong resource usage bugs and two low-efficiency bugs. These results demonstrate the practicality of our method.",
      "is_oa": true,
      "oa_url": "https://doi.org/10.1145/3728972",
      "openalex_id": "https://openalex.org/W4411522836",
      "openalex_score": 1.03
    },
    {
      "title": "An Empirical Study of Code Simplification Methods in Code Intelligence Tasks.",
      "year": 2025,
      "venue": "TOSEM",
      "venue_full": "ACM Transactions on Software Engineering and Methodology",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1145/3720540",
      "cited_by_count": 2,
      "abstract": "In recent years, pre-trained language models have seen significant success in natural language processing and have been increasingly applied to code-related tasks. Code intelligence tasks have shown promising performance with the support of code pre-trained language models. Pre-processing code simplification methods have been introduced to prune code tokens from the model’s input while maintaining task effectiveness. These methods improve the efficiency of code intelligence tasks while reducing computational costs. Post-prediction code simplification methods provide explanations for code intelligence task outcomes, enhancing the reliability and interpretability of model predictions. However, comprehensive evaluations of these methods across diverse code pre-trained model architectures and code intelligence tasks are lacking. To assess the effectiveness of code simplification methods, we conduct an empirical study integrating these code simplification methods with various pre-trained code models across multiple code intelligence tasks. Our empirical findings suggest that developing task-specific code simplification methods would be beneficial. Then, we recommend leveraging post-prediction methods to summarize prior knowledge, which can pre-process code simplification strategies. Moreover, establishing more evaluation mechanisms for code simplification is crucial. Finally, we propose incorporating code simplification methods into the pre-training phase of code pre-trained models to enhance their program comprehension and code representation capabilities.",
      "is_oa": true,
      "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3720540",
      "openalex_id": "https://openalex.org/W4408021172",
      "openalex_score": 1.03
    },
    {
      "title": "An Empirical Study on Challenges for LLM Application Developers.",
      "year": 2025,
      "venue": "TOSEM",
      "venue_full": "ACM Transactions on Software Engineering and Methodology",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1145/3715007",
      "cited_by_count": 10,
      "abstract": "In recent years, large language models (LLMs) have seen rapid advancements, significantly impacting various fields such as computer vision, natural language processing, and software engineering. These LLMs, exemplified by OpenAI's ChatGPT, have revolutionized the way we approach language understanding and generation tasks. However, in contrast to traditional software development practices, LLM development introduces new challenges for AI developers in design, implementation, and deployment. These challenges span different areas (such as prompts, APIs, and plugins), requiring developers to navigate unique methodologies and considerations specific to LLM application development. Despite the profound influence of LLMs, to the best of our knowledge, these challenges have not been thoroughly investigated in previous empirical studies. To fill this gap, we present the first comprehensive study on understanding the challenges faced by LLM developers. Specifically, we crawl and analyze 29,057 relevant questions from a popular OpenAI developer forum. We first examine their popularity and difficulty. After manually analyzing 2,364 sampled questions, we construct a taxonomy of challenges faced by LLM developers. Based on this taxonomy, we summarize a set of findings and actionable implications for LLM-related stakeholders, including developers and providers (especially the OpenAI organization).",
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W4406738283",
      "openalex_score": 1.03
    },
    {
      "title": "Defending Code Language Models against Backdoor Attacks with Deceptive Cross-Entropy Loss.",
      "year": 2025,
      "venue": "TOSEM",
      "venue_full": "ACM Transactions on Software Engineering and Methodology",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.48550/arxiv.2407.08956",
      "cited_by_count": 2,
      "abstract": "Code Language Models (CLMs), particularly those leveraging deep learning, have achieved significant success in code intelligence domain. However, the issue of security, particularly backdoor attacks, is often overlooked in this process. The previous research has focused on designing backdoor attacks for CLMs, but effective defenses have not been adequately addressed. In particular, existing defense methods from natural language processing, when directly applied to CLMs, are not effective enough and lack generality, working well in some models and scenarios but failing in others, thus fall short in consistently mitigating backdoor attacks. To bridge this gap, we first confirm the phenomenon of \"early learning\" as a general occurrence during the training of CLMs. This phenomenon refers to that a model initially focuses on the main features of training data but may become more sensitive to backdoor triggers over time, leading to overfitting and susceptibility to backdoor attacks. We then analyze that overfitting to backdoor triggers results from the use of the cross-entropy loss function, where the unboundedness of cross-entropy leads the model to increasingly concentrate on the features of the poisoned data. Based on this insight, we propose a general and effective loss function DeCE (Deceptive Cross-Entropy) by blending deceptive distributions and applying label smoothing to limit the gradient to bounded, which prevents the model from overfitting to backdoor triggers and then enhances the security of CLMs against backdoor attacks.",
      "is_oa": true,
      "oa_url": "https://arxiv.org/pdf/2407.08956",
      "openalex_id": "https://openalex.org/W4400667839",
      "openalex_score": 1.03
    },
    {
      "title": "Experimental Evaluation of Parameter-Efficient Fine-Tuning for Software Engineering Tasks.",
      "year": 2025,
      "venue": "TOSEM",
      "venue_full": "ACM Transactions on Software Engineering and Methodology",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1145/3722107",
      "cited_by_count": 0,
      "abstract": "Pre-trained models (PTMs) have succeeded in various software engineering (SE) tasks following the “pre-train then fine-tune” paradigm. As fully fine-tuning all parameters of PTMs can be computationally expensive, a potential solution is parameter-efficient fine-tuning (PEFT), which freezes PTMs while introducing extra parameters. Although PEFT methods have been applied to SE tasks, researchers often focus on specific scenarios and lack a comprehensive comparison of PTMs from different aspects such as field, size, and architecture. To fill this gap, we have conducted an empirical study on six PEFT methods, eight PTMs, and four SE tasks. The experimental results reveal several noteworthy findings. For example, model architecture has little impact on PTM performance when using PEFT methods. Additionally, we provide a comprehensive discussion of PEFT methods from three perspectives. First, we analyze the effectiveness and efficiency of PEFT methods. Second, we explore the impact of the scaling factor hyperparameter. Finally, we investigate the application of PEFT methods on the latest open source large language model, Llama 3.2. These findings provide valuable insights to guide future researchers in effectively applying PEFT methods to SE tasks.",
      "is_oa": true,
      "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3722107",
      "openalex_id": "https://openalex.org/W4408214724",
      "openalex_score": 1.03
    },
    {
      "title": "Less is More: DocString Compression in Code Generation.",
      "year": 2025,
      "venue": "TOSEM",
      "venue_full": "ACM Transactions on Software Engineering and Methodology",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1145/3735636",
      "cited_by_count": 2,
      "abstract": "The widespread use of Large Language Models (LLMs) in software engineering has intensified the need for improved model and resource efficiency. In particular, for neural code generation, LLMs are used to translate function/method signature and DocString to executable code. DocStrings, which capture user requirements for the code and are typically used as the prompt for LLMs, often contain redundant information. Recent advancements in prompt compression have shown promising results in Natural Language Processing (NLP), but their applicability to code generation remains uncertain. Our empirical study shows that the state-of-the-art prompt compression methods achieve only about 10% reduction, as further reductions would cause significant performance degradation. In our study, we propose a novel compression method, ShortenDoc, dedicated to DocString compression for code generation. Our experiments on six code generation datasets, five open source LLMs (1B to 10B parameters), and one closed-source LLM GPT-4o confirm that ShortenDoc achieves 25–40% compression while preserving the quality of generated code, outperforming other baseline methods at similar compression levels. The benefit of this method is to improve efficiency and reduce the token processing cost while maintaining the quality of the generated code, especially when calling third-party APIs.",
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W4410361283",
      "openalex_score": 1.03
    },
    {
      "title": "DevMuT: Testing Deep Learning Framework via Developer Expertise-Based Mutation.",
      "year": 2024,
      "venue": "ASE",
      "venue_full": "In Proceedings of the 39th ACM/IEEE International Conference on Automated Software Engineering (ASE 2024)",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1145/3691620.3695523",
      "cited_by_count": 1,
      "abstract": null,
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W4403536241",
      "openalex_score": 1.03
    },
    {
      "title": "IATT: Interpretation Analysis based Transferable Test Generation for Convolutional Neural Networks.",
      "year": 2024,
      "venue": "TOSEM",
      "venue_full": "ACM Transactions on Software Engineering and Methodology",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1145/3705301",
      "cited_by_count": 1,
      "abstract": "Convolutional Neural Networks (CNNs) have been widely used in various fields. However, it is essential to perform sufficient testing to detect internal defects before deploying CNNs, especially in security-sensitive scenarios. Generating error-inducing inputs to trigger erroneous behavior is the primary way to detect CNN model defects. However, in practice, when the model under test is a black-box CNN model without accessible internal information, in some scenarios it is still necessary to generate high-quality test inputs within a limited testing budget. In such a new scenario, a potential approach is to generate transferable test inputs by analyzing the internal knowledge of other white-box CNN models similar to the model under test, and then use transferable test inputs to test the black-box CNN model. The main challenge in generating transferable test inputs is how to improve their error-inducing capability for different CNN models without changing the test oracle. We found that different CNN models make predictions based on features of similar important regions in images. Adding targeted perturbations to important regions will generate transferable test inputs with high realism. Therefore, we propose the Interpretable Analysis-based Transferable Test (IATT) Generation method for CNNs, which employs interpretation methods of CNN models to explain and localize important regions in test inputs, using backpropagation optimizer and perturbation mask process to add targeted perturbations to these important regions, thereby generating transferable test inputs. This process is repeated to iteratively optimize the transferability and realism of the test inputs. To verify the effectiveness of IATT, we perform experimental studies on nine deep learning models, including ResNet-50 and Vit-B/16, and commercial computer vision system Google Cloud Vision , and compared our method with four state-of-the-art baseline methods. Experimental results show that transferable test inputs generated by IATT can effectively cause black-box target models to output incorrect results. Compared to existing testing and adversarial attack methods, the average Error-inducing Success Rate (ESR) in different testing scenarios is 18.1%–52.7% greater than the baseline methods. Additionally, the test inputs generated by IATT achieve high ESR while maintaining high realism.",
      "is_oa": true,
      "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3705301",
      "openalex_id": "https://openalex.org/W4404711463",
      "openalex_score": 1.03
    },
    {
      "title": "Improving Source Code Pre-training via Type-Specific Masking.",
      "year": 2024,
      "venue": "TOSEM",
      "venue_full": "ACM Transactions on Software Engineering and Methodology",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1145/3699599",
      "cited_by_count": 0,
      "abstract": "The Masked Language Modeling (MLM) task is widely recognized as one of the most effective pre-training tasks and currently derives many variants in the Software Engineering (SE) field. However, most of these variants mainly focus on code representation without distinguishing between different code token types, while some focus on a specific type, such as code identifiers. Indeed, various code token types exist, and there is no evidence that only identifiers can improve PTMs. Thus, to improve PTMs through different types, we conducted an extensive study to evaluate how different type-specific masking tasks can affect PTMs. First, we extract five code token types, convert them into type-specific masking tasks, and generate their combinations. Second, we pre-train CodeBERT and PLBART using combinations and fine-tuned them on four SE downstream tasks. Experimental results show that type-specific masking tasks can enhance CodeBERT and PLBART on all downstream tasks. Furthermore, we discuss topics related to low-resource datasets, conflicting PTMs that original pre-training tasks conflict with our methods, the cost and performance of our methods, factors that impact the performance of our methods, and applying our methods on state-of-the-art PTMs. These discussions comprehensively analyze the strengths and weaknesses of different type-specific masking tasks.",
      "is_oa": true,
      "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3699599",
      "openalex_id": "https://openalex.org/W4403336760",
      "openalex_score": 1.03
    },
    {
      "title": "An Empirical Study on Correlations between Deep Neural Network Fairness and Neuron Coverage Criteria.",
      "year": 2024,
      "venue": "TSE",
      "venue_full": "IEEE Transactions on Software Engineering",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1109/tse.2023.3349001",
      "cited_by_count": 33,
      "abstract": "Recently, with the widespread use of deep neural networks (DNNs) in high-stakes decision-making systems (such as fraud detection and prison sentencing), concerns have arisen about the fairness of DNNs in terms of the potential negative impact they may have on individuals and society. Therefore, fairness testing has become an important research topic in DNN testing. At the same time, the neural network coverage criteria (such as criteria based on neuronal activation) is considered as an adequacy test for DNN white-box testing. It is implicitly assumed that improving the coverage can enhance the quality of test suites. Nevertheless, the correlation between DNN fairness (a test property) and coverage criteria (a test method) has not been adequately explored. To address this issue, we conducted a systematic empirical study on seven coverage criteria, six fairness metrics, three fairness testing techniques, and five bias mitigation methods on five DNN models and nine fairness datasets to assess the correlation between coverage criteria and DNN fairness. Our study achieved the following findings: 1) with the increase in the size of the test suite, some of the coverage and fairness metrics changed significantly, as the size of the test suite increased; 2) the statistical correlation between coverage criteria and DNN fairness is limited; and 3) after bias mitigation for improving the fairness of DNN, the change pattern in coverage criteria is different; 4) Models debiased by different bias mitigation methods have a lower correlation between coverage and fairness compared to the original models. Our findings cast doubt on the validity of coverage criteria concerning DNN fairness (i.e., increasing the coverage may even have a negative impact on the fairness of DNNs). Therefore, we warn DNN testers against blindly pursuing higher coverage of coverage criteria at the cost of test properties of DNNs (such as fairness).",
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W4390660035",
      "openalex_score": 1.03
    },
    {
      "title": "Chain-of-Thought in Neural Code Generation: From and For Lightweight Language Models.",
      "year": 2024,
      "venue": "TSE",
      "venue_full": "IEEE Transactions on Software Engineering",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1109/tse.2024.3440503",
      "cited_by_count": 32,
      "abstract": null,
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W4401508141",
      "openalex_score": 1.03
    },
    {
      "title": "Unifying Defect Prediction, Categorization, and Repair by Multi-task Deep Learning.",
      "year": 2023,
      "venue": "ASE",
      "venue_full": "In Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering,Industry Challenge (Competition) track (ASE-Inch 2023)",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1109/ase56229.2023.00083",
      "cited_by_count": 4,
      "abstract": "Just-In- Time defect prediction models can identify defect-inducing commits at check-in time and many approaches are proposed with remarkable performance. However, these approaches still have a few limitations which affect their effectiveness and practical usage: (1) partially using semantic information or structure information of code, (2) coarsely providing results to a commit (buggy or clean), and (3) independently investigating the defect prediction model and defect repair model. In this study, to handle the aforementioned limitations, we propose a unified defect prediction and repair framework named COMPDEFECT,which can identify whether a changed function inside a commit is defect-prone, categorize the type of defect, and repair such a defect automatically if it falls into several scenarios, e.g., defects with single statement fixes, or those that match a small set of defect templates. Technically, the first two tasks in COMPDEFECT are treated as a multiclass classification task, while the last task is treated as a sequence generation task. To verify the effectiveness of COMPDEFECT, we first build a large-scale function-level dataset (i.e., 21,047) named Function-SStuBs4J and then compare COMPDEFECT with tens of state-of-the-art (SOTA) approaches by considering five performance measures. The experimental results indicate that COMPDEFECT outperforms all SOTAs with a substantial improvement in three tasks separately. Moreover, the pipeline experimental results also indicate the feasibility of COMPDEFECT to unify three tasks in a model.",
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W4388483190",
      "openalex_score": 1.03
    },
    {
      "title": "Exploring Better Black-Box Test Case Prioritization via Log Analysis.",
      "year": 2023,
      "venue": "TOSEM",
      "venue_full": "ACM Transactions on Software Engineering and Methodology",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1145/3569932",
      "cited_by_count": 26,
      "abstract": "Test case prioritization (TCP) has been widely studied in regression testing, which aims to optimize the execution order of test cases so as to detect more faults earlier. TCP has been divided into white-box test case prioritization (WTCP) and black-box test case prioritization (BTCP) . WTCP can achieve better prioritization effectiveness by utilizing source code information, but is not applicable in many practical scenarios (where source code is unavailable, e.g., outsourced testing). BTCP has the benefit of not relying on source code information, but tends to be less effective than WTCP. That is, both WTCP and BTCP suffer from limitations in the practical use. To improve the practicability of TCP, we aim to explore better BTCP, significantly bridging the effectiveness gap between BTCP and WTCP. In this work, instead of statically analyzing test cases themselves in existing BTCP techniques, we conduct the first study to explore whether this goal can be achieved via log analysis. Specifically, we propose to mine test logs produced during test execution to more sufficiently reflect test behaviors, and design a new BTCP framework (called LogTCP), including log pre-processing, log representation, and test case prioritization components. Based on the LogTCP framework, we instantiate seven log-based BTCP techniques by combining different log representation strategies with different prioritization strategies. We conduct an empirical study to explore the effectiveness of LogTCP. Based on 10 diverse open-source Java projects from GitHub, we compared LogTCP with three representative BTCP techniques and four representative WTCP techniques. Our results show that all of our LogTCP techniques largely perform better than all the BTCP techniques in average fault detection, to the extent that they become competitive to the WTCP techniques. That demonstrates the great potential of logs in practical TCP.",
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W4307510928",
      "openalex_score": 1.03
    },
    {
      "title": "How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective.",
      "year": 2023,
      "venue": "TOSEM",
      "venue_full": "ACM Transactions on Software Engineering and Methodology",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1145/3630010",
      "cited_by_count": 21,
      "abstract": "Pre-trained code generation models (PCGMs) have been widely applied in neural code generation, which can generate executable code from functional descriptions in natural languages, possibly together with signatures. Despite substantial performance improvement of PCGMs, the role of method names in neural code generation has not been thoroughly investigated. In this article, we study and demonstrate the potential of benefiting from method names to enhance the performance of PCGMs from a model robustness perspective. Specifically, we propose a novel approach, named neu RA l co D e gener A tor R obustifier (RADAR). RADAR consists of two components: RADAR -Attack and RADAR -Defense. The former attacks a PCGM by generating adversarial method names as part of the input, which are semantic and visual similar to the original input but may trick the PCGM to generate completely unrelated code snippets. As a countermeasure to such attacks, RADAR -Defense synthesizes a new method name from the functional description and supplies it to the PCGM. Evaluation results show that RADAR -Attack can reduce the CodeBLEU of generated code by 19.72% to 38.74% in three state-of-the-art PCGMs (i.e., CodeGPT, PLBART, and CodeT5) in the fine-tuning code generation task and reduce the Pass@1 of generated code by 32.28% to 44.42% in three state-of-the-art PCGMs (i.e., Replit, CodeGen, and CodeT5+) in the zero-shot code generation task. Moreover, RADAR -Defense is able to reinstate the performance of PCGMs with synthesized method names. These results highlight the importance of good method names in neural code generation and implicate the benefits of studying model robustness in software engineering.",
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W4387869031",
      "openalex_score": 1.03
    },
    {
      "title": "Achieving High MAP-Coverage through Pattern Constraint Reduction.",
      "year": 2023,
      "venue": "TSE",
      "venue_full": "IEEE Transactions on Software Engineering",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1109/tse.2022.3144480",
      "cited_by_count": 3,
      "abstract": "Testing multi-threaded programs is challenging due to the enormous space of thread interleavings. Recently, a code coverage criterion for multi-threaded programs called MAP-coverage has been proposed and shown to be effective for testing concurrent programs. Existing approaches for achieving high MAP-coverage are based on random testing with simple heuristics, which is ineffective in systematically triggering rare thread interleavings. In this study, we propose a novel approach called pattern constraint reduction (PCR), which employs optimized constraint solving to generate thread interleavings for high MAP-coverage. The idea is to iteratively encode and solve path conditions to generate thread interleavings which are guaranteed to improve MAP-coverage. Furthermore, we effectively apply interpolation techniques to reduce the efforts of constraint solving by avoiding solving infeasible constraints. The experiment results on 20 benchmark programs show that our approach complements existing random testing based approaches when there are rare failure-inducing interleaving in the whole search space. Specifically, PCR finds concurrency bugs faster in 18 out of 20 programs, with an average speedup of 4.2x and a maximum speedup of 11.4x.",
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W4206915968",
      "openalex_score": 1.03
    },
    {
      "title": "Automated Question Title Reformulation by Mining Modification Logs From Stack Overflow.",
      "year": 2023,
      "venue": "TSE",
      "venue_full": "IEEE Transactions on Software Engineering",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1109/tse.2023.3292399",
      "cited_by_count": 10,
      "abstract": "In Stack Overflow, developers may not clarify and summarize the critical problems in the question titles due to a lack of domain knowledge or poor writing skills. Previous studies mainly focused on automatically generating the question titles by analyzing the posts' problem descriptions and code snippets. In this study, we aim to improve title quality from the perspective of question title reformulation and propose a novel approach <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">QETRA</monospace> motivated by the findings of our formative study. Specifically, by mining modification logs from Stack Overflow, we first extract title reformulation pairs containing the original title and the reformulated title. Then we resort to multi-task learning by formalizing title reformulation for each programming language as separate but related tasks. Later we adopt a pre-trained model T5 to automatically learn the title reformulation patterns. Automated evaluation and human study both show the competitiveness of <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">QETRA</monospace> after compared with six state-of-the-art baselines. Moreover, our ablation study results also confirm that our studied question title reformulation task is more practical than the direct question title generation task for generating high-quality titles. Finally, we develop a browser plugin based on <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">QETRA</monospace> to facilitate the developers to perform title reformulation. Our study provides a new perspective for studying the quality of post titles and can further generate high-quality titles.",
      "is_oa": true,
      "oa_url": "https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=9228&context=sis_research",
      "openalex_id": "https://openalex.org/W4385757764",
      "openalex_score": 1.03
    },
    {
      "title": "An Empirical Study on Numerical Bugs in Deep Learning Programs.",
      "year": 2022,
      "venue": "ASE",
      "venue_full": "In Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, the New Ideas and Emerging Results （NIER） track (ASE-NIER 2022)",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1145/3551349.3559561",
      "cited_by_count": 12,
      "abstract": "The task of a deep learning (DL) program is to train a model with high precision and apply it to different scenarios. A DL program often involves massive numerical calculations. Therefore, the robustness and stability of the numerical calculations are dominant in the quality of DL programs. Indeed, numerical bugs are common in DL programs, producing NaN (Not-a-Number) and INF (Infinite). A numerical bug may render the DL models inaccurate, causing the DL applications unusable. In this work, we conduct the first empirical study on numerical bugs in DL programs by analyzing the programs implemented on the top of two popular DL libraries (i.e., TensorFlow and PyTorch). Specifically, We collect a dataset of 400 numerical bugs in DL programs. Then, we classify these numerical bugs into nine categories based on their root causes and summarize two findings. Finally, we provide the implications of our study on detecting numerical bugs in DL programs.",
      "is_oa": true,
      "oa_url": "https://dl.acm.org/doi/pdf/10.1145/3551349.3559561",
      "openalex_id": "https://openalex.org/W4313547562",
      "openalex_score": 1.03
    },
    {
      "title": "JIT-MTL: Just-in-Time Defect Localization and Prediction with Multi-Task Learning.",
      "year": 2022,
      "venue": "TOSEM",
      "venue_full": "ACM Transactions on Software Engineering and Methodology",
      "ccf": "A",
      "source_links": [
        "https://www.ptpress.com.cn/shopping/buy?bookId=81ea5d77-12b8-450a-9f0f-e3b85490fd7a",
        "https://item.jd.com/13620692.html",
        "http://product.dangdang.com/29373269.html"
      ],
      "openalex_error": "no_match (best_score=0.20)"
    },
    {
      "title": "Revisiting Supervised and Unsupervised Methods for Effort-Aware Cross-Project Defect Prediction.",
      "year": 2022,
      "venue": "TSE",
      "venue_full": "IEEE Transactions on Software Engineering",
      "ccf": "A",
      "source_links": [
        "https://ieeexplore.ieee.org/abstract/document/9115238/"
      ],
      "doi": "https://doi.org/10.1109/tse.2020.3001739",
      "cited_by_count": 87,
      "abstract": "Cross-project defect prediction (CPDP), aiming to apply defect prediction models built on source projects to a target project, has been an active research topic. A variety of supervised CPDP methods and some simple unsupervised CPDP methods have been proposed. In a recent study, Zhou <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">et al.</i> found that simple unsupervised CPDP methods (i.e., ManualDown and ManualUp) have a prediction performance comparable or even superior to complex supervised CPDP methods. Therefore, they suggested that the ManualDown should be treated as the baseline when considering non-effort-aware performance measures (NPMs) and the ManualUp should be treated as the baseline when considering effort-aware performance measures (EPMs) in future CPDP studies. However, in that work, these unsupervised methods are only compared with existing supervised CPDP methods using a small subset of NPMs, and the prediction results of baselines are directly collected from the primary literatures. Besides, the comparison has not considered other recently proposed EPMs, which consider context switches and developer fatigue due to initial false alarms. These limitations may not give a holistic comparison between the supervised methods and unsupervised methods. In this paper, we aim to revisit Zhou <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">et al.</i> 's study. To the best of our knowledge, we are the first to make a comparison between the existing supervised CPDP methods and the unsupervised methods proposed by Zhou <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">et al.</i> in the same experimental setting when considering both NPMs and EPMs. We also propose an improved supervised CPDP method EASC and make a further comparison with the unsupervised methods. According to the results on 82 projects in terms of 11 performance measures, we find that when considering NPMs, EASC can achieve prediction performance comparable or even superior to unsupervised method ManualDown in most cases. Besides, when considering EPMs, EASC can statistically significantly outperform the unsupervised method ManualUp with a large improvement in terms of Cliff's delta in most cases. Therefore, the supervised CPDP methods are more promising than the unsupervised method in practical application scenarios, since the limitation of testing resource and the impact on developers cannot be ignored in these scenarios.",
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W3034708533",
      "openalex_score": 1.0
    },
    {
      "title": "A Comprehensive Study of Deep Learning Compiler Bugs.",
      "year": 2021,
      "venue": "FSE",
      "venue_full": "In Proceedings of The ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2021)",
      "ccf": "A",
      "source_links": [],
      "doi": "https://doi.org/10.1145/3468264.3468591",
      "cited_by_count": 104,
      "abstract": "There are increasing uses of deep learning (DL) compilers to generate optimized code, boosting the runtime performance of DL models on specific hardware. Like their traditional counterparts, DL compilers can generate incorrect code, resulting in unexpected model behaviors that may cause catastrophic consequences in mission-critical systems. On the other hand, the DL models processed by DL compilers differ fundamentally from imperative programs in that the program logic in DL models is implicit. As such, various characteristics of the bugs arising from traditional compilers need to be revisited in the context of DL compilers. In this paper, we present the first systematic study of DL compiler bugs by analyzing 603 bugs arising in three popular DL compilers (i.e., TVM from Apache, Glow from Facebook, and nGraph from Intel). We analyzed these bugs according to their root causes, symptoms, and the stages where they occur during compilation. We obtain 12 findings, and provide a series of valuable guidelines for future work on DL compiler bug detection and debugging. For example, a large portion (nearly 20%) of DL compiler bugs are related to types, especially tensor types. The analysis of these bugs helps design new mutation operators (e.g., adding type cast for a tensor to promote implicit type conversion in subsequent tensor computations) to facilitate type-related bug detection. Further, we developed TVMfuzz as a proof-of-concept application of our findings to test the TVM DL compiler. It generates new tests based on TVM's original test suite. They expose 8 TVM bugs that are missed by the original test suite. The result demonstrates the usefulness of our findings. © 2021 ACM.",
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W3196239222",
      "openalex_score": 1.03
    },
    {
      "title": "Automated Query Reformulation for Efficient Search Based on Query Logs from Stack Overflow.",
      "year": 2021,
      "venue": "ICSE",
      "venue_full": "In Proceedings of the 43rd International Conference on Software Engineering (ICSE 2021)",
      "ccf": "A",
      "source_links": [
        "https://ieeexplore.ieee.org/abstract/document/9402151",
        "https://arxiv.org/pdf/2102.00826.pdf",
        "https://github.com/kbcao/sequer",
        "https://xchencs.github.io/awards/2021ICSE2021.pdf"
      ],
      "doi": "https://doi.org/10.1109/icse43902.2021.00116",
      "cited_by_count": 60,
      "abstract": "As a popular Q&A site for programming, Stack Overflow is a treasure for developers. However, the amount of questions and answers on Stack Overflow make it difficult for developers to efficiently locate the information they are looking for. There are two gaps leading to poor search results: the gap between the user's intention and the textual query, and the semantic gap between the query and the post content. Therefore, developers have to constantly reformulate their queries by correcting misspelled words, adding limitations to certain programming languages or platforms, etc. As query reformulation is tedious for developers, especially for novices, we propose an automated software-specific query reformulation approach based on deep learning. With query logs provided by Stack Overflow, we construct a large-scale query reformulation corpus, including the original queries and corresponding reformulated ones. Our approach trains a Transformer model that can automatically generate candidate reformulated queries when given the user's original query. The evaluation results show that our approach outperforms five state-of-the-art baselines, and achieves a 5.6% to 33.5% boost in terms of ExactMatch and a 4.8% to 14.4% boost in terms of GLEU.",
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W3160339882",
      "openalex_score": 1.03
    },
    {
      "title": "SEthesaurus: WordNet in Software Engineering.",
      "year": 2021,
      "venue": "TSE",
      "venue_full": "IEEE Transactions on Software Engineering",
      "ccf": "A",
      "source_links": [
        "https://ieeexplore.ieee.org/abstract/document/8827962",
        "htps://se-thesaurus.appspot.com",
        "htps://pypi.org/project/DomainThesaurus/",
        "htps://github.com/ccywch/se-dict-browser-extension",
        "htps://sedict.github.io/"
      ],
      "doi": "https://doi.org/10.1109/tse.2019.2940439",
      "cited_by_count": 54,
      "abstract": "Informal discussions on social platforms (e.g., Stack Overflow, CodeProject) have accumulated a large body of programming knowledge in the form of natural language text. Natural language process (NLP) techniques can be utilized to harvest this knowledge base for software engineering tasks. However, consistent vocabulary for a concept is essential to make an effective use of these NLP techniques. Unfortunately, the same concepts are often intentionally or accidentally mentioned in many different morphological forms (such as abbreviations, synonyms and misspellings) in informal discussions. Existing techniques to deal with such morphological forms are either designed for general English or mainly resort to domain-specific lexical rules. A thesaurus, which contains software-specific terms and commonly-used morphological forms, is desirable to perform normalization for software engineering text. However, constructing this thesaurus in a manual way is a challenge task. In this paper, we propose an automatic unsupervised approach to build such a thesaurus. In particular, we first identify software-specific terms by utilizing a software-specific corpus (e.g., Stack Overflow) and a general corpus (e.g., Wikipedia). Then we infer morphological forms of software-specific terms by combining distributed word semantics, domain-specific lexical rules and transformations. Finally, we perform graph analysis on morphological relations. We evaluate the coverage and accuracy of our constructed thesaurus against community-cumulated lists of software-specific terms, abbreviations and synonyms. We also manually examine the correctness of the identified abbreviations and synonyms in our thesaurus. We demonstrate the usefulness of our constructed thesaurus by developing three applications and also verify the generality of our approach in constructing thesauruses from data sources in other domains.",
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W2972492919",
      "openalex_score": 1.0
    },
    {
      "title": "MAP-Coverage: a Novel Coverage Criterion for Testing Thread-Safe Classes",
      "year": 2019,
      "venue": "ASE",
      "venue_full": "In Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering (ASE 2019)",
      "ccf": "A",
      "source_links": [
        "https://ieeexplore.ieee.org/abstract/document/8952403/",
        "http://www.novmeet.com"
      ],
      "doi": "https://doi.org/10.1109/ase.2019.00073",
      "cited_by_count": 11,
      "abstract": "Concurrent programs must be thoroughly tested, as concurrency bugs are notoriously hard to detect. Code coverage criteria can be used to quantify the richness of a test suite (e.g., whether a program has been tested sufficiently) or provide practical guidelines on test case generation (e.g., as objective functions used in program fuzzing engines). Traditional code coverage criteria are, however, designed for sequential programs and thus ineffective for concurrent programs. In this work, we introduce a novel code coverage criterion for testing thread-safe classes called MAP-coverage (short for memory-access patterns). The motivation is that concurrency bugs are often correlated with certain memory-access patterns, and thus it is desirable to comprehensively cover all memory-access patterns. Furthermore, we propose a testing method for maximizing MAP-coverage. Our method has been implemented as a self-contained toolkit, and the experimental results on 20 benchmark programs show that our toolkit outperforms existing testing methods. Lastly, we show empirically that there exists positive correlation between MAP-coverage and the effectiveness of a set of test executions.",
      "is_oa": false,
      "oa_url": null,
      "openalex_id": "https://openalex.org/W2999226845",
      "openalex_score": 1.03
    }
  ]
}