EDIT SUM: A Retrieve-and-Edit Framework for
Source Code Summarization
Jia Li ♂
Key Lab of High Conﬁdence Software
Technology, MoE (Peking University)
Beijing, China
lijia@stu.pku.edu.cn
Yongmin Li
Key Lab of High Conﬁdence Software
Technology, MoE (Peking University)
Beijing, China
liyongmin@pku.edu.cn
Ge Li*
Key Lab of High Conﬁdence Software
Technology, MoE (Peking University)
Beijing, China
lige@pku.edu.cn
Xing Hu
School of Software Technology
Zhejiang University, Ningbo, China
xinghu@zju.edu.cn
Xin Xia
Faculty of Information Technology
Monash University, Melbourne, Australia
Xin.Xia@monash.edu
Zhi Jin*
Key Lab of High Conﬁdence Software
Technology, MoE (Peking University)
Beijing, China
zhijin@pku.edu.cn
Abstract—Existing studies show that code summaries help
developers understand and maintain source code. Unfortunately,
these summaries are often missing or outdated in software
projects. Code summarization aims to generate natural language
descriptions automatically for source code. According to Gros
et al., code summaries are highly structured and have repetitive
patterns (e.g. “ return true if... ”). Besides the patternized words,
a code summary also contains important keywords, which are
the key to reﬂecting the functionality of the code. However,
the state-of-the-art approaches perform poorly on predicting the
keywords, which leads to the generated summaries suffer a loss in
informativeness. To alleviate this problem, this paper proposes
a novel retrieve-and-edit approach named EDIT SUM for code
summarization. Speciﬁcally, EDIT SUM ﬁrst retrieves a similar
code snippet from a pre-deﬁned corpus and treats its summary
as a prototype summary to learn the pattern. Then, EDIT SUM
edits the prototype automatically to combine the pattern in the
prototype with the semantic information of input code. Our
motivation is that the retrieved prototype provides a good start-
point for post-generation because the summaries of similar code
snippets often have the same pattern. The post-editing process
further reuses the patternized words in prototype and generates
keywords based on the semantic information of input code. We
conduct experiments on a large-scale Java corpus (2M) and
experimental results demonstrate that EDIT SUM outperforms the
state-of-the-art approaches by a substantial margin. The human
evaluation also proves the summaries generated by EDIT SUM
are more informative and useful. We also verify that EDIT SUM
performs well on predicting the patternized words and keywords.
Index Terms —Code summarization, Information retrieval,
Deep learning
I. I NTRODUCTION
During software development and maintenance, developers
spend around 59% of their time on program comprehension
activities [1]–[3]. A code summary provides a concise natural
language description for a code snippet, which can help
developers understand the program quickly and correctly [4].
Unfortunately, the code summaries are often mismatched,
* Corresponding authors
missing or outdated in the software projects [5]. Additionally,
manually writing summaries during the development is time-
consuming for developers. Therefore, it is important to explore
automatic code summarization approaches.
Traditional approaches generate code summaries based on
the template-based approaches and information retrieval (IR)
based approaches. Template-based approaches [4], [6] ﬁrstly
extract the keywords from the source code, and then ﬁll the
keywords into the predeﬁned templates to generate a code
summary. The IR-based approaches use code summaries of
similar code snippets as outputs directly. Among these IR-
based approaches, they retrieve the similar code snippets by
various similarity metrics [7], [8] from open-source software
repositories in GitHub or software Q&A sites [9], [10].
Although the traditional approaches are simple, they have
achieved good results. This is because code summaries are
highly structured and contain many repetitive patterns, e.g.,
“ return true if... ” and “ create a new... ” [11]. The manually-
crafted templates and retrieved summaries provide a lot of
reusable patternized words, which play an key role in the code
summaries. However, for template-based approaches, manu-
ally deﬁning templates is time-consuming and laborious, and
requires a lot of expert experience. For IR-based approaches,
there may be semantic inconsistencies between the retrieved
summary and the input code.
With the development of deep learning, there is an emerging
interest in applying neural networks for automatic code sum-
marization. Previous studies [12]–[14] often adopt the encoder-
decoder architecture [15] to learn the mapping between words
and even the grammatical structure from source code to
natural language based on the large-scale corpus. By virtue
of the naturalness of the source code [16], [17], these neural
models can mine patterns for generating code summaries
from a large corpus. Besides the patternized words, a code
summary also contains important keywords, which have a low
frequency in training data, but are the key to reﬂecting the
functionality of source code (more details can be found in
Section II). However, the state-of-the-art nerual models [12]–
[14] perform poorly on predicting keywords. For example,
LeClair et al. [14] found 21% summaries written by humans
in the test set contain words with the frequency of less than
100, but only 7% summaries generated by their proposed
approach contain these words. The lack of keywords leads
to the generated summaries suffer a loss in informativeness,
which have a negative impact on program comprehension.
Recently, Wei et al. [18] and Zhang et al. [19] proposed
two retrieval-based neural models to address the problem of
keywords. They used the IR techniques to get the similar
code and its summary, and then input the retrieved results
and the input code into the encoder. With the assistance of
the retrieved summary, their models can accurately generate
patternized words. However, their models only treated the
retrieved results as auxiliary information and don’t solve the
problem of keywords.
In this paper, we propose a novel retrieve-and-edit ap-
proach E DIT SUM for code summarization. The improvement
by template-based approaches proves that the importance of
the patterns in code summaries. The improvement by IR-
based approaches shows that the summaries of similar code
snippets often have the same pattern. So, we treat the summary
of similar code as a prototype and extract the pattern from
the prototype. Considering the inconsistencies between the
prototype and input code, we design a neural network to
further edit the prototype automatically based on the semantic
information of input code. Our motivation is that the pattern
in a prototype tells the neural model “how to say” and the
semantic information of input code tells the neural model
“what to say”.
EDIT SUM consists of two modules: a Retrieve module and
an Edit module. In the Retrieve module, given an input code
snippet, we use IR techniques to retrieve the similar code
snippet from a large parallel corpus and treat the summary of
the similar code snippet as a prototype. Then, the Edit module
generates a summary by fusing the pattern in prototype and
semantic information of input code. Speciﬁcally, we propose
a sequence-to-sequence (seq2seq) neural network to learn to
revise the prototype based on the semantic differences of the
input code and the similar code. To represent the semantic
differences, we calculate an edit vector by concatenating the
weighted sums of insertion word embeddings (words in input
code but not in similar code) and deletion word embeddings
(words in similar code but not in input code). After that, we
revise the prototype summary conditioning on the edit vector
to obtain a new summary.
To evaluate our approach, we conduct experiments on a
real-world Java dataset. The dataset comes from the Sourcerer
repository1 and has been processed by LeClair et al. [14], in-
cluding removing duplicates and dividing into training, valida-
tion, and test sets by projects. We employ the mainstream eval-
uation metric BLEU [20], METEOR [21] and ROUGE [22]
score that are widely used in summary generation task to
1https://www.ics.uci.edu/lopes/datasets/
TABLE I: The patterns of summaries in dataset.
Real Samples
write a test ﬁnish to the mesa logger
write this tilemap to an xml ﬁle
write the buffer to the output stream
write grid data to the geotiff ﬁle
write cdl representation to output stream
Pattern write to
Real Samples
this method sets the help button visible
this method sets the vaule of ﬁeld
this method sets a search argument for list
this method sets the client id
this method sets the range as a double
Pattern this method sets
Real Samples
convert an image to an array of integer
convert this ip packet to a readable string
convert a jingle description to xml
convert the speciﬁed string to a url
convert the date to the given timezone
Pattern convert to
evaluate the generated summaries. Experimental results show
that E DIT SUM performs substantially better than the IR-
based baselines and outperforms the state-of-the-art neural
baselines. The human evaluation and qualitative analysis prove
the summaries generated by E DIT SUM are informative and
useful for developers to understand programs. Besides, we
verify that EDIT SUM not only accurately generates patternized
words, but also generates more keywords.
Our main contributions are outlined as follows:
• We propose a novel retrieve-and-edit approach, namely
EDIT SUM, for code summarization. We use the sum-
maries of similar code snippets as prototypes to assist
in generating summaries.
• We design an effective editing module for summary
generation, which can combine the pattern in prototype
with the semantic information of code.
• We conduct extensive experiments to evaluate our ap-
proach on a large-scale Java dataset. The experimental
results show that E DIT SUM substantially outperforms the
state-of-the-art approaches.
Paper Organization. The rest of this paper is organized
as follows. Section II describes motivating examples. Section
III presents our proposed approach. Section IV and Section
V describe the experimental setup and results. Section VI
and Section VII discuss some results and describe the related
work, respectively. Finally, Section VIII concludes the paper
and points out future directions.
II. M OTIVATING EXAMPLES
A closer look at the code summarization dataset shows that
patterns such as “ creates a new ”, “ returns true if ”, “ load
into”, “ convert into ” are very frequent [11]. Table I shows
some samples from the dataset provided by LeClair et al.
[14]. The bold words are patternized words, and the dashed
words denote the keywords. Such a code summary can be
regarded as composed of patternized words and keywords.
The pattern ensures the readability of the summary, and the
keywords reﬂect the functionality of the source code. A good
InputCode:publicIteratorgetPrefixes(StringnamespaceURI) {Listl= URIMap.get(namespaceURI);return(l == null) ?null:l.iterator();}SimilarCode:publicStringgetPrefix(StringnamespaceURI) {List<String> l= URIMap.get(namespaceURI);return(l == null) ?null:l.get(0);}Rencos(InputCode):returnsaniteratoroverthevaluestoaspecifiedurl.Human-written(Input Code): returnan iterator over all prefixes toa urlHuman-written(SimilarCode): returna prefix correspondingtoa url
Fig. 1: An example of the input code and similar code.
code summary should contains suitable patternized words and
meaningful keywords.
However, previous models perform well on predicting the
patternized word, ignoring the importance of keywords. As
Figure 1 shows, for the input code, we use the open-source
search engine Lucene2 to retrieve the most similar code
snippet from the training corpus. The retrieval metric is based
on the lexical level similarity of the source code.
In Figure 1, the summaries of input code and similar
code have the same pattern ( return...to a url ), but there are
semantic differences between the similar code and input code.
Although the two Java methods are lexically similar, the input
code returns all preﬁxes, while the similar code returns a
certain preﬁx. In Figure 1, the state-of-the-art neural model
Rencos [19] can correctly predict the patternized words (e.g.,
return, to), but it performs poorly on keywords (e.g., preﬁxes).
The code summaries generated by Rencos achieve high scores
on the patternized words, but they do not clearly express the
purposes of the programs.
In this paper, we address that both pattern and keywords are
important for a code summary. Inspired by previous studies,
we propose a retrieve-and-edit approach by combining the
pattern in existing summaries and the semantic information
of input code to generate informative summaries with suitable
patterns.
III. P ROPOSED APPROACH
In this paper, we propose a retrieve-and-edit approach
named E DIT SUM for source code summarization, which can
combine the strengths of traditional approaches and neural
models. The overall framework of our model is shown in
Figure 2. Our approach E DIT SUM consists of a Retrieve
module and an Edit module and generates a summary in three
steps:
Step 1: Selecting a suitable prototype summary. We use
a massive training set as the retrieval corpus. Given an input
code, the Retrieve module uses the search engine to search for
the similar code-summary pair from the corpus. The retrieval
process is explained in Section III-A.
Step 2: Extracting the semantic information of the input
code. In Figure 2, we mark the lexical differences between the
2https://lucene.apache.org/
two Java methods. We ﬁnd that the different words between
the two methods reﬂect their semantic differences to a certain
extent, such as “Iteration” vs “String”, and “Preﬁxes” vs
“Preﬁx”. Therefore, we calculate an edit vector based on the
lexical differences between similar code and input code to
represent their semantic differences. The details of this part is
described in Section III-B.
Step 3: Combining the pattern in prototype with semantic
information of input code. To this end, we design a neural
edit module to revise the prototype based on the semantic
differences between the input code and similar code. The
details is presented in Section III-B.
A. Retrieve Module
In our approach, the Retrieve module aims to retrieve the
similar code-summary pair from a corpus given the input
code. Inspired by previous studies [18], [19], we choose the
lexical-level similarity as retrieval metric. Speciﬁcally, we
adoptBM 25 [23] as the similarity evaluation metric, which is
a bag-of-words retrieval function to estimate the relevance of
documents to a given query. Given a query and a document,
based on TF-IDF [24], the BM 25 function calculates the term
frequency in the document of each keyword in the query and
multiplies it by the inverse document frequency of this term.
The more relevant two documents have, the higher the value
of BM 25 score. We leverage the open-source search engine
Lucene to build the Retrieve module. Since the size of the
training set is quite large (over 1.9M), we use it as the retrieval
corpus. We ﬁrst tokenize the source code and summaries and
process each code and summary pair into a document, add it
to the index library, and store it on disk.
As shown in Figure 2, we use different strategies to select
prototypes for training and testing. In testing, we search for
the most similar code from the training set and treat its
summary as the prototype. During the training phase, as we
already know the targrt summary, we ﬁrst retrieve top-20 code-
summary pairs based on the summary similarity. Then, we
reserve the retrieved summaries as prototypes whose Jaccard
similarity [25] to target summary in the range of [0.3, 0.7].
The Jaccard similarity measures text similarity from a bag-
of-words view, that is formulated as
J(A,B ) = |A ∩B|
|A ∪B| (1)
where A and B are two bags of words and | · | denotes the
number of elements in a collection. The motivation behind
ﬁltering out summaries with Jaccard similarity < 0.3 is the
edit module performs well only if a prototype is lexically
similar to its target summary [26]. Besides, we hope the edit
module does not copy the prototype so we discard summaries
where the prototype and target summary are nearly identical
(i.e.Jaccard similarity> 0.7). We do not use code similarity
to construct training data, because similar code snippets may
correspond to totally different summaries. This is not con-
ducive to our model learning how to revise a prototype. The
preliminary experiments also show that constructing training