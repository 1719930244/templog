IEEE TRANSACTIONS ON SOFTW ARE ENGINEERING, VOL. 50, NO. 12, DECEMBER 2024 3435
Fight Fire With Fire: How Much Can We Trust
ChatGPT on Source Code-Related Tasks?
Xiao Yu , Lei Liu ,X i n gH u , Jacky Wai Keung , Senior Member, IEEE , Jin Liu ,
and Xin Xia , Senior Member, IEEE
Abstract—With the increasing utilization of large language
models such as ChatGPT during software development, it has
become crucial to verify the quality of code content it generates.
Recent studies proposed utilizing ChatGPT as both a developer
and tester for multi-agent collaborative software development.
The multi-agent collaboration empowers ChatGPT to produce
test reports for its generated code, enabling it to self-verify the
code content and ﬁx bugs based on these reports. However,
these studies did not assess the effectiveness of the generated
test reports in validating the code. Therefore, we conduct a
comprehensive empirical investigation to evaluate ChatGPT’s
self-veriﬁcation capability in code generation, code completion,
and program repair. We request ChatGPT to (1) generate correct
code and then self-verify its correctness; (2) complete code
without vulnerabilities and then self-verify for the presence
of vulnerabilities; and (3) repair buggy code and then self-
verify whether the bugs are resolved. Our ﬁndings on two
code generation datasets, one code completion dataset, and two
program repair datasets reveal the following observations: (1)
ChatGPT often erroneously predicts its generated incorrect code
as correct, its vulnerable completed code as non-vulnerable, and
its failed program repairs as successful during its self-veriﬁcation.
(2) The self-contradictory hallucinations in ChatGPT’s behavior
arise: (a) ChatGPT initially generates code that it believes to
be correct but later predicts it to be incorrect; (b) ChatGPT
initially generates code completions that it deems secure but later
predicts them to be vulnerable; (c) ChatGPT initially outputs
code that it considers successfully repaired but later predicts it
to be buggy during its self-veriﬁcat ion. (3) The self-veriﬁcation
Received 20 April 2024; revised 21 October 2024; accepted 27 October
2024. Date of publicatio n 5 November 2024; date of current version 12 De-
cember 2024. This work was supported in part by the National Natural
Science Foundation of China under Grant 61972290, in part by Ningbo
Natural Science Foundation under Grant 2023J292, and in part by the General
Research Fund of the Research Grants Council of Hong Kong and the research
funds of the City University of Hong Kong under Grant 6000796, Grant
9229109, Grant 9229098, Grant 9220103, and Grant 9229029. Recommended
for acceptance by F. Ferrucci. (Corresponding author: Xing Hu.)
Xiao Yu and Xing Hu are with the State Key Laboratory of Blockchain
and Data Security, Zhejiang Unive rsity, Hangzhou, Zhejiang 310058, China
(e-mail: xiaoyu_cs@hotmail.com; xinghu@zju.edu.cn).
Lei Liu is with the Faculty of Electr onic and Information Engineering,
Xi’an Jiaotong University, Xi’an , Shanxi 710049, China (e-mail: Lei.Liu@
stu.xjtu.edu.cn).
Jacky Wai Keung is with the Depar tment of Computer Science, City
University of Hong Kong, Hong Kong 999077, China (e-mail: jacky.keung@
cityu.edu.hk).
Jin Liu is with the School of Computer Science, Wuhan University, Wuhan
430072, China (e-mail: jin liu@whu.edu.cn).
Xin Xia is with the College of Computer Science and Technology, Zhejiang
University, Hangzhou 310058, China (e-mail: xin.xia@acm.org).
Digital Object Identiﬁ er 10.1109/TSE.2024.3492204
capability of ChatGPT can be enhanced by asking the guiding
question, which queries whether ChatGPT agrees with assertions
about incorrectly generated or r epaired code and vulnerabilities
in completed code. (4) Using test reports generated by ChatGPT
can identify more vulnerabilities in completed code, but the
explanations for incorrectly generated code and failed repairs are
mostly inaccurate in the test rep orts. Based on these ﬁndings, we
provide implications for further research or development using
ChatGPT.
Index Terms —Empirical study, ChatGPT, self-veriﬁcation,
code generation, code completion, program repair.
I. I NTRODUCTION
L
ARGE L anguage M odels (LLMs), especially the high-
performing ChatGPT have demonstrated impressive ca-
pabilities across various software development tasks, including
code generation [1], [2], [3], [4], code completion [5], [6], and
program repair [7], [8]. These capabilitie s accelerate develop-
ment processes and simplify daily tasks for software developers.
However, ChatGPT-generated code may have quality issues or
vulnerabilities [9], [10], emphasizing the need for thorough
quality checks. Recently, researchers[1], [3] proposed utilizing
multi-agents where ChatGPT act s both as a developer and a
tester. This approach enables ChatGPT to generate test reports
for its generated code and ﬁx bugs based on the reports. How-
ever, they did not evaluate whether the generated test reports
effectively validate the code (i.e., ChatGPT’s self-veriﬁcation
capability). Therefore, we conduct a comprehensive empirical
study to evaluate ChatGPT’s self-veriﬁcation capability across
three code-related tasks (i.e., code generation, code comple-
tion, and program repair) using the three speciﬁcally designed
prompts: direct question, guiding question, and test report.
We address the following three R
esearch Questions (RQs):
RQ1: How effective is ChatGPT’s self-veriﬁcation ca-
pability in code generation using the direct question,
guiding question, and test report prompts? We ﬁrst ask
ChatGPT to generate code based on the requirement description
and then verify if the code meets the requirements. During self-
veriﬁcation, we use the direct question prompt to evaluate if
the code correctly implements the requirements, the guiding
question prompt to agree or disagree with assertions that the
code does not implement the function based on the requirement
description, and the test report prompt to generate test reports
for the generated code to verify correctness.
0098-5589 © 2024 IEEE. Personal use is permitted, but repub lication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Zhejiang University. Downloaded on August 26,2025 at 02:43:14 UTC from IEEE Xplore.  Restrictions apply. 
3436 IEEE TRANSACTIONS ON SOFTW ARE ENGINEERING, VOL. 50, NO. 12, DECEMBER 2024
RQ2: How effective is ChatGPT’s self-veriﬁcation ca-
pability in code completion u sing the direct question,
guiding question, and test report prompts? We ask Chat-
GPT to complete the code and ens ure it has no vulnerabilities,
then question ChatGPT about any potential vulnerabilities in
the completed code. During self-veriﬁcation, we use the direct
question prompt to explicitly as k if the code correctly imple-
ments the requirement description, the guiding question prompt
to ask for agreement or disagreement with assertions that the
completed code has vul nerabilities, and the test report prompt
to generate a test report for the completed code to self-verify
any vulnerabilities.
RQ3: How effective is ChatGPT’s self-veriﬁcation ca-
pability in program repair u sing the direct question,
guiding question, and test report prompts?We ask ChatGPT
to repair a buggy program and then question if the code is
successfully repaired. During self-veriﬁcation, we use the direct
question prompt to explicitly ask if the repaired code correctly
implements the function, the guiding question prompt to ask
for agreement or disagreement with assertions that the repaired
code does not correctly implement the function, and the test
report prompt to generate a test report for the repaired code to
self-verify the success of the repair process.
We conduct experiments on two code generation datasets,
one code completion dataset, and two program repair datasets.
The experiment results are as follows:
(1) ChatGPT possesses a cer tain level of capability in gen-
erating correct code with an average success rate of 57%,
providing code completions without vulnerabilities with a suc-
cess rate of 73%, and successfully repairing code with an
average success rat e of 70%. When explicitly asking about
the correctness of the generated code, absence of vulnera-
bilities in code completions, o r the success of code repairs,
ChatGPT often erroneously believes that it has accomplished
these tasks, with average error rates of 39%, 25%, and 28%,
respectively.
(2) The guiding question prompt leads to the detection of
an average of 25% more incorrectly generated code, the iden-
tiﬁcation of 69% more vulner abilities in com pleted code, and
the recognition of an average of 33% more failed program
repairs. However, it is important to acknowledge that despite
these improvements, there are still many cases where ChatGPT
is unable to successfully self-verify incorrectly generated code
(67% average missing report rate), vulnerabilities in completed
code (23% missing report rate), and failed program repairs
(59% average missing report rate).
(3) Utilizing the test report p rompt enables ChatGPT to
successfully identify an average of 77% more vulnerable com-
pleted code and provide accurate explanations for the vulner-
abilities. For the program repair t ask, the test report prompt
can identify an average of 28% more failed program repairs.
However, in the code generation task, the test report prompt
does not improve the detection of generated incorrect code
substantially. Furthermore, the explanations provided in the test
report are mostly (an average of 75%) inaccurate for incorrectly
generated code and failed repairs.
(4) There are instances of self-contradictory hallucinations
1
in ChatGPT’s behavior: (a) Initially, ChatGPT generates or
completes code that it believes to be correct and non-vulnerable,
but it predicts it to be incorrect and vulnerable during self-
veriﬁcation. (b) ChatGPT initially outputs code that it believes
to be successfully repaired, but it p redicts it to fail during self-
veriﬁcation.
Overall, the inaccuracies and self-contradictory hallucina-
tions observed during ChatGPT’s self-veriﬁcation highlight the
crucial role of human expertise and judgment in software de-
velopment and evaluation in the current stage. ChatGPT should
be seen as a tool to assist deve lopers rather than a substitute
for their role as autonomous software developers and testers.
Developers must carefully evaluate the output of ChatGPT,
and conduct their own assessment s to ensure the quality and
reliability of the generated code. Furthermore, efforts to en-
hance the performance of ChatGPT should focus on eliminat-
ing self-contradictory hallucinations to ensure a more reliable
experience.
Our contributions are summarized as follows:
(1) To the best of our knowledge, we are the ﬁrst to perform a
comprehensive empirical study to examine the self-veriﬁcation
capability of ChatGPT in code-rela ted tasks, i.e., code genera-
tion, code completion, and program repair.
(2) We make actionable ﬁndings regarding the self-
veriﬁcation performance of ChatGPT and provide implications
for the adoption and development of ChatGPT.
II. S
TUDY DESIGN
Given ChatGPT’s primary focus on content generation, we
evaluate its performance on three code-related tasks: code gen-
eration, code completion, and program repair. These tasks are
widely used in daily software development and involve exten-
sive code creation. It is crucial to ensure that the generated code
is free from vulnerabilities or bugswhen developers incorporate
it into their projects. For the code generation and program
repair tasks, we assess the correctness of the generated code
using the provided test cases within the experimental datasets.
Any code that fails to pass a test case is considered incorrect
[12]. In the code generation task, code that successfully passes
all test cases is deemed correct. For the program repair task,
automatic program repair techniques may suffer from the patch
overﬁtting problem [13], where a repaired program passes all
the tests but is still incorrect. Therefore, apart from assessing if
the repaired code can pass all test cases, we engage three soft-
ware developers with over ﬁve years of experience to conduct
independent evaluations. Each developer thoroughly reviews
the repaired program to conﬁrm the successful ﬁxing of the
buggy code. For the code completion task, we utilize the GitHub
CodeQL [14] tool to scan the completed code for vulnerabilities
associated with speciﬁc CWEs, as outlined by Pearce et al. [9].
1In the context of LLMs, “hallucination” refers to the phenomenon where
LLMs produce text that is incorrect, nonsensical, or fabricated. Mündler et al.
[11] deﬁne “self-contradictory hallucinations” as instances where an LLM pro-
duces two logically inconsistent senten ces within the same context. We have
adopted Mündler et al.’s deﬁnition of s elf-contradictory hallucinations.
Authorized licensed use limited to: Zhejiang University. Downloaded on August 26,2025 at 02:43:14 UTC from IEEE Xplore.  Restrictions apply. 
YU et al.: FIGHT FIRE WITH FIRE: HOW MUCH CAN WE TRUST C HATGPT ON SOURCE CODE-RELATED TASKS? 3437
After the automated scan, the same three software develop-
ers manually inspect the vul nerabilities ﬂagged by CodeQL
to verify their correctness. This manual inspection serves two
purposes: verifying the accuracy of the CodeQL results and
correcting any false positives or overlooked vulnerabilities. To
further ensure the reliability of our evaluations and reduce bias,
we calculate the Fleiss’ kappa score, which is 0.81, indicating
substantial agreement among the three developers. In cases of
disagreement, the three developers discuss the issues to reach
a consensus, thereby mitiga ting individual biases.
A. Code Generation
Datasets. We select the two widely used datasets that con-
tain test cases, namely, MBXP [15] and HumanEval-X [16].
The MBXP dataset [15] consists of 848-974 coding prob-
lems for 13 programming languages. Each problem includes
task_id, declaration, docstring, prompt, canonical_solution, and
test program with 3 test cases. The HumanEval-X dataset [16]
consists of 820 human-crafted problem-solution pairs covering
164 coding problems in ﬁve languages. Each problem includes
the task_id, prompt, declaration, canonical_solution, and test
program with some test cases.
Step 1: We ﬁrst request ChatGPT to generate the correct code
based on the provided requirement description. We design the
prompt consisting of triple items, i.e., <requirement, function
description, function declaration>. Test cases are not included
in the prompt, since Cassano et al. [12] suggested that it is a
better way to evaluate code generation. For instance, in Fig. 1,
the requirement is “ Generate the code and ensure that the
generated code is correct.”, the function description is “Write a
JavaScript function to ﬁnd the element that appears only once
i nas o r t e da r r a y .”, and the function declaration is “ function
search(arr, n){”.
Step 2 (a) : After generating the code, we utilize the direct
question prompt to explicitly ask ChatGPT whether the code
correctly implements the function based on the requirement
description and requests a direct response. The direct question
prompt shown in Fig. 1 is “ Does the code correctly imple-
ment ‘search’ function based on the description of ‘Write
a JavaScript ... sorted array. ’? Answer the question (yes or
no): $code$”, where $code$ represents the generated code by
ChatGPT.
Step 2 (b) : In addition, we introduce the guiding question
prompt to guide ChatGPT’s responses. This prompt asks Chat-
GPT whether it agrees with the assertion that the code does not
implement the function based on the requirement description
correctly. The guiding question prompt shown in Fig.1 is “The
code does not implement ‘search’ function based on the
description of ‘Write a JavaScript ... sorted array. ’ correctly.
If true, return ‘Yes’ and provide an explanation. Otherwise,
return ‘No’. $code$” .
Step 2 (c) : We also propose the utilization of the test report
prompt to ask ChatGPT to generate a test report to self-verify
the correctness of the generated code. The test report prompt
shown in Fig.1 is “Generate a test report based on the function
declaration ‘function search(arr, n)’ and description ‘Write a
Fig. 1. The designed self-veriﬁ cation prompts for code generation.
JavaScript ... sorted array. ’ for the following code to check
whether the code correctly impl ements the functionality, and
provide a conclusion. $code$” .
B. Code Completion
Datasets. Pearce et al. [9] discovered that the majority of
code generated by Codex 2 for security scenarios. Hence, our
objective is to investigate whether ChatGPT can detect such
security ﬂaws in its completed code. To achieve this, we employ
the same dataset introduced by Pearce et al. [9], which fea-
tures intentionally designed completion scenarios for a subset
of MITRE’s C
ommon Weakness Enumerations (CWEs) listed
2Both ChatGPT and Codex are LLMs developed by OpenAI. ChatGPT is
geared toward natural language conv ersations, while Codex is specialized in
code generation and understanding. C odex has been discontinued and is no
longer actively used. As a result, res earchers have shif ted their attention to
utilizing the API of ChatGPT for th eir research and development.
Authorized licensed use limited to: Zhejiang University. Downloaded on August 26,2025 at 02:43:14 UTC from IEEE Xplore.  Restrictions apply.