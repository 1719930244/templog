CoLeFunDa: Explainable Silent Vulnerability Fix
Identification
Jiayuan Zhou ∗, Michael Pacheco ∗, Jinfu Chen ∗, Xing Hu †‡, Xin Xia ∥, David Lo § and Ahmed E. Hassan ¶
∗ Centre for Software Excellence, Huawei, Toronto, Canada
† School of Software Technology, Zhejiang University, Ningbo, China
∥ Huawei, China
§ School of Information Systems, Singapore Management University, Singapore
¶ Software Analysis and Intelligence Lab (SAIL), Queen’s University
{jiayuan.zhou1,michael.pacheco1,jinfu.chen1}@huawei.com,xinghu@zju.edu.cn, xin.xia@acm.org,
davidlo@smu.edu.sg, ahmed@cs.queensu.ca
Abstract—It is common practice for OSS users to leverage
and monitor security advisories to discover newly disclosed
OSS vulnerabilities and their corresponding patches for vulner-
ability remediation. It is common for vulnerability fixes to be
publicly available one week earlier than their disclosure. This
gap in time provides an opportunity for attackers to exploit
the vulnerability. Hence, OSS users need to sense the fix as
early as possible so that the vulnerability can be remediated
before it is exploited. However, it is common for OSS to adopt
a vulnerability disclosure policy which causes the majority of
vulnerabilities to be fixed silently, meaning the commit with the
fix does not indicate any vulnerability information. In this case
even if a fix is identified, it is hard for OSS users to understand
the vulnerability and evaluate its potential impact. To improve
early sensing of vulnerabilities, the identification of silent fixes
and their corresponding explanations (e.g., the corresponding
common weakness enumeration (CWE) and exploitability rating)
are equally important.
However, it is challenging to identify silent fixes and provide
explanations due to the limited and diverse data. To tackle this
challenge, we propose CoLeFunDa: a framework consisting of a
Contrastive Learner and FunDa, which is a novel approach for
Function change Data augmentation. FunDa first increases the fix
data (i.e., code changes) at the function level with unsupervised
and supervised strategies. Then the contrastive learner leverages
contrastive learning to effectively train a function change encoder,
FCBERT, from diverse fix data. Finally, we leverage FCBERT
to further fine-tune three downstream tasks, i.e., silent fix identi-
fication, CWE category classification, and exploitability rating
classification, respectively. Our result shows that CoLeFunDa
outperforms all the state-of-art baselines in all downstream
tasks. We also conduct a survey to verify the effectiveness of
CoLeFunDa in practical usage. The result shows that CoLeFunDa
can categorize 62.5% (25 out of 40) CVEs with correct CWE
categories within the top 2 recommendations.
I. I NTRODUCTION
With the prevalent use of open-source software (OSS),
OSS users must manage OSS vulnerabilities (e.g., sensing
and fixing vulnerabilities) in time. Otherwise, OSS users
will be exposed to large security risks which may lead to
significant consequences. For example, with a late fix to
the CVE-2017-5638 vulnerability [1], Equifax, one of the
largest credit reporting agencies, suffered from a data breach,
‡Corresponding author.
resulting in a loss of more than $650 million [2]. For better
OSS vulnerability management, the Coordinated Vulnerability
Disclosure (CVD) process [3] is widely adopted [4], [5], [6],
[7], [8]. Following this process, a vulnerability should be
fixed first and then disclosed publicly. This allows OSS users
to start the remediation process immediately since the patch
is already available once the vulnerability is disclosed. To
limit the degree of dissemination of information related to the
vulnerability, CVD also suggests that vulnerabilities should
be fixed silently, for example, the commit message should not
carry any information that may reveal the vulnerability.
Due to various reasons (e.g., the limited human resource
or the long fix-to-integration release cycle [9]), the timing
of public disclosure does not closely align with the fixed
date. This time gap could vary from days to months (more
than one week in median [9], [10]). For example, Log4Shell
(CVE-2021-44228 [11]) is a vulnerability in Apache Log4J, a
Java logging framework. An adversary can utilize Log4Shell
to take complete control over the system by sending crafted
requests, making the vulnerability easily exploitable. Given
the popular use of Log4J in Java systems and the ease of
its exploitability, Log4Shell is considered one of the most
dangerous vulnerabilities. Log4Shell was first disclosed on
Dec. 10, 2021 [11]. However, the corresponding fix [12]
publicly existed 11 days earlier (Nov. 29, 2021). Such a time
gap may provide a window of opportunity for exploitation,
causing OSS users to be exposed to huge security risks
during the time gap. Given the transparent nature of OSS,
the malicious parties could easily uncover the fix and derive
the corresponding vulnerability for developing and deploying
exploits in advance. Hence, OSS users must sense silent fixes
as early as possible to start the remediation process as soon
as possible.
Moreover, we argue that identifying silent vulnerability fixes
is just the first step and that an explanation of them is also
important. The reason is that OSS users may not be experts on
every OSS they use, making it challenging to understand and
analyze silently fixed vulnerabilities. For example, “Restrict
LDAP access via JNDI” is the commit message of the fix [12]
for Log4Shell. Since no vulnerability information is provided
1
(e.g., no security keywords), it is hard for general OSS users
to understand the fixed vulnerability. Due to such limited
information, even if a tool could identify this fix, OSS users
might ignore the fix because of the misunderstanding, making
such an early warning ineffective. Hence, after receiving an
alert of a silent fix, providing basic yet important information
(e.g., the CWE category and the exploitability rating) of the
corresponding fixed vulnerability could help OSS users to
understand and evaluate the impact of the vulnerability.
Providing explanations for silent fixes is another challenge
because of limited and diverse data. A previous study [10]
showed that the median percentage of vulnerability fixes in
OSS is only 0.35%. Moreover, fixed vulnerabilities are associ-
ated with a wide range of CWE categories [13], indicating the
diverse causes, behaviors, and consequences of vulnerabilities.
Given the extremely imbalanced class distribution combined
with diverse patterns of the fix data, it is difficult for traditional
machine learning and deep learning approaches to effectively
learn information from the data. The current state-of-the-
art, VulFixMiner [10], utilizes the added and removed code
snippets from entire commits to identify silent fixes. As
vulnerability fixing commits can contain mixed information
from the whole commit, and along with the lack of code
context information, it is hard for VulFixMiner to provide
explanations for diverse fixes.
To tackle these challenges, we propose a framework, CoLe-
FunDa (Figure 1), which consists of a contrastive learner
and a novel function change data augmentation component,
FunDa. FunDa first increases the fix data (i.e., code changes)
at the function-level. Then the contrastive learner [14], [15]
effectively learns the representations of the diverse fix data
by minimizing the distance between positives (i.e., similar
data representations) and maximizing the distance between
negatives (i.e., dissimilar data representations). More specifi-
cally, FunDa combines program slicing techniques [16], [17],
[18], [19], [20] and CWE category information to augment
function changes with unsupervised (i.e., the self-based) and
supervised (i.e., the group-based) strategies (Phase 1). Next,
the contrastive learner learns function-level code change rep-
resentations from the diverse fix data and trains the function
change encoder FCBERT (Phase 2). Based on the pre-trained
encoder, CoLeFunDa is then fine-tuned creating three models,
namely CoLeFunDa f ix, CoLeFunDa cwe, and CoLeFunDa exp,
for three downstream tasks, i.e., automated silent fix identifi-
cation, CWE category classification, and exploitability rating
classification, respectively (Phase 3).
We conduct our experiments on 1,436 Java CVE patches
from 310 OSS projects. For the silent fix identification tasks,
CoLeFunDaf ix is evaluated under a practical setting (i.e., the
class distribution of the fixes is extremely imbalanced). The
evaluation result shows that CoLeFunDa f ix outperforms the
state-of-the-art baseline VulFixMiner from 11% to 14% in
terms of all effort-aware performance metrics (i.e., CostEf-
fort@5%, CostEffort@20%, and P opt), indicating the effec-
tiveness of CoLeFunDa f ix in identifying more vulnerabilities
with less manual inspection effort. In the CWE classification
task, the evaluation result shows that CoLeFunDa cwe outper-
forms the best SOTA baseline by 6% to 72% in terms of macro
AUC, macro precision, macro recall, and macro F1 score.
For the exploitability rating classification task, the evaluation
result shows that CoLeFunDa exp outperforms the best SOTA
baseline by 24% to 54% in terms of macro AUC, macro
precision, macro recall, and macro F1 score.
Since not every CVE is well maintained, some CVEs do
not contain CWE category information (i.e. no-CWE CVEs).
To help OSS users better understand and evaluate the potential
risk of no-CWE CVEs, it is also practical to recommend the
corresponding CWE categories. We apply CoLeFunDa cwe on
the patches of 40 no-CWE CVEs and conduct a user study
with 5 security experts. The result shows that the CWE of
37.5% (62.5%) of CVEs are correctly categorized within the
top one (two) recommendations, indicating the effectiveness
of CoLeFunDa cwe in practical usage.
In summary, this paper makes the following contributions:
(1) We advocate the importance of explainable vulnerability
silent fix identification for better OSS vulnerability man-
agement. We propose a framework, CoLeFunDa, which sig-
nificantly outperforms all state-of-the-art baselines in three
explainable silent fix identification tasks.
(2) To the best of our knowledge, FunDa is the first approach
for function-level code change data augmentation. Specifically,
FunDa provides an unsupervised strategy for data augmenta-
tion that can be applied for large-scale general commit data
directly.
(3) To the best of our knowledge, we propose the first unsuper-
vised solution for function-level code change representation
learning. The solution can be applied for training general
function-level code change representation encoders, which is
important for many software tasks (e.g., just-in-time defect
prediction and commit message generation).
(4) We release the vulnerability fix dataset with the enhanced
CVE information (e.g., the CWE categories and exploitability
ratings) of our study [21].
II. B ACKGROUND
In this section, we briefly introduce contrastive learning,
Common Vulnerabilities and Exposures (CVE), Common
Weakness Enumeration (CWE), Common Vulnerability Scor-
ing System (CVSS) and Exploitability Metrics.
A. Contrastive learning
Contrastive learning is widely used in Computer Vision [15]
(CV) and Natural Language Processing domains [22] (NLP).
The key characteristic of contrastive learning is data aug-
mentation, which generates new data from existing data. By
applying augmentation on a data point, two samples that are
different but semantically similar are generated. Contrastive
learning then tries to learn similar knowledge within the
samples from the same data points, and learn the differences
between samples generated from different data points. In
the NLP domain, data augmentation commonly consists of
manipulation of tokens (e.g., token reordering and similar
2
KnowledgeTransfer
CoLeFunDaFix
CoLeFunDaExp
CoLeFunDaCWEFCBERT
+-
Pre-trained Function Change EncoderFine-tuned modelsHistorical Fixes PositiveFCSample Pair
Contrastive LearnerFunDA FF
FC
Commit DataFix DataPhase 1: Function ChangeData Augmentation
EXPEncoderCWEEncoderFixEncoder
CFPhase 2: Function Change Representation LearningPhase 3: Downstream Task Fine-tuning
Fig. 1: Overall framework of CoLeFunDa.
OriginalFunctionModifiedFunction
Function Change Description GeneratorFCDesc
Ori/Mod FSlice
Function Change Augmentor
Original/ModifiedFunction Slice (FSlices)
Function Change Description
Augmented Function Change Sample (FCSamples)Correlated Sample Pair ConstructorSame func. ASame CWE-X
Positive Sample pairs
Diffentfunc.Same CWE-X
Same func. BSame CWE-Y
… …CWE Category
①①
②②③③④③
④
④
Self-based StrategyGroup-based Strategy
OriFSlice ModFSlice FCDescChanged-variable-based Function Slicer
Fig. 2: The workflow of Function Change Data Augmentation (FunDa) in Phase 1.
token replacement). In the software engineering domain, prior
studies focus on data augmentation of source code based on
approaches from NLP. This includes sampling or augmentation
strategies based on a compilation mechanism to generate
source code samples [23] (e.g., code compression, identifier
modification, regularization). These approaches achieve good
performance in source code representation learning.
B. Common Vulnerabilities and Exposures (CVE) and com-
mon weakness enumeration (CWE)
Common Vulnerabilities and Exposure (CVE) database pro-
vides a reference-method for the disclosure, identification, and
management of publicly known vulnerabilities. NVD [24] is
a popular CVE database that provides enhanced vulnerabil-
ity information such as CWE. CWE provides a dictionary
of common weaknesses that can result in vulnerabilities in
software or hardware. They include various details regarding
several types of vulnerabilities. A CWE can be assigned to
CVEs, providing a way to categorize and provides additional
information about CVEs and their corresponding vulnerability.
CVEs can be assigned multiple CWEs depending on the nature
of the vulnerability, however CVEs without any assigned
CWEs exist in NVD.
C. CVSS and Exploitability Metrics
CVSS helps define and categorize vulnerabilities based on
their potential impact and risk. There are two CVSS versions,
i.e., CVSS 2.0 and 3.0. CVSS version 3.0 was released in
2015, and is used for CVEs disclosed from then on. CVEs
disclosed before 2015 have a CVSS 2.0 score [25] instead of
the CVSS 3.0 version. Therefore, we use CVSS version 2.0
in our study. Exploitability is one of the base group metrics
in CVSS, which is used to measure the risk of a vulnerabil-
ity being exploited. The more easily a vulnerability can be
exploited, the higher its exploitability score is. Therefore, the
exploitability metric is valuable for practitioners to prioritize
and fix the vulnerability.
III. P ROPOSED APPROACH
The goal of CoLeFunDa is to learn function-level code
change representations from diverse and limited vulnerabil-
ity fixes to support explainable vulnerability silent fix early
sensing. In this section, we first introduce the overall frame-
work of CoLeFunDa (Figure 1). We then elaborate on the
details of each phase. Finally, we explain the applications of
CoLeFunDa.
CoLeFunDa has three phases: function change data aug-
mentation, function change representation learning, and down-
stream task fine-tuning. In phase 1, we propose a novel
approach, FunDa, to increase the amount of the vulnerability
fix patch data. Specifically, for one function change from a
patch, we augment it into a set of semantics-preserving func-
tion change samples ( FCSamples). We then consider every
two semantically-similar or functionality-similar FCSamples
as a positive pair for contrastive learning in the next phase.
In phase 2, we leverage contrastive learning to further pre-
train a language model to recognize similar and dissimilar
FCSamples and to generate the function change encoder,
FCBERT. In phase 3, we leverage FCBERT to further fine-
tune a silent fix identification model (CoLeFunDaf ix), a CWE
classification model ( CoLeFunDacwe), and an exploitability
rating classification model ( CoLeFunDaexp), respectively.
A. Phase 1: Function Change Data Augmentation
Given the limited amount of vulnerability fixes in the dataset,
it is a challenge to effectively learn the representations of func-
tion changes of the fixes, especially since the CWE categories
of the corresponding fixes are diverse (see Section IV-B).
To tackle this challenge, we propose FunDa for augmenting
function change data. As shown in Figure 2, the workflow of
our approach contains four steps. In step 1, for each function
change, we generate function slices for original and modified
functions (i.e., OriFSlices and ModFSlices), respec-
tively. In step 2, we generate the function change description
(FCDesc) for each function change. In step 3, we generate
FCSamples by combining the FCDesc, OriFSlices, and
3