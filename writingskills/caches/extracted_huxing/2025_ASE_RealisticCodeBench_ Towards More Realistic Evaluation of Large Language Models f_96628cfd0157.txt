RealisticCodeBench: Towards More Realistic
Evaluation of Large Language Models for Code
Generation
Xiao Y u1, Haoxuan Chen 2, Lei Liu 3, Xing Hu 1, Jacky Wai Keung 4, and Xin Xia 1*
1The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China,
xiao.yu@zju.edu.cn, xinghu@zju.edu.cn, xin.xia@acm.org
2School of Computer Science and Artiﬁcial Intelligence, Wuhan University of Technology, Wuhan, China,
haoxuan.chen@whut.edu.cn
3Faculty of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China, lei.liu@stu.xjtu.edu.cn
4Department of Computer Science, City University of Hong Kong, Hong Kong, China, jacky.keung@cityu.edu.hk
Abstract—Evaluating the code generation capabilities of
Large Language Models (LLMs) remains an open question.
Recently, more advanced benchmarks—such as CoderEval,
EvoCodeBench, and ClassEval—have been introduced to evaluate
LLMs on practical coding tasks from GitHub repositories, such
as non-standalone function generation and class-level code gener-
ation. However, even the most sophisticated LLMs struggle with
these complex tasks; for instance, GPT-4 achieves only a 37.0%
pass@1 on ClassEval. Prior studies show that developers often
discard LLM-generated code or abandon code generation models
when outputs are incorrect or require extensive debugging, which
leads them to rely on LLMs primarily for code generation tasks
that high-performing models can reliably handle.
In response to this gap, we introduce RealisticCodeBench, a
benchmark speciﬁcally designed to reﬂect the types of problems
developers commonly tackle with LLMs. By mining GitHub
repositories for code samples tagged as generated by ChatGPT
or Copilot, we collect real-world coding tasks that capture
typical LLM usage scenarios. We modify these tasks, generate
reference solutions and test cases, and adapt the problems
into multiple programming languages. This effort results in
RealisticCodeBench, comprising a total of 376 programming
problems translated across multiple languages: 361 in Python,
346 in JavaScript, 343 in TypeScript, 307 in Java, and 323
in C++, each with corresponding reference solutions and test
cases. We evaluate 12 general-purpose and code-speciﬁc LLMs on
RealisticCodeBench. Our ﬁndings reveal that GPT-4.1 achieves
the highest average pass@1 score across languages, closely
followed by DeepSeek-V3-671B, suggesting that DeepSeek-V3-
671B provides a viable open-source alternative to GPT-4.1 for
large companies with sufﬁcient GPU resources and privacy
concerns. CodeGeeX4-9B, a cost-effective model, emerges as a
suitable substitute for GPT-4o-mini for individual developers and
smaller organizations with similar privacy considerations. Addi-
tionally, LLM performance discrepancies between HumanEval
and RealisticCodeBench suggest that some LLMs are either
overly specialized for HumanEval-style problems or insufﬁciently
optimized for real-world coding challenges. Finally, we analyze
failed cases, summarize common LLM limitations, and provide
implications for researchers and practitioners.
Index Terms —Code Generation, Large Language Model,
Benchmark, GitHub
*Corresponding author: Xin Xia, xin.xia@acm.org
I. I NTRODUCTION
Code generation, which automatically creates code snip-
pets from natural language descriptions, has been widely
adopted to enhance development efﬁciency and productiv-
ity, attracting signiﬁcant attention in academic research [ 1],
[2], [ 3], [ 4]. Recent advances in Large Language Models
(LLMs)—trained on massive volumes of both general and
code-speciﬁc datasets—have further accelerated progress in
this ﬁeld [ 5]. To evaluate the performance of these emerging
LLMs on code generation tasks, several benchmarks have
been introduced, starting with HumanEval [ 6] and MBPP [ 7].
Reporting performance on these benchmarks has seemingly
become mandatory for a model to be considered competi-
tive in code generation [ 8]. Indeed, nearly all new LLMs
released in 2023-2025 highlight code generation results on
one or both of these benchmarks. While they have been
widely used and provide valuable insights, the programming
problems they contain are largely algorithmic and basic pro-
gramming problems, which do not fully reﬂect the challenges
of real-world coding [ 9]. To address this, more complex
benchmarks—such as CoderEval [ 10], EvoCodeBench [ 11],
ComplexCodeEval [ 12], and ClassEval [ 13]—have been de-
veloped to assess LLM performance on more challenging,
practical coding tasks collected from real-world GitHub code
repositories, such as non-standalone function generation and
class-level code generation. These benchmarks offer a deeper
understanding of the upper limits of LLM capabilities when
tackling intricate programming problems.
However, developers currently tend not to rely on LLMs
for overly complex coding tasks, primarily due to the low
success rates of LLMs on more challenging benchmarks.
For example, GPT-3.5 achieves only a 21% pass@1 rate for
non-standalone function generation on CoderEval [ 10], while
GPT-4 reaches just a 37.0% pass@1 rate for class-level code
generation on ClassEval [13], which can discourage developers
from using LLMs for such sophisticated code generation tasks.
A large-scale survey conducted by Liang et al. [ 14] found
3020
2025 40th IEEE/ACM International Conference on Automated Software Engineering (ASE)
2643-1572/25/$31.00 ©2025 IEEE
DOI 10.1109/ASE63991.2025.00248
that developers often discard LLM-generated code or abandon
the use of code generation models when they fail to meet
functional or non-functional requirements, when developers
struggle to control the models to produce the desired output,
or when signiﬁcant effort is needed to debug and reﬁne
the LLM-generated code. In other words, while developers
often work on complex programming problems like those in
CoderEval [10], EvoCodeBench [ 11], ComplexCodeEval [ 12],
and ClassEval [13], current LLMs are not yet ready to generate
such sophisticated code at scale. Instead, developers are more
likely to use LLMs for more manageable coding tasks that
high-performing models (e.g., GPT-4.1) can generate correctly
without requiring extensive debugging or modiﬁcation. There-
fore, to better align benchmarks with current developer prac-
tices of using LLMs for code generation, we need to shift our
focus toward understanding the types of code developers are
actually generating with LLMs daily and create benchmarks
based on these practical use cases.
To achieve this, we collect real-world coding tasks that
reﬂect typical LLM code generation scenarios by mining high-
star GitHub repositories for code samples explicitly labeled as
generated by ChatGPT or Copilot. Speciﬁcally, our previous
study [ 15] ﬁnd that nearly all LLM-generated code on GitHub
is produced by tools like ChatGPT or Copilot, with very few
samples from other LLMs. Developers frequently annotate
such code snippets with comments like “the code is generated
by ChatGPT,” indicating they are created using these tools.
Using search terms like “generated by ChatGPT”, we leverage
the GitHub REST API to locate and collect relevant Python,
Java, JavaScript, TypeScript, and C++ code samples from
high-star projects, which represent how developers use LLMs
for code generation in real-world scenarios. After collecting
the samples, we carefully ﬁlter out overly simplistic, repetitive,
or difﬁcult-to-test codes.
We then make modiﬁcations to each sample’s requirements
while preserving the original intent and complexity as much
as possible. Where applicable, we also adjust the number
and types of input and output parameters to further mitigate
data leakage risks. Using ChatGPT-4o, we generate reference
solutions for each modiﬁed programming problem, followed
by manual corrections. ChatGPT-4o also creates multiple
test cases based on the problem descriptions and reference
solutions, which are reﬁned manually to ensure accuracy and
adequate line and branch coverage. Next, we use ChatGPT-
4o to generate multi-language versions of each programming
problem, followed by manual validation of the accuracy of the
translated solutions, test cases, and coverage. It is important
to note that some programming problems do not translate di-
rectly across languages due to language-speciﬁc data types or
operations. In such cases, we retain the problems as language-
speciﬁc to reﬂect real-world development practices. Finally,
we invite 13 experienced engineers to assess whether the pro-
gramming problems, including their multi-language versions,
represent realistic development scenarios and if proprietary
developers would also likely use LLMs to solve them. Only
problems approved by a majority (at least 10 out of 13 engi-
neers) are retained. Ultimately, we construct our benchmark,
RealisticCodeBench, comprising 376 programming problems
translated across multiple languages: 361 in Python, 346
in JavaScript, 343 in TypeScript, 307 in Java, and 323 in
C++. Each problem includes corresponding reference solutions
and test cases, spanning 9 distinct domains such as data
structures and algorithms, text processing, ﬁle handling, data
visualization and graphic applications, network programming,
and frontend development. This provides a comprehensive
assessment of LLM capabilities on coding challenges that
developers currently address with LLM assistance.
Based on RealisticCodeBench, we conduct extensive exper-
iments on 12 general-purpose and code-speciﬁc models com-
monly studied in recent benchmarks, such as GPT-4.1, GPT-
4o-mini, DeepSeek-V3-671B, Llama 3.1-8B, CodeGeeX4-9B,
DeepSeek-Coder-6.7B, CodeLlama-7B, and StarCoder2-7B.
Across ﬁve programming languages, GPT-4.1 achieves the
highest average pass@1 score at 60.65%, with DeepSeek-V3-
671B close behind at 58.86%. This suggests that companies
with sufﬁcient resources and privacy concerns could consider
deploying DeepSeek-V3-671B as an open-source alternative to
GPT-4.1 for everyday coding tasks. CodeGeeX4-9B achieves
an average pass@1 score of 45.75%, compared to GPT-4o-
mini’s 53.11%, showing only a moderate gap between them.
Thus, individual developers and smaller organizations with
similar privacy concerns can deploy CodeGeeX4-9B as an
affordable substitute for GPT-4o-mini, using a setup with
two NVIDIA GeForce RTX 3090 (24GB) GPUs (approxi-
mately $3,000) to balance privacy, cost, and code genera-
tion performance. Furthermore, we observe substantial per-
formance discrepancies of some LLMs between HumanEval
and RealisticCodeBench. While models like CodeGeeX4-9B
reach impressive pass@1 scores on HumanEval (82.3%) and
DeepSeek-Coder-6.7B scores 78.6%, their performance drops
substantially on RealisticCodeBench’s Python subset (54.02%
and 45.15%, respectively). This suggests that current LLMs
may either be overly specialized for HumanEval-style prob-
lems or lack optimization for practical coding tasks. Finally, by
analyzing failed cases, we identify critical areas where LLMs
fall short in RealisticCodeBench, offering insights into poten-
tial improvements for practical code generation capabilities.
In summary, our contributions are as follows:
(1) We propose RealisticCodeBench, a benchmark that
aligns with the types of coding problems developers typically
solve with LLMs in practical development settings. Our bench-
mark is available in [ 16].
(2) We systematically benchmark 12 LLMs’ code generation
capabilities using RealisticCodeBench. Based on the results,
we provide implications for researchers and practitioners.
II. B
ACKGROUND AND RELA TED WORK
A. LLMs for Code Generation
Code generation involves creating code snippets based
on given natural language requirements. General LLMs are
typically trained on a combination of general textual data,
code corpora, and instructions. Among the most well-known
3021
general LLMs are GPT-4 [ 17] and GPT-3.5 [ 18], both of
which have demonstrated signiﬁcant capabilities across a wide
range of tasks. Additionally, other general-purpose models like
DeepSeek-V3 [ 19], Llama 3.1 [ 20], Phi-3 [ 21], Mistral [ 22],
and ChatGLM [ 23] have gained attention for their capabilities.
Technical reports for these models often emphasize their
strengths not only in general natural language processing tasks
but also their promising potential in code generation.
On the other hand, specialized code LLMs are primarily
trained on large-scale code-speciﬁc datasets with tailored in-
structions, often outperforming general-purpose LLMs in code
generation tasks. Notable examples include CodeGen [ 24],
StarCoder [ 25], CodeLlama [ 26], DeepSeek-Coder [ 27], and
CodeGeeX [28]. For instance, DeepSeek-Coder is trained from
scratch on 2 trillion tokens, with a composition of 87% code
and 13% natural languages in both English and Chinese.
StarCoder2 is trained on 17 programming languages from
the Stack v2 [ 25]. These models are designed to focus more
speciﬁcally on understanding and generating code, typically
demonstrating superior performance in handling code-related
tasks compared to general LLMs.
B. Code Generation Benchmarks
Literature Search: To understand the progress of code
generation benchmarks, we conduct a literature search cov-
ering publications from 2021 to 2025 by using a forward
snowballing approach [ 35]
1. The starting year of 2021 is
selected, as it marks the publication of the earliest prominent
benchmarks for code generation, which include test cases for
evaluating LLMs’ code generation accuracy (i.e., APPS [ 36],
HumanEval [ 6], and MBPP [ 7]). Although earlier code gen-
eration benchmarks, such as Concode [ 37] and JuICe [ 38],
were proposed before 2021, they mainly focused on evaluating
deep learning models, like LSTM and Transformer, rather
than LLMs. Moreover, these datasets lacked test cases, relying
instead on metrics like exact accuracy and BLEU to compare
model performance. Consequently, they are rarely used in later
research evaluating LLMs for code generation.
Therefore, our search process begins by gathering all papers
that cite APPS [ 36], HumanEval [ 6], and MBPP [ 7] using
Google Scholar. We then ﬁlter these citations to identify
papers proposing new benchmarks or signiﬁcantly extending
existing ones in the context of code generation, considering
only studies written in English with full text available. We
exclude papers that introduce benchmarks for unrelated ﬁelds
(e.g., program repair, code completion, or code translation)
and focus solely on those proposing code generation bench-
marks. For each selected paper, we recursively examine its
citations, focusing on new or updated benchmarks developed.
This process continues until no further relevant papers are
found, ensuring that no signiﬁcant benchmark developments
are missed during the search. Finally, the overall search pro-
cess results in 57 code generation benchmarks. The identiﬁed
benchmarks can be broadly classiﬁed into three categories. The
1This literature review was conducted in April 2025.
ﬁrst category, comprising 25 papers [ 39], [40], [41], [42], [43],
[44], [ 45], [ 46], [ 47], [ 48], [ 49], [ 50], [ 51], [ 52], [ 53], [ 54],
[55], [ 56], [ 57], [ 58], [ 59], [ 60], [ 61], [ 62], [ 63], focuses on
domain-speciﬁc code generation abilities, such as generating
security code [46], [49], [61], VHDL code [ 53], bioinformatics
code [47], V erilog code [43], data science code [ 7], [40], [41],
[59], AI code [ 48], object-oriented code in Java [ 50], parallel
code [ 54], Infrastructure-as-Code (IaC) programs [ 56], web
design [ 60], etc. The second category, comprising 21 papers
focusing on non-realistic code generation development scenar-
ios [ 6], [ 7], [ 64], [ 65], [ 66], [ 28], [
67], [ 68], [ 69], [ 8], [ 36],
[70], [71], [72], [10], [73], [74], [75], [76], [77], [78], includes
one subset of benchmarks such as HumanEval [ 6], MBPP [ 7],
or their modiﬁed versions [ 64], [ 65], [ 66], [ 28], [ 68], [ 69],
[33], [ 64], [ 65], [ 33] that focus on pure-method algorithm or
logic tasks and often exhibit exceptionally high performance
on state-of-the-art models. The other subset [ 74], [ 76], [ 72],
[73], [ 75], such as LiveCodeBench [ 74] and CodeElo [ 76],
focus on algorithmic tasks for competitive programming, but
these competitive programming problems are rarely encoun-
tered in real-world development scenarios. The third category,
which includes 11 papers [ 29], [ 12], [ 34], [ 11], [ 10], [ 13],
[30], [ 32], [ 31], [ 33], [ 9], focuses on evaluating general code
generation capabilities that reﬂect real-world development
scenarios, which aligns with the goals of our benchmark. Due
to space constraints, we only discuss the difference between
these benchmarks and our RealisticCodeBench.
Table I
overviews the 11 benchmarks, including details such
as the year of introduction, target programming language,
the source of programming problems, target code granularity,
the number of programming problems (“#Tasks”), average
lines of code (“#LOC”) in reference solutions, average token
lengths of the task prompt (usually the requirements and
function signature) (“#Tokens”), and the best model perfor-
mance (usually GPT-4) in pass@1. In the table, “
” indicates
that the corresponding information was not provided in the
benchmark paper. Among them, the two benchmarks without
a pass@1 values, the MCoNaLa benchmark [ 29] focuses
solely on statement-level code generation scenarios collected
from Stack Overﬂow. In contrast, ComplexCodeEval [ 12]
includes function-level tasks sourced from real and complex
development environments in GitHub repositories. However,
it lacks test cases to accurately assess the generated code.
For benchmarks involving complex, non-standalone func-
tions and classes [ 34], [11], [10], [13], [30], [32], low pass@1
scores are mainly due to the intricate dependencies inherent to
these tasks. For example, EvoCodeBench [ 11], DevEval [ 30],
and HumanEvo [ 34] focus on complex function dependencies
or repository-level dependencies, resulting in pass@1 scores
of 20.7%, 53.0%, and 34.5% on GPT-4, respectively. Simi-
larly, ClassEval [ 13], a benchmark of 100 manually created
Python problems that simulate real-world class generation
scenarios, yielded a 37.0% pass@1 score on GPT-4. In
addition, two benchmarks were created from open source
data. Paul et al. [ 31] developed ScenEval, collecting various
statements, methods, and classes from open source platforms
3022