# CODE-DITING: Automatic Evaluation of Code Generation without References or Test Cases
# CODE-DITING：无需参考代码或测试用例的代码生成自动评估

**来源**: ASE 2025 | **作者**: Guang Yang 等

## 摘要
可信的代码评估方法对神经代码生成至关重要。传统方法依赖参考解决方案或可执行测试用例，在灵活性和可扩展性上存在固有局限。LLM-as-Judge方法通过直接评估问题描述与生成代码之间的功能一致性提供了有前景的替代方案。本文首先对三个多样化数据集进行全面实证研究，揭示了两类LLM-as-Judge方法的优缺点：基于通用基础模型的方法性能好但需复杂提示且缺乏可解释性，基于推理模型的方法可解释性更好但计算资源需求大。为此提出CODE-DITING，通过数据蒸馏框架将DeepSeek-R1-671B的推理能力迁移到1.5B和7B模型。CODE-DITING 7B超越了GPT-4o和DeepSeek-V3 671B，仅使用其1%的参数量。

## 引言核心
- 传统评估指标（BLEU、Pass@k等）存在固有局限：参考方法惩罚正确但不同的实现，测试方法需要人工设计和安全执行环境
- LLM-as-Judge方法快速发展，但其在代码生成评估中的性能仍存在不确定性
- 基于通用模型的方法需要精心设计的提示且缺乏可解释性；基于推理模型的方法计算成本高
- 需要在准确性、效率和可解释性之间取得平衡的代码评估方法
- 现有方法存在偏好泄露问题（评估模型偏向同系列架构生成的代码）

## 方法概述
CODE-DITING的核心是数据蒸馏框架，将强大的DeepSeek-R1-671B模型的推理能力迁移到更紧凑的模型中。具体而言，作者构建了CODEJUDGE-17K高质量数据集，包含17,000个精心策划的带推理路径的样本。通过这一过程，不仅增强了评估的可解释性，还使推理过程更加可理解。

在模型训练方面，CODE-DITING采用PiSSA技术进行模型训练，并在推理阶段使用多数投票策略进一步提升性能。作者还构建了三个新基准数据集（HumanEval-Judge、MBPP-Judge和BigCodeBench-Judge）用于实证研究，系统比较了不同LLM-as-Judge方法在代码生成评估中的表现。

## 关键实验发现
- RQ1（实证研究）：通用模型方法性能好但需复杂提示且缺乏可解释性；推理模型方法可解释性好但计算成本高
- RQ2（CODE-DITING性能）：CODE-DITING 1.5B超越所有同参数量级模型，达到5倍参数量模型的性能；CODE-DITING 7B超越GPT-4o和DeepSeek-V3 671B
- RQ3（消融实验）：所有组件（数据蒸馏、PiSSA训练、多数投票）对最终性能均不可或缺
- RQ4（偏好泄露）：CODE-DITING对偏好泄露具有鲁棒性，不会偏向同系列架构生成的代码

## 写作特征备注
- 标题结构：工具命名型（"CODE-DITING" + 任务描述 + 约束条件）
- 摘要是否有数字：是（1.5B、7B、671B、1%、17K等）
- 是否有RQ：否（采用实证研究+方法提出的结构）
- 是否有Motivating Example：否（通过实证研究引出动机）
- 贡献点数量：4

## 结论
CODE-DITING通过数据蒸馏将大模型推理能力迁移到小模型，在仅使用1%参数量的情况下超越GPT-4o等大模型，有效平衡了代码评估的准确性、效率和可解释性，为无参考无测试的代码评估提供了实用方案。
