# Correlating Automated and Human Evaluation of Code Documentation Generation Quality
# 代码文档生成质量的自动评估与人工评估相关性研究

**来源**: TOSEM 2021 | **作者**: Xing Hu 等

## 摘要
自动代码文档生成是软件工程领域的关键任务。基于深度学习的方法广泛使用BLEU、METEOR、ROUGE-L、CIDEr和SPICE等自动指标来评估模型，但这些指标与人工判断之间的相关性尚未得到充分验证。本文在代码注释生成和提交消息生成两个任务上，复现了六种最先进方法，并招募24名评估者从语言、内容和有效性三个方面对生成文档进行人工评分。结果表明自动指标的排名与人工评估不一致，METEOR与人工评估的相关性最强（Pearson r约0.7），但仍低于标注者间一致性（r约0.8）。

## 引言核心
- 深度学习方法广泛使用自动指标评估代码文档生成质量，但这些指标是否能替代人工评估尚不明确
- 自动指标主要通过计算生成文本与参考文本的N-gram重叠来衡量质量，可能无法反映人类对文档质量的真实判断
- 在神经机器翻译领域，自动指标已被证明与人工判断有较好的相关性，但在代码文档生成领域缺乏类似验证
- 人工评估虽然更可靠但成本高昂且耗时，因此需要确定自动指标的可靠程度

## 方法概述
本文在代码注释生成和提交消息生成两个任务上进行实验。对于代码注释生成任务，复现了Hybrid-DeepCom、Code2Seq和Re2Com三种方法；对于提交消息生成任务，复现了NMT、NNGen和PtrGNCMsg三种方法。所有方法均使用BLEU、METEOR、ROUGE-L、CIDEr和SPICE五种自动指标进行评估。

人工评估方面，招募24名评估者对200个随机采样的注释和提交消息进行评分。评估从三个方面展开：语言相关（自然性和表达性）、内容相关（内容充分性和简洁性）、有效性相关（有用性和代码可理解性）。通过计算Kendall τ和Pearson r相关系数分析自动指标与人工评估之间的相关性。

## 关键实验发现
- RQ1（自动指标与人工评估结果）：自动指标对不同方法的排名与人工评估者的排名不一致
- RQ2（指标内部一致性）：人工评估指标之间具有较高的一致性，自动指标之间也具有一定一致性
- RQ3（自动与人工指标相关性）：METEOR与人工评估的相关性最强（中等Pearson相关r约0.7），但仍远低于标注者间一致性（r约0.8）和NMT领域报告的相关性

## 写作特征备注
- 标题结构：动名词短语（Correlating Automated and Human Evaluation...）
- 摘要是否有数字：是（24名参与者、Pearson r约0.7和0.8）
- 是否有RQ：是（3个明确的研究问题）
- 是否有Motivating Example：否
- 贡献点数量：3（隐含在研究问题中）

## 结论
本文指出现有自动评估指标不足以替代人工评估来衡量代码文档生成质量，METEOR是与人工判断相关性最强的指标，但仍有较大改进空间，呼吁开发更贴近人工评估的专用自动评估指标。
