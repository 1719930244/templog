# Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks
# 评估与推进面向软件工程任务的大语言模型基准测试

**来源**: TOSEM 2025 | **作者**: Xing Hu 等

## 摘要
大语言模型（LLM）因其在各类应用中的卓越表现，在软件工程（SE）领域日益受到关注，被广泛应用于需求工程与设计、代码分析与生成、软件维护和质量保证等任务。随着LLM在SE中的深入应用，评估其有效性对于理解其潜力至关重要。本文对291个基准测试进行了全面综述，从三个方面展开：有哪些可用的基准测试、基准测试如何构建、以及基准测试的未来展望。论文分析了现有基准测试的局限性，讨论了LLM在不同软件任务中的成功与失败，并探索了SE相关基准测试的未来机遇与挑战。

## 引言核心
- LLM在代码生成、自动程序修复、测试用例生成等SE任务中展现了显著的适应性和性能提升
- 基准测试（如HumanEval、SWE-bench、CodeXGLUE）为评估LLM提供了标准化框架，对于理解模型性能至关重要
- 现有基准测试在构建方法、评估指标和覆盖范围上存在差异，缺乏系统性综述
- 本文是首个针对SE领域LLM基准测试的全面系统文献综述（SLR）
- 涵盖需求工程与设计、编码助手、软件测试、AIOps、软件维护和质量管理六大SE任务

## 方法概述
本文采用系统文献综述（SLR）方法，对截至2025年6月发布的291个基准测试进行了全面分析。研究从三个维度展开：首先，识别当前LLM所解决的SE任务类别，包括需求工程与设计、编码助手、软件测试、AIOps、软件维护和质量管理；其次，分析每个任务下的基准测试及其构建方法，包括手动构建、自动收集和混合方法；最后，讨论基准测试的未来展望，包括已解决的问题和仍存在的挑战。

论文还对基准测试按SE任务进行了分类，总结了各任务下的使用情况、构建方法和发展趋势，并维护了一个开源GitHub仓库以持续更新高质量基准测试资源。

## 关键实验发现
- RQ1（可用基准测试）：识别并分类了291个SE相关基准测试，覆盖六大SE任务领域
- RQ2（构建方法）：基准测试的构建方法包括手动构建、自动收集和混合方法，各有优劣
- RQ3（未来展望）：现有基准测试面临数据泄露、评估指标单一、任务覆盖不全等挑战，需要更实际、更动态的评估工具

## 写作特征备注
- 标题结构：综述型（Assessing and Advancing...）
- 摘要是否有数字：是（291个基准测试）
- 是否有RQ：是（三个维度的研究问题）
- 是否有Motivating Example：否
- 贡献点数量：4

## 结论
本文作为首个SE领域LLM基准测试的系统综述，全面梳理了291个基准测试的现状、构建方法和发展趋势，为社区创建更有效的评估工具提供了重要参考。
