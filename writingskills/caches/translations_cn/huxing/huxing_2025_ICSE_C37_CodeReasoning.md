# Reasoning Runtime Behavior of a Program with LLM: How Far Are We?
# 用大语言模型推理程序运行时行为：我们还有多远？

**来源**: ICSE 2025 | **作者**: Junkai Chen 等

## 摘要
代码大语言模型在代码理解和生成方面展现出强大能力，但现有代码推理基准（如HumanEval、CRUXEval）主要关注程序输入输出的预测，忽略了程序执行过程中的中间行为以及推理的逻辑一致性。本文提出REval框架，包含两个评估组件：运行时行为推理（Runtime Behavior Reasoning）和增量一致性评估（Incremental Consistency Evaluation）。运行时行为推理包括代码覆盖预测、程序状态预测、执行路径预测和输出预测四个任务。增量一致性提出新指标IC来衡量模型在递进相关任务上的逻辑一致性。大规模实证研究表明，大多数LLM在运行时行为推理（平均准确率44.4%）和增量一致性（平均IC分数10.3）上表现不佳。

## 引言核心
- 代码推理是代码LLM最核心的能力之一，即预测代码执行行为（程序输出、执行路径、变量值等）
- 现有基准（如CRUXEval）仅评估输入输出预测，忽略了执行过程中的中间运行时行为
- 运行时行为（如程序状态、执行路径）对程序理解和调试至关重要
- 模型可能出现逻辑不一致：正确预测下一条执行语句但无法判断变量值，这在人类推理中不会发生
- 现有一致性研究局限于语义一致性（如NL与代码的反向翻译），忽略了逻辑一致性

## 方法概述
REval框架包含两个评估组件。运行时行为推理组件定义了四个递进任务：(1) 代码覆盖预测（CCP）——判断给定输入下某语句是否被执行；(2) 程序状态预测（PSP）——预测变量的值和类型；(3) 执行路径预测（EPP）——预测下一条被执行的语句；(4) 输出预测（OP）——预测程序输出。这四个任务覆盖了控制流、数据流和类型依赖等程序执行的多个方面。

增量一致性评估组件提出IC指标，衡量模型在递进难度的相关任务上保持逻辑一致性的程度。四个运行时行为推理任务具有递进关系——完成当前任务所需的知识是下一个任务的前提。框架基于HumanEval和ClassEval等现有可执行数据集构建适配基准，通过提取运行时行为、构建和过滤问题来生成评估数据。

## 关键实验发现
- RQ1（运行时行为推理）：大多数LLM表现不佳，平均准确率仅44.4%，表明代码推理能力亟需加强
- RQ2（增量一致性）：所有开源LLM的IC分数低于20，平均IC分数仅10.3，逻辑一致性严重不足
- RQ3（模型对比）：通用LLM和代码专用LLM在运行时行为推理上均存在显著局限
- 评估结果凸显了社区加强代码LLM推理能力的迫切需求

## 写作特征备注
- 标题结构：任务描述+反问句（Reasoning Runtime Behavior of a Program with LLM: How Far Are We?）
- 摘要是否有数字：是（44.4%平均准确率，10.3平均IC分数）
- 是否有RQ：是（通过实证研究的研究问题组织）
- 是否有Motivating Example：是（Figure 1通过代码示例展示运行时行为推理的各个方面）
- 贡献点数量：4

## 结论
REval框架首次系统评估了代码LLM对程序运行时行为的推理能力和逻辑一致性，揭示了当前模型在这两方面的严重不足，为后续研究指明了提升代码推理能力的方向。
